Experiments and Results analysis:
In this section, we demonstrate the experiments we ran for table detection and table decomposition and analyse the results.

Table Detection
As described in Section 5 in this paper, we posed the table detection problem as a classification problem where every line is classified as either a TABLELINE or NONTABLELINE. We trained models using 3 different Machine Learning techniques - CRF, SVM (Gaussian kernel, degree 5) and LR and evaluated their performance. 

Data Set:
One of the biggest challenges faced was that there was no off-the-shelf annotated data set available for this problem. So, we manually annotated our dataset. We took 15 pdf files taken at random from the publications page of CS faculty from the University of Wisconsin, Madison. Each pdf file was first converted to xml using pdftohtml. They were then passed through the preprocessing algorithm as described in Algorithm 1. Each xml preprocessed was converted to a HTML file which was hosted on a web server. The HTML file was designed in a way to allow the user to demarcate the table boundary. When the user clicked on the 'Submit' button on the page, a CGI script written in python read the HTML post data and wrote the annotated data to a file. Though, annotations for all these 15 pdfs were done by us, the main reason to create HTML files and publish them in a web server is two fold: (1)It is easy to annotate the training data on a web page for the format that is required by our system, (2) For annotating a large number of pdf documents (going forward), we can easily crowd source the effort. Crowd sourcing drastically reduces the time required to annotate data and is very cheap. There are, of course, many challenges associated with crowd sourcing and tackling them is beyond the scope of this paper. 

The 15 pdf files contained a total of 16052 lines out of which 14310 were NONTABLELINEs and 1742 lines were TABLELINEs. Also they contain 65 tables in total.

Empirical Evaluation of the different techniques:
Detection of TABLELINE:
We adopt a 5 fold Cross Validation approach to empirically compare the 3 different learning settings - CRF, SVM and LR. The confusion matrix (indicating the actual vs predicted classes) for the 3 methods - CRF, SVM and LR are listed in Tables 1, 2 and 3 respectively. Also, We can infer the precision and recall for both the classes (TABLELINE and NONTABLELINE) from the Confusion matrices.
Precision is defined as True Positives / (True Positives + False Positives)
Recall is defined as True Positives / (True Positives + False Negatives)
The precision and recall for the NONTABLELINE class are listed in Table 4. Table 5 contains the precision and recall for the TABLELINE class.

[======================= Replace NS with NONTABLELINE =======================
 and =================== S with TABLELINE========== also x axis is Actual and y axis is predicted =======================]
Table 1
CRF (with Initial Learning Rate 0.2 and running for 80 epochs)
	 NS    S   
    NS14156   154  
     S 1291   451 
Table 2
SVM (with a Gaussian Kernel of degree 5 and C = 20)
        NS    S   
    NS13809   501  
     S 821    921 
Table 3
LR (with Initial Learning Rate 0.2 and running for 50 epochs)
	 NS    S   
    NS14224   86  
     S 1216   526 

Table 4: Precision and Recall of NONTABLELINE
			Precision Recall (in percentage)
CRF			91.64      98.92
SVM Gaussian		94.39	   96.50
Logistic Regression	92.12	   99.40

Table 5: Precision and Recall of TABLELINE
			Precision Recall (in percentage)
CRF			74.55      25.89
SVM Gaussian		64.77	   52.87
Logistic Regression	85.95	   30.19


The reason for the very low precision and recall for the TABLELINE class are two fold: (1) Lack of a large number of TABLELINEs in our dataset. (2) Features engineered towards reducing the error of NONTABLELINEs instead of that of TABLELINEs. Nevertheless, if we add more training data with a lot of TABLELINEs and add in more features which are geared towards reducing the error rate of TABLELINEs, we are certain than the Precision and Recall of the TABLELINE class would improve.

Impact of PostProcessing:

Even though the precision and recall of TABLELINEs is very low, when combined with our postprocessing algorithm (explained in Section 5.4) for Table boundary detection, surprisingly most of the tables and their boundaries are properly detected. This can be explained by the following reasons (1) A low precision and recall for TABLELINE class means that some of the TABLELINEs are missed and are classified as NONTABLELINEs. However, the precision and recall of the NONTABLELINEs is very high. This means that not many NONTABLELINEs are classified as TABLELINEs. (2) Since our postprocessing algorithm tries to include those lines classified as NONTABLELINEs in between TABLELINEs (which seems to be the major scenario in our case), the actual TABLELINEs which were misclassified as NONTABLELINEs are added back to the result. The precision and recall of TABLELINE after postprocessing is given by Table 6.

Table 6: Precision and Recall of TABLELINE after postprocessing
			Precision Recall (in percentage)
CRF			79.10      33.46
SVM Gaussian		67.76	   60.44
Logistic Regression	88.49	   37.94

Our initial assumption from the works of Liu et al. [4] was that sequence labeling using CRF should perform better than any classifier. But surprisingly, in our experiments we found that SVM with a Gaussian Kernel gave better detection of table boundaries than CRF and LR. Eventhough the precision of SVM is lower, the recall is high relative to the other techniques which implies most of the TABLELINEs are retrieved after postprocessing leading to better table boundary detection. However, to conclusively say that SVM with a Gaussian Kernel is the best approach for this problem, we need to try out more experiments with carefully selected new features and varying the number of epochs and the learning rate of CRF and LR. We plan to do this as part of our future work.

Code Open Source:
The implementation is open source and is located in GitHub (https://github.com/shriram-sridharan/TableExtraction). Please read the README.txt file for the dependencies and the procedure to execute the code. We greatly encourage and appreciate any feedback on the project. 

Conclusion:
In this project, we have built a prototype of a table extraction system. We empirically tested the table detection part on a sequence labeler and two classifiers and have shared our experimental results. [Insert for Table decomposition ???????????]We learnt that for this problem, a purely heuristic solution or a purely Machine Learning technique based solution perform poorly when compared with a hybrid approach involving both heuristics and Machine Learning (ML) techniques. Also, we learnt that feature selection and evaluation plays a vital role in the performance of a ML technique. Going forward, we plan to add in more features and evaluate the performance. We also plan to use Schema Matching techniques to identify the significance of the detected rows/columns so that the information in the data rows become more relevant in the context of an Information Extraction (IE) system. We plan to incorporate and test this with a real time IE system such as GeoDeepDive (http://hazy.cs.wisc.edu/hazy/geodeepdive/) and evaluate how the precision and recall of GeoDeepDive improves with the table information. 





