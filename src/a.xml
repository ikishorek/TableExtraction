<text font="0" height="25" left="202" textpieces="0" top="109" width="512">Tuffy: Scaling up Statistical Inference in</text>
<text font="0" height="25" left="195" textpieces="0" top="138" width="531">Markov Logic Networks using an RDBMS&#8727;</text>
<text font="2" height="17" left="186" textpieces="3" top="207" width="543">Feng Niu      Christopher R&#233;      AnHai Doan      Jude Shavlik</text>
<text font="2" height="17" left="336" textpieces="0" top="234" width="259">University of Wisconsin-Madison</text>
<text font="3" height="13" left="298" textpieces="0" top="255" width="336">{leonn,chrisre,anhai,shavlik}@cs.wisc.edu</text>
<text font="2" height="16" left="81" textpieces="0" top="312" width="97">ABSTRACT</text>
<text font="3" height="14" left="81" textpieces="0" top="337" width="359">Markov Logic Networks (MLNs) have emerged as a powerful</text>
<text font="3" height="12" left="81" textpieces="0" top="352" width="359">framework that combines statistical and logical reasoning;</text>
<text font="3" height="12" left="81" textpieces="0" top="368" width="359">they have been applied to many data intensive problems in-</text>
<text font="3" height="12" left="81" textpieces="0" top="384" width="359">cluding information extraction, entity resolution, and text</text>
<text font="3" height="14" left="81" textpieces="0" top="399" width="359">mining. Current implementations of MLNs do not scale to</text>
<text font="3" height="12" left="81" textpieces="0" top="415" width="359">large real-world data sets, which is preventing their wide-</text>
<text font="3" height="14" left="81" textpieces="0" top="431" width="359">spread adoption. We present Tuffy that achieves scalabil-</text>
<text font="3" height="12" left="81" textpieces="0" top="446" width="359">ity via three novel contributions: (1) a bottom-up approach</text>
<text font="3" height="12" left="81" textpieces="0" top="462" width="359">to grounding that allows us to leverage the full power of the</text>
<text font="3" height="12" left="81" textpieces="0" top="478" width="359">relational optimizer, (2) a novel hybrid architecture that al-</text>
<text font="3" height="12" left="81" textpieces="0" top="494" width="359">lows us to perform AI-style local search e&#64259;ciently using an</text>
<text font="3" height="12" left="81" textpieces="0" top="509" width="359">RDBMS, and (3) a theoretical insight that shows when one</text>
<text font="3" height="12" left="81" textpieces="0" top="525" width="359">can (exponentially) improve the e&#64259;ciency of stochastic local</text>
<text font="3" height="12" left="81" textpieces="0" top="541" width="359">search. We leverage (3) to build novel partitioning, loading,</text>
<text font="3" height="12" left="81" textpieces="0" top="556" width="359">and parallel algorithms. We show that our approach outper-</text>
<text font="3" height="12" left="81" textpieces="0" top="572" width="359">forms state-of-the-art implementations in both quality and</text>
<text font="3" height="12" left="81" textpieces="0" top="588" width="265">speed on several publicly available datasets.</text>
<text font="2" height="16" left="81" textpieces="1" top="623" width="174">1.  INTRODUCTION</text>
<text font="3" height="14" left="94" textpieces="0" top="646" width="345">Over the past few years, Markov Logic Networks (MLNs)</text>
<text font="3" height="12" left="81" textpieces="0" top="661" width="359">have emerged as a powerful and popular framework com-</text>
<text font="3" height="14" left="81" textpieces="0" top="677" width="359">bining logical and probabilistic reasoning. MLNs have been</text>
<text font="3" height="12" left="81" textpieces="0" top="693" width="359">successfully applied to a wide variety of data management</text>
<text font="3" height="12" left="81" textpieces="0" top="709" width="359">problems, e.g., information extraction, entity resolution, and</text>
<text font="3" height="12" left="81" textpieces="0" top="724" width="359">text mining. In contrast to probability models like factor</text>
<text font="3" height="12" left="81" textpieces="0" top="740" width="359">graphs [23] that require complex distributions to be speci-</text>
<text font="3" height="14" left="81" textpieces="0" top="756" width="359">&#64257;ed in tedious detail, MLNs allow us to declare a rigorous</text>
<text font="3" height="12" left="81" textpieces="0" top="771" width="359">statistical model at a much higher conceptual level using</text>
<text font="3" height="12" left="81" textpieces="0" top="787" width="359">&#64257;rst-order logic. For example, to classify papers by research</text>
<text font="3" height="12" left="81" textpieces="0" top="803" width="359">area, one could write a rule such as &#8220;it is likely that if one</text>
<text font="3" height="12" left="81" textpieces="0" top="818" width="337">paper cites another they are in the same research area.&#8221;</text>
<text font="1" height="8" left="81" textpieces="0" top="843" width="358">&#8727;We gratefully acknowledge the support of Defense Ad-</text>
<text font="3" height="12" left="81" textpieces="0" top="859" width="359">vanced Research Projects Agency (DARPA) Machine Read-</text>
<text font="3" height="12" left="81" textpieces="0" top="873" width="359">ing Program under Air Force Research Laboratory (AFRL)</text>
<text font="3" height="12" left="81" textpieces="0" top="886" width="359">prime contract no. FA8750-09-C-0181. Any opinions, &#64257;nd-</text>
<text font="3" height="12" left="81" textpieces="0" top="900" width="359">ings, and conclusion or recommendations expressed in this</text>
<text font="3" height="12" left="81" textpieces="0" top="913" width="359">material are those of the authors and do not necessarily re-</text>
<text font="3" height="12" left="81" textpieces="0" top="926" width="343">&#64258;ect the view of DARPA, AFRL, or the US government.</text>
<text font="4" height="11" left="81" textpieces="0" top="950" width="359">Permission to make digital or hard copies of all or part of this work for</text>
<text font="4" height="11" left="81" textpieces="0" top="964" width="359">personal or classroom use is granted without fee provided that copies are</text>
<text font="4" height="11" left="81" textpieces="0" top="977" width="359">not made or distributed for pro&#64257;t or commercial advantage and that copies</text>
<text font="4" height="11" left="81" textpieces="0" top="990" width="359">bear this notice and the full citation on the &#64257;rst page. To copy otherwise, to</text>
<text font="4" height="11" left="81" textpieces="0" top="1004" width="359">republish, to post on servers or to redistribute to lists, requires prior speci&#64257;c</text>
<text font="4" height="11" left="81" textpieces="0" top="1017" width="359">permission and/or a fee. Articles from this volume were invited to present</text>
<text font="4" height="11" left="81" textpieces="0" top="1031" width="359">their results at The 37th International Conference on Very Large Data Bases,</text>
<text font="4" height="11" left="81" textpieces="0" top="1044" width="270">August 29th - September 3rd 2011, Seattle, Washington.</text>
<text font="4" height="10" left="81" textpieces="1" top="1058" width="251">Proceedings of the VLDB Endowment,Vol. 4, No. 6</text>
<text font="4" height="11" left="81" textpieces="2" top="1071" width="309">Copyright 2011 VLDB Endowment 2150-8097/11/03...$10.00.</text>
<text font="3" height="14" left="489" textpieces="0" top="315" width="345">Our interest in MLNs stems from our involvement in a</text>
<text font="3" height="12" left="475" textpieces="0" top="331" width="359">DARPA project called &#8220;Machine Reading.&#8221; The grand chal-</text>
<text font="3" height="12" left="475" textpieces="0" top="346" width="359">lenge is to build software that can read the Web, i.e., extract</text>
<text font="3" height="12" left="475" textpieces="0" top="362" width="359">and integrate structured data (e.g., entities, relationships)</text>
<text font="3" height="12" left="475" textpieces="0" top="378" width="359">from Web data, then use this structured data to answer user</text>
<text font="3" height="14" left="475" textpieces="0" top="393" width="359">queries. The current approach is to use MLNs as a lingua</text>
<text font="3" height="12" left="475" textpieces="0" top="409" width="359">franca to combine many di&#64256;erent kinds of extractions into</text>
<text font="3" height="12" left="475" textpieces="0" top="425" width="359">one coherent picture. To accomplish this goal, it is critical</text>
<text font="3" height="14" left="475" textpieces="0" top="440" width="215">that MLNs scale to large data sets.</text>
<text font="3" height="14" left="489" textpieces="0" top="456" width="345">Unfortunately, none of the current MLN implementations</text>
<text font="3" height="12" left="475" textpieces="0" top="472" width="359">scale beyond relatively small data sets (and even on many</text>
<text font="3" height="12" left="475" textpieces="0" top="488" width="359">of these data sets, existing implementations routinely take</text>
<text font="3" height="12" left="475" textpieces="0" top="503" width="359">hours to run). The &#64257;rst obvious reason is that these are in-</text>
<text font="3" height="12" left="475" textpieces="0" top="519" width="359">memory implementations: when manipulating large inter-</text>
<text font="3" height="12" left="475" textpieces="0" top="535" width="359">mediate data structures that over&#64258;ow main memory, they</text>
<text font="3" height="12" left="475" textpieces="1" top="550" width="359">either crash or thrash badly.  Consequently, there is an</text>
<text font="3" height="12" left="475" textpieces="0" top="566" width="359">emerging e&#64256;ort across several research groups to scale up</text>
<text font="3" height="12" left="475" textpieces="1" top="584" width="358">MLNs.  In this paper, we describe our system, Tuffy1,</text>
<text font="3" height="12" left="475" textpieces="0" top="597" width="359">that leverages an RDBMS to address the above scalability</text>
<text font="3" height="12" left="475" textpieces="0" top="613" width="165">and performance problems.</text>
<text font="3" height="14" left="489" textpieces="0" top="629" width="353">There are two aspects of MLNs: learning and inference [21].</text>
<text font="3" height="12" left="475" textpieces="0" top="644" width="359">We focus on inference, since typically a model is learned</text>
<text font="3" height="12" left="475" textpieces="0" top="660" width="359">once, and then an application may perform inference many</text>
<text font="3" height="12" left="475" textpieces="0" top="676" width="359">times using the same model; hence inference is an on-line</text>
<text font="3" height="14" left="475" textpieces="0" top="692" width="359">process, which must be fast. Moreover, MLN learning al-</text>
<text font="3" height="12" left="475" textpieces="0" top="707" width="359">gorithms typically invoke inference as a subroutine repeat-</text>
<text font="3" height="12" left="475" textpieces="1" top="723" width="358">edly. Conceptually, inference2 in MLNs has two phases: a</text>
<text font="3" height="12" left="475" textpieces="0" top="739" width="359">grounding phase, which constructs a large, weighted SAT</text>
<text font="3" height="12" left="475" textpieces="0" top="754" width="359">formula, and a search phase, which searches for a low cost</text>
<text font="3" height="12" left="475" textpieces="0" top="770" width="359">(weight) assignment (called a solution) to the SAT formula</text>
<text font="3" height="14" left="475" textpieces="0" top="786" width="359">from grounding (using WalkSAT [13], a local search proce-</text>
<text font="3" height="12" left="475" textpieces="0" top="801" width="359">dure). Grounding is a non-trivial portion of the overall in-</text>
<text font="3" height="12" left="475" textpieces="0" top="817" width="359">ference e&#64256;ort: on a classi&#64257;cation benchmark (called RC) the</text>
<text font="3" height="14" left="475" textpieces="0" top="833" width="359">state-of-the-art MLN inference engine, Alchemy [7], spends</text>
<text font="3" height="12" left="475" textpieces="0" top="848" width="359">over 96% of its execution time in grounding. The state-of-</text>
<text font="3" height="12" left="475" textpieces="0" top="864" width="359">the-art strategy for the grounding phase (and the one used</text>
<text font="3" height="14" left="475" textpieces="0" top="880" width="359">by Alchemy) is a top-down procedure (similar to the proof</text>
<text font="3" height="12" left="475" textpieces="0" top="896" width="359">strategy in Prolog). In contrast, we propose a bottom-up</text>
<text font="3" height="12" left="475" textpieces="0" top="911" width="359">grounding strategy. Intuitively, bottom-up grounding allows</text>
<text font="3" height="12" left="475" textpieces="0" top="929" width="359">Tuffy to fully exploit the RDBMS optimizer, and thereby</text>
<text font="3" height="14" left="475" textpieces="0" top="943" width="359">signi&#64257;cantly speed up the grounding phase of MLN infer-</text>
<text font="3" height="14" left="475" textpieces="0" top="958" width="359">ence. On an entity resolution task, Alchemy takes over</text>
<text font="3" height="14" left="475" textpieces="0" top="974" width="359">7 hours to complete grounding, while Tuffy&#8217;s grounding</text>
<text font="3" height="12" left="475" textpieces="0" top="990" width="185">&#64257;nishes in less than 2 minutes.</text>
<text font="1" height="8" left="476" textpieces="0" top="1022" width="245">1http://www.cs.wisc.edu/hazy/tuffy/</text>
<text font="1" height="8" left="476" textpieces="0" top="1037" width="357">2We focus on maximum a posteriori inference which is crit-</text>
<text font="3" height="12" left="475" textpieces="0" top="1053" width="359">ical for many integration tasks. We discuss marginal infer-</text>
<text font="3" height="12" left="475" textpieces="0" top="1067" width="135">ence in Appendix A.5.</text>
<text font="3" height="12" left="94" textpieces="0" top="86" width="345">But not all phases are well-optimized by the RDBMS: dur-</text>
<text font="3" height="12" left="81" textpieces="0" top="102" width="359">ing the search phase, we found that the RDBMS implemen-</text>
<text font="3" height="12" left="81" textpieces="0" top="118" width="359">tation performed poorly. The underlying reason is a funda-</text>
<text font="3" height="12" left="81" textpieces="0" top="133" width="359">mental problem for pushing local search procedures into an</text>
<text font="3" height="12" left="81" textpieces="0" top="149" width="359">RDBMS: search procedures often perform inherently sequen-</text>
<text font="3" height="12" left="81" textpieces="1" top="165" width="358">tial, random data accesses.  Consequently, any RDBMS-</text>
<text font="3" height="12" left="81" textpieces="0" top="180" width="359">based solution must execute a large number of disk ac-</text>
<text font="3" height="12" left="81" textpieces="0" top="196" width="359">cesses, each of which has a substantial overhead (due to the</text>
<text font="3" height="12" left="81" textpieces="0" top="212" width="359">RDBMS) versus direct main-memory access. Not surpris-</text>
<text font="3" height="12" left="81" textpieces="0" top="228" width="359">ingly, given the same amount of time, an in-memory solu-</text>
<text font="3" height="12" left="81" textpieces="0" top="243" width="359">tion can execute between three and &#64257;ve orders of magnitude</text>
<text font="3" height="12" left="81" textpieces="0" top="259" width="359">more search steps than an approach that uses an RDBMS.</text>
<text font="3" height="12" left="81" textpieces="0" top="275" width="359">Thus, to achieve competitive performance, we developed a</text>
<text font="3" height="12" left="81" textpieces="0" top="290" width="359">novel hybrid architecture that supports local search proce-</text>
<text font="3" height="12" left="81" textpieces="0" top="306" width="359">dures in main memory whenever possible. This is our second</text>
<text font="3" height="12" left="81" textpieces="0" top="322" width="136">technical contribution.</text>
<text font="3" height="12" left="94" textpieces="0" top="337" width="345">Our third contribution is a simple partitioning technique</text>
<text font="3" height="14" left="81" textpieces="0" top="353" width="359">that allows Tuffy to introduce parallelism and use less</text>
<text font="3" height="12" left="81" textpieces="0" top="369" width="359">memory than state-of-the-art approaches. Surprisingly, this</text>
<text font="3" height="14" left="81" textpieces="0" top="384" width="359">same technique often allows Tuffy to speed up the search</text>
<text font="3" height="12" left="81" textpieces="0" top="400" width="359">phase exponentially. The underlying idea is simple: in many</text>
<text font="3" height="12" left="81" textpieces="0" top="416" width="359">cases, a local search problem can be divided into multiple</text>
<text font="3" height="12" left="81" textpieces="0" top="432" width="359">independent subproblems. For example, the formula that</text>
<text font="3" height="12" left="81" textpieces="0" top="447" width="359">is output by the grounding phase may consist of multiple</text>
<text font="3" height="12" left="81" textpieces="0" top="463" width="359">connected components. On such datasets, we derive a suf-</text>
<text font="3" height="12" left="81" textpieces="0" top="479" width="359">&#64257;cient condition under which solving the subproblems inde-</text>
<text font="3" height="12" left="81" textpieces="0" top="494" width="359">pendently results in exponentially faster search than running</text>
<text font="3" height="12" left="81" textpieces="0" top="510" width="359">the larger global problem (Thm. 3.1). An application of our</text>
<text font="3" height="12" left="81" textpieces="0" top="526" width="359">theorem shows that on an information extraction testbed,</text>
<text font="3" height="12" left="81" textpieces="0" top="541" width="359">a system that is not aware of this phenomenon (such as</text>
<text font="3" height="12" left="81" textpieces="1" top="559" width="358">Alchemy) must take at least 2200 more steps than Tuffy</text>
<text font="3" height="12" left="81" textpieces="0" top="573" width="359">to reach a solution with the same quality. Empirically we</text>
<text font="3" height="12" left="81" textpieces="0" top="588" width="359">found that, on some real-world datasets, solutions found by</text>
<text font="3" height="12" left="81" textpieces="0" top="607" width="359">Tuffy within one minute have higher quality than those</text>
<text font="3" height="14" left="81" textpieces="0" top="620" width="359">found by non-partitioning systems (such as Alchemy) even</text>
<text font="3" height="12" left="81" textpieces="0" top="635" width="136">after running for days.</text>
<text font="3" height="12" left="94" textpieces="0" top="651" width="345">The exponential di&#64256;erence in running time for indepen-</text>
<text font="3" height="12" left="81" textpieces="0" top="667" width="359">dent subproblems versus the larger global problem suggests</text>
<text font="3" height="12" left="81" textpieces="0" top="683" width="359">that in some cases, further decomposing the search space</text>
<text font="3" height="12" left="81" textpieces="0" top="698" width="359">may improve the overall runtime. To implement this idea</text>
<text font="3" height="14" left="81" textpieces="0" top="714" width="359">for MLNs, we must address two di&#64259;cult problems: (1) parti-</text>
<text font="3" height="12" left="81" textpieces="0" top="730" width="359">tioning the formula from grounding (and so the search space)</text>
<text font="3" height="12" left="81" textpieces="0" top="745" width="359">to minimize the number of formula that are split between</text>
<text font="3" height="12" left="81" textpieces="0" top="761" width="359">partitions, and (2) augmenting the search algorithm to be</text>
<text font="3" height="12" left="81" textpieces="0" top="777" width="359">aware of partitioning. We show that the &#64257;rst problem is NP-</text>
<text font="3" height="12" left="81" textpieces="0" top="792" width="359">hard (even to approximate), and design a scalable heuristic</text>
<text font="3" height="12" left="81" textpieces="0" top="808" width="359">partitioning algorithm. For the second problem, we apply</text>
<text font="3" height="12" left="81" textpieces="0" top="824" width="359">a technique from non-linear optimization to leverage the in-</text>
<text font="3" height="12" left="81" textpieces="0" top="839" width="359">sights gained from our characterization of the phenomenon</text>
<text font="3" height="12" left="81" textpieces="0" top="855" width="359">described above. The e&#64256;ect of such partitioning is dramatic.</text>
<text font="3" height="12" left="81" textpieces="0" top="871" width="359">As an example, on a classi&#64257;cation benchmark (called RC),</text>
<text font="3" height="12" left="81" textpieces="0" top="889" width="359">Tuffy (using 15MB of RAM) produces much better result</text>
<text font="3" height="14" left="81" textpieces="0" top="902" width="359">quality in minutes than Alchemy (using 2.8GB of RAM)</text>
<text font="3" height="14" left="81" textpieces="0" top="918" width="359">even after days of running. In fact, Tuffy is able to an-</text>
<text font="3" height="12" left="81" textpieces="0" top="934" width="359">swer queries on a version of the RC dataset that is over two</text>
<text font="3" height="14" left="81" textpieces="0" top="949" width="359">orders of magnitude larger. (We estimate that Alchemy</text>
<text font="3" height="12" left="81" textpieces="0" top="965" width="266">would need 280GB+ of RAM to process it.)</text>
<text font="5" height="14" left="81" textpieces="1" top="997" width="358">Related Work. MLNs are an integral part of state-of-the-</text>
<text font="3" height="12" left="81" textpieces="0" top="1014" width="359">art approaches in a variety of applications: natural language</text>
<text font="3" height="12" left="81" textpieces="0" top="1030" width="359">processing [22], ontology matching [29], information extrac-</text>
<text font="3" height="12" left="81" textpieces="0" top="1046" width="359">tion [18], entity resolution [25], etc. And so, there is an</text>
<text font="3" height="14" left="81" textpieces="0" top="1061" width="215">application push to support MLNs.</text>
<text font="3" height="12" left="489" textpieces="0" top="86" width="345">Pushing statistical reasoning models inside a database sys-</text>
<text font="3" height="12" left="475" textpieces="0" top="102" width="359">tem has been a goal of many projects [5, 10, 11, 20, 27]. Most</text>
<text font="3" height="14" left="475" textpieces="0" top="118" width="359">closely related is the BayesStore project, in which the</text>
<text font="3" height="12" left="475" textpieces="0" top="133" width="359">database essentially stores Bayes Nets [17] and allows these</text>
<text font="3" height="12" left="475" textpieces="0" top="149" width="359">networks to be retrieved for inference by an external pro-</text>
<text font="3" height="14" left="475" textpieces="0" top="165" width="359">gram. In contrast, Tuffy uses an RDBMS to optimize the</text>
<text font="3" height="12" left="475" textpieces="0" top="180" width="359">inference procedure. The Monte-Carlo database [10] made</text>
<text font="3" height="12" left="475" textpieces="0" top="196" width="359">sampling a &#64257;rst-class citizen inside an RDBMS. In contrast,</text>
<text font="3" height="14" left="475" textpieces="0" top="212" width="359">in Tuffy our approach can be viewed as pushing classical</text>
<text font="3" height="14" left="475" textpieces="0" top="228" width="359">search inside the database engine. One way to view an MLN</text>
<text font="3" height="12" left="475" textpieces="0" top="243" width="359">is a compact speci&#64257;cation of factor graphs [23]. Sen et al. [23]</text>
<text font="3" height="12" left="475" textpieces="0" top="259" width="359">proposed new algorithms; in contrast, we take an existing,</text>
<text font="3" height="12" left="475" textpieces="0" top="275" width="359">widely used class of algorithms (local search), and our focus</text>
<text font="3" height="12" left="475" textpieces="0" top="290" width="311">is to leverage the RDBMS to improve performance.</text>
<text font="3" height="12" left="489" textpieces="0" top="306" width="345">There has also been an extensive amount of work on prob-</text>
<text font="3" height="12" left="475" textpieces="0" top="322" width="359">abilistic databases [1, 2, 4, 19] that deal with simpler proba-</text>
<text font="3" height="12" left="475" textpieces="0" top="337" width="359">bilistic models. Finding the most likely world is trivial in</text>
<text font="3" height="14" left="475" textpieces="0" top="353" width="359">these models; in contrast, it is highly non-trivial in MLNs</text>
<text font="3" height="12" left="475" textpieces="1" top="369" width="359">(in fact, it is NP-hard [6]).  Finally, none of these prior ap-</text>
<text font="3" height="14" left="475" textpieces="0" top="384" width="359">proaches deal with the core technical challenge Tuffy ad-</text>
<text font="3" height="12" left="475" textpieces="0" top="400" width="359">dresses, which is handling AI-style search inside a database.</text>
<text font="3" height="12" left="475" textpieces="0" top="416" width="325">Additional related work can be found in Appendix D.</text>
<text font="5" height="14" left="475" textpieces="1" top="448" width="359">Contributions, Validation, and Outline. To summarize,</text>
<text font="3" height="12" left="475" textpieces="0" top="465" width="222">we make the following contributions:</text>
<text font="3" height="15" left="495" textpieces="0" top="491" width="339">&#8226; In Section 3.1, we design a solution that pushes MLNs</text>
<text font="3" height="12" left="509" textpieces="1" top="507" width="325">into RDBMSes.  The key idea is to use bottom-up</text>
<text font="3" height="12" left="509" textpieces="0" top="523" width="325">grounding that allows us to leverage the RDBMS opti-</text>
<text font="3" height="12" left="509" textpieces="0" top="539" width="333">mizer; this idea improves the performance of the ground-</text>
<text font="3" height="12" left="509" textpieces="0" top="554" width="251">ing phase by several orders of magnitude.</text>
<text font="3" height="13" left="495" textpieces="0" top="572" width="339">&#8226; In Section 3.2, we devise a novel hybrid architecture to</text>
<text font="3" height="12" left="509" textpieces="0" top="588" width="325">support e&#64259;cient grounding and in-memory inference.</text>
<text font="3" height="12" left="509" textpieces="0" top="604" width="325">By itself, this architecture is far more scalable and,</text>
<text font="3" height="12" left="509" textpieces="0" top="619" width="325">given the same amount of time, can perform orders of</text>
<text font="3" height="12" left="509" textpieces="0" top="635" width="267">magnitude more search steps than prior art.</text>
<text font="3" height="13" left="495" textpieces="0" top="652" width="339">&#8226; In Section 3.3, we describe novel data partitioning</text>
<text font="3" height="12" left="509" textpieces="0" top="669" width="325">techniques to decrease the memory usage and to in-</text>
<text font="3" height="12" left="509" textpieces="0" top="685" width="325">crease parallelism (and so improve the scalability) of</text>
<text font="3" height="12" left="509" textpieces="0" top="703" width="325">Tuffy&#8217;s in-memory inference algorithms. Addition-</text>
<text font="3" height="12" left="509" textpieces="0" top="716" width="325">ally, we show that for any MLN with an MRF that</text>
<text font="3" height="12" left="509" textpieces="0" top="732" width="325">contains multiple components, partitioning could ex-</text>
<text font="3" height="12" left="509" textpieces="0" top="747" width="325">ponentially improve the expected (average case) search</text>
<text font="3" height="12" left="509" textpieces="0" top="763" width="31">time.</text>
<text font="3" height="13" left="495" textpieces="0" top="780" width="339">&#8226; In Section 3.4, we generalize our partitioning results</text>
<text font="3" height="14" left="509" textpieces="0" top="797" width="325">to arbitrary MLNs using our characterization of the</text>
<text font="3" height="12" left="509" textpieces="0" top="812" width="325">partitioning phenomenon. These techniques result in</text>
<text font="3" height="12" left="509" textpieces="0" top="828" width="300">our highest quality, most space-e&#64259;cient solutions.</text>
<text font="3" height="12" left="475" textpieces="0" top="854" width="359">We present an extensive experimental study on a diverse set</text>
<text font="3" height="14" left="475" textpieces="0" top="870" width="359">of MLN testbeds to demonstrate that our system Tuffy is</text>
<text font="3" height="12" left="475" textpieces="0" top="886" width="359">able to get better result quality more quickly and work over</text>
<text font="3" height="12" left="475" textpieces="0" top="902" width="315">larger datasets than the state-of-the-art approaches.</text>
<text font="2" height="16" left="475" textpieces="1" top="934" width="178">2.  PRELIMINARIES</text>
<text font="3" height="12" left="489" textpieces="0" top="957" width="345">We illustrate a Markov Logic Network program using the</text>
<text font="3" height="12" left="475" textpieces="0" top="973" width="359">example of classifying papers by topic area. We then de&#64257;ne</text>
<text font="3" height="14" left="475" textpieces="0" top="988" width="336">the semantics of MLNs and the mechanics of inference.</text>
<text font="2" height="16" left="475" textpieces="1" top="1013" width="201">2.1  The Syntax of MLNs</text>
<text font="3" height="14" left="489" textpieces="0" top="1035" width="347">Figure 1 shows an example input MLN program for Tuffy</text>
<text font="3" height="12" left="475" textpieces="0" top="1051" width="359">that is used to classify paper references by topic area, such</text>
<text font="3" height="12" left="475" textpieces="0" top="1067" width="359">as databases, systems, AI, etc. In this example, a user gives</text>
<text font="4" height="10" left="138" textpieces="0" top="96" width="123">paper(PaperID, URL)</text>
<text font="4" height="10" left="139" textpieces="0" top="110" width="122">wrote(Author, Paper)</text>
<text font="4" height="10" left="139" textpieces="0" top="123" width="121">refers(Paper, Paper)</text>
<text font="4" height="10" left="140" textpieces="0" top="137" width="120">cat(Paper, Category)</text>
<text font="4" height="11" left="282" textpieces="1" top="82" width="73">weight   rule</text>
<text font="4" height="11" left="297" textpieces="2" top="95" width="354">5      cat(p, c1), cat(p, c2) =&gt; c1 = c2                         (F1)</text>
<text font="4" height="11" left="297" textpieces="2" top="109" width="354">1      wrote(x, p1), wrote(x, p2), cat(p1, c) =&gt; cat(p2, c)   (F2)</text>
<text font="4" height="11" left="297" textpieces="2" top="122" width="354">2      cat(p1, c), refers(p1, p2) =&gt; cat(p2, c)                 (F3)</text>
<text font="4" height="11" left="289" textpieces="2" top="136" width="362">+&#8734;    paper(p, u) =&gt; &#8707;x. wrote(x, p)                           (F4)</text>
<text font="4" height="11" left="295" textpieces="2" top="149" width="356">-1      cat(p, &#8216;Networking&#8217;)                                        (F5)</text>
<text font="4" height="10" left="673" textpieces="0" top="83" width="97">wrote(&#8216;Joe&#8217;, &#8216;P1&#8217;)</text>
<text font="4" height="10" left="673" textpieces="0" top="96" width="97">wrote(&#8216;Joe&#8217;, &#8216;P2&#8217;)</text>
<text font="4" height="10" left="673" textpieces="0" top="110" width="103">wrote(&#8216;Jake&#8217;, &#8216;P3&#8217;)</text>
<text font="4" height="10" left="673" textpieces="0" top="123" width="100">refers(&#8216;P1&#8217;, &#8216;P3&#8217;)</text>
<text font="4" height="10" left="673" textpieces="0" top="137" width="84">cat(&#8216;P2&#8217;, &#8216;DB&#8217;)</text>
<text font="4" height="11" left="673" textpieces="0" top="148" width="15">&#183; &#183; &#183;</text>
<text font="4" height="11" left="179" textpieces="2" top="163" width="571">Schema                                      A Markov Logic Program                                    Evidence</text>
<text font="3" height="12" left="81" textpieces="0" top="192" width="753">Figure 1: A Sample Markov Logic Program: The goal is to classify papers by area. As evidence we are given</text>
<text font="3" height="12" left="81" textpieces="0" top="208" width="753">author and citation information of all papers, as well as the labels of a subset of the papers; we want to</text>
<text font="3" height="12" left="81" textpieces="0" top="223" width="650">classify the remaining papers. Any variable not explicitly quanti&#64257;ed is universally quanti&#64257;ed.</text>
<text font="3" height="12" left="81" textpieces="0" top="255" width="359">Tuffy a set of relations that capture information about the</text>
<text font="3" height="12" left="81" textpieces="0" top="268" width="359">papers in her dataset: she has extracted authors and cita-</text>
<text font="3" height="12" left="81" textpieces="0" top="284" width="359">tions and stored them in the relations wrote(Author,Paper)</text>
<text font="3" height="12" left="81" textpieces="0" top="300" width="359">and refers(Paper,Paper). She may also provide evidence,</text>
<text font="3" height="12" left="81" textpieces="0" top="316" width="359">which is data that she knows to be true (or false). Here, the</text>
<text font="3" height="12" left="81" textpieces="0" top="331" width="359">evidence shows that Joe wrote papers P1 and P2 and P1</text>
<text font="3" height="12" left="81" textpieces="0" top="347" width="359">cited another paper P3. In the relation cat (for &#8216;category&#8217;),</text>
<text font="3" height="14" left="81" textpieces="0" top="363" width="359">she provides Tuffy with a subset of papers and the cate-</text>
<text font="3" height="12" left="81" textpieces="0" top="378" width="359">gories into which they fall. The cat relation is incomplete:</text>
<text font="3" height="12" left="81" textpieces="0" top="394" width="359">some papers are not labeled. We can think of each possi-</text>
<text font="3" height="12" left="81" textpieces="0" top="410" width="359">ble labeling of these papers as an instantiation of the cat</text>
<text font="3" height="12" left="81" textpieces="0" top="425" width="359">relation, which can be viewed as a possible world [8]. The</text>
<text font="3" height="12" left="81" textpieces="0" top="441" width="359">classi&#64257;cation task is to &#64257;nd the most likely labeling of papers</text>
<text font="3" height="12" left="81" textpieces="0" top="457" width="336">by topic area, and hence the most likely possible world.</text>
<text font="3" height="12" left="94" textpieces="0" top="472" width="345">To tell the system which possible world it should produce,</text>
<text font="3" height="12" left="81" textpieces="0" top="488" width="359">the user provides (in addition to the above data) a set of</text>
<text font="3" height="12" left="81" textpieces="1" top="504" width="358">rules that incorporate her knowledge of the problem.  A</text>
<text font="3" height="12" left="81" textpieces="0" top="520" width="157">simple example rule is F1:</text>
<text font="3" height="11" left="143" textpieces="1" top="546" width="233">cat(p, c1), cat(p, c2) =&gt; c1 = c2  (F1)</text>
<text font="3" height="12" left="85" textpieces="1" top="571" width="354">Intuitively, F1 says that a paper should be in one category.</text>
<text font="3" height="14" left="81" textpieces="0" top="587" width="359">In MLNs, this rule may be hard, meaning that it behaves</text>
<text font="3" height="12" left="81" textpieces="0" top="603" width="359">like a standard key constraint: in any possible world, each</text>
<text font="3" height="12" left="81" textpieces="0" top="618" width="359">paper must be in at most one category. This rule may also</text>
<text font="3" height="12" left="81" textpieces="0" top="634" width="359">be soft, meaning that it may be violated in some possible</text>
<text font="3" height="12" left="81" textpieces="0" top="650" width="359">worlds. For example, in some worlds a paper may be in two</text>
<text font="3" height="12" left="81" textpieces="0" top="665" width="359">categories. Soft rules also have weights that intuitively tell</text>
<text font="3" height="12" left="81" textpieces="0" top="681" width="359">us how likely the rule is to hold in a possible world. In this</text>
<text font="3" height="12" left="81" textpieces="1" top="697" width="359">example, F1 is a soft rule and has weight 5. Roughly, this</text>
<text font="3" height="12" left="81" textpieces="1" top="712" width="359">means that a &#64257;xed paper is at least e5 times more likely</text>
<text font="3" height="12" left="81" textpieces="0" top="728" width="359">to be in a single category compared to being in multiple</text>
<text font="3" height="14" left="81" textpieces="0" top="744" width="359">categories. MLNs can also involve data in non-trivial ways,</text>
<text font="3" height="12" left="81" textpieces="0" top="759" width="359">we refer the reader to Appendix A.1 for a more complete</text>
<text font="3" height="12" left="81" textpieces="0" top="775" width="65">exposition.</text>
<text font="5" height="14" left="81" textpieces="1" top="807" width="358">Query Model. Given the data and the rules, a user may</text>
<text font="3" height="14" left="81" textpieces="0" top="824" width="359">write arbitrary queries in terms of the relations. In Tuffy,</text>
<text font="3" height="12" left="81" textpieces="0" top="840" width="359">the system is responsible for &#64257;lling in whatever missing data</text>
<text font="3" height="12" left="81" textpieces="0" top="856" width="359">is needed: in this example, the category of each unlabeled</text>
<text font="3" height="12" left="81" textpieces="0" top="871" width="359">paper is unknown, and so to answer a query the system infers</text>
<text font="3" height="12" left="81" textpieces="0" top="887" width="336">the most likely labels for each paper from the evidence.</text>
<text font="2" height="16" left="81" textpieces="1" top="913" width="190">2.2  Semantics of MLNs</text>
<text font="3" height="14" left="94" textpieces="0" top="935" width="345">We describe the semantics of MLNs. Formally, we &#64257;rst</text>
<text font="3" height="12" left="81" textpieces="0" top="951" width="359">&#64257;x a schema &#963; (as in Figure 1) and a domain D. Given as</text>
<text font="3" height="12" left="81" textpieces="3" top="967" width="357">input a set of formula &#175;  F = F1, . . . , FN (in clausal form3)</text>
<text font="3" height="12" left="81" textpieces="1" top="982" width="359">with weights w1, . . . , wN, they de&#64257;ne a probability distribu-</text>
<text font="3" height="12" left="81" textpieces="0" top="998" width="359">tion over possible worlds (deterministic databases). To con-</text>
<text font="3" height="12" left="81" textpieces="0" top="1014" width="359">struct this probability distribution, the &#64257;rst step is ground-</text>
<text font="1" height="8" left="81" textpieces="1" top="1037" width="359">3Clausal form is a disjunction of positive or negative literals.</text>
<text font="3" height="12" left="81" textpieces="0" top="1053" width="359">For example, the rule is R(a) =&gt; R(b) is not in clausal form,</text>
<text font="3" height="12" left="81" textpieces="0" top="1067" width="358">but is equivalent to &#172;R(a) &#8744; R(b), which is in clausal form.</text>
<text font="3" height="12" left="475" textpieces="2" top="253" width="359">ing: given a formula F with free variables &#175; x = (x1, &#183; &#183; &#183; , xm),</text>
<text font="3" height="12" left="475" textpieces="1" top="268" width="311">then for each &#175; d &#8712; Dm, we create a new formula g</text>
<text font="1" height="8" left="789" textpieces="2" top="272" width="45">&#175;  d  called</text>
<text font="3" height="12" left="475" textpieces="0" top="284" width="152">a ground clause where g</text>
<text font="1" height="8" left="629" textpieces="2" top="288" width="205">&#175;  d  denotes the result of substitut-</text>
<text font="3" height="12" left="475" textpieces="2" top="300" width="358">ing each variable xi of F with di.  For example, for F3</text>
<text font="3" height="12" left="475" textpieces="1" top="316" width="358">the variables are {p1, p2, c}: one tuple of constants is &#175; d =</text>
<text font="3" height="12" left="475" textpieces="0" top="331" width="259">(&#8216;P1&#8217;, &#8216;P2&#8217;, &#8216;DB&#8217;) and the ground formula f</text>
<text font="1" height="8" left="736" textpieces="2" top="335" width="22">&#175;  d is:</text>
<text font="3" height="11" left="493" textpieces="0" top="358" width="323">cat(&#8216;P1&#8217;, &#8216;DB&#8217;), refers(&#8216;P1&#8217;, &#8216;P2&#8217;) =&gt; cat(&#8216;P2&#8217;, &#8216;DB&#8217;)</text>
<text font="3" height="12" left="480" textpieces="0" top="383" width="354">Each constituent in the ground formula, such as cat(&#8216;P1&#8217;,</text>
<text font="3" height="12" left="475" textpieces="0" top="399" width="359">&#8216;DB&#8217;) and refers(&#8216;P1&#8217;, &#8216;P2&#8217;), is called a ground predicate</text>
<text font="3" height="12" left="475" textpieces="1" top="414" width="359">or atom for short. In the worst case there are D3 ground</text>
<text font="3" height="12" left="475" textpieces="1" top="430" width="359">clauses for F3. For each formula Fi (for i = 1 . . . N ), we per-</text>
<text font="3" height="12" left="475" textpieces="0" top="446" width="359">form the above process. Each ground clause g of a formula</text>
<text font="3" height="12" left="475" textpieces="1" top="461" width="359">Fi is assigned the same weight, wi. So, a ground clause of</text>
<text font="3" height="12" left="475" textpieces="2" top="477" width="359">F1 has weight 5, while any ground clause of F2 has weight</text>
<text font="3" height="12" left="475" textpieces="1" top="493" width="359">1. We denote by G = (&#175; g, w) the set of all ground clauses of</text>
<text font="3" height="12" left="478" textpieces="1" top="505" width="356">&#175; F and a function w that maps each ground clause to its as-</text>
<text font="3" height="14" left="475" textpieces="1" top="524" width="359">signed weight. Fix an MLN &#175; F , then for any possible world</text>
<text font="3" height="12" left="475" textpieces="0" top="540" width="359">(instance) I we say a ground clause g is violated if w(g) &gt; 0</text>
<text font="3" height="12" left="475" textpieces="0" top="556" width="359">and g is false in I or if w(g) &lt; 0 and g is true in I. We</text>
<text font="3" height="12" left="475" textpieces="0" top="571" width="359">denote the set of ground clauses violated in a world I as</text>
<text font="3" height="12" left="475" textpieces="0" top="587" width="194">V (I). The cost of the world I is</text>
<text font="3" height="12" left="587" textpieces="0" top="616" width="56">cost(I) =</text>
<text font="1" height="8" left="647" textpieces="0" top="636" width="36">g&#8712;V (I)</text>
<text font="3" height="13" left="686" textpieces="1" top="615" width="148">|w(g)|                 (1)</text>
<text font="3" height="14" left="475" textpieces="0" top="657" width="359">Through cost, an MLN de&#64257;nes a probability distribution</text>
<text font="3" height="12" left="475" textpieces="0" top="673" width="215">over all instances (denoted Inst) as:</text>
<text font="3" height="12" left="475" textpieces="0" top="702" width="57">Pr[I] = Z</text>
<text font="1" height="8" left="534" textpieces="1" top="698" width="179">&#8722;1 exp {&#8722;cost(I)} where Z =</text>
<text font="1" height="8" left="716" textpieces="0" top="721" width="33">J &#8712;Inst</text>
<text font="3" height="12" left="752" textpieces="0" top="702" width="92">exp {&#8722;cost(J )}</text>
<text font="3" height="12" left="475" textpieces="0" top="741" width="359">A lowest cost world I is called a most likely world. Since</text>
<text font="3" height="12" left="475" textpieces="0" top="757" width="359">cost(I) &#8805; 0, if cost(I) = 0 then I is a most likely world. On</text>
<text font="3" height="12" left="475" textpieces="0" top="773" width="359">the other hand the most likely world may have positive cost.</text>
<text font="3" height="14" left="475" textpieces="0" top="788" width="359">There are two main types of inference with MLNs: MAP</text>
<text font="3" height="12" left="475" textpieces="0" top="804" width="359">(maximum a posteriori) inference, where we want to &#64257;nd a</text>
<text font="3" height="12" left="475" textpieces="0" top="820" width="359">most likely world, and marginal inference, where we want to</text>
<text font="3" height="14" left="475" textpieces="0" top="835" width="359">compute marginal probabilities. Tuffy is capable of both</text>
<text font="3" height="12" left="475" textpieces="0" top="851" width="359">types of inference, but we present only MAP inference in the</text>
<text font="3" height="12" left="475" textpieces="0" top="867" width="359">body of this paper. We refer the reader to Appendix A.5 for</text>
<text font="3" height="12" left="475" textpieces="0" top="882" width="174">details of marginal inference.</text>
<text font="2" height="16" left="475" textpieces="1" top="908" width="113">2.3  Inference</text>
<text font="3" height="14" left="489" textpieces="0" top="931" width="345">We now describe the state of the art of inference for MLNs</text>
<text font="3" height="14" left="475" textpieces="0" top="946" width="330">(as in Alchemy, the reference MLN implementation).</text>
<text font="5" height="14" left="475" textpieces="1" top="971" width="359">Grounding. Conceptually, to obtain the ground clauses of</text>
<text font="3" height="14" left="475" textpieces="0" top="988" width="359">an MLN formula F , the most straightforward way is to</text>
<text font="3" height="12" left="475" textpieces="0" top="1004" width="359">enumerate all possible assignments to the free variables in</text>
<text font="3" height="12" left="475" textpieces="0" top="1020" width="359">F . There have been several heuristics in the literature that</text>
<text font="3" height="12" left="475" textpieces="0" top="1035" width="359">improve the grounding process by pruning groundings that</text>
<text font="3" height="12" left="475" textpieces="0" top="1051" width="359">have no e&#64256;ect on inference results; we describe the heuristics</text>
<text font="3" height="14" left="475" textpieces="0" top="1067" width="359">that Tuffy (and Alchemy) implements in Appendix A.3.</text>
<text font="3" height="12" left="81" textpieces="0" top="86" width="359">The set of ground clauses corresponds to a hypergraph where</text>
<text font="3" height="12" left="81" textpieces="0" top="102" width="359">each atom is a node and each clause is a hyperedge. This</text>
<text font="3" height="12" left="81" textpieces="0" top="118" width="370">graph structure is often called a Markov Random Field (MRF).</text>
<text font="3" height="12" left="81" textpieces="0" top="133" width="323">We describe this structure formally in Appendix A.2.</text>
<text font="5" height="14" left="81" textpieces="1" top="165" width="358">Search. Finding a most likely world of an MLN is a general-</text>
<text font="3" height="12" left="81" textpieces="0" top="183" width="359">ization of the (NP-hard) MaxSAT problem. In this paper we</text>
<text font="3" height="12" left="81" textpieces="0" top="198" width="359">concentrate on one of the most popular heuristic search al-</text>
<text font="3" height="14" left="81" textpieces="0" top="214" width="359">gorithms, WalkSAT [13], which is used by Alchemy. Walk-</text>
<text font="3" height="12" left="81" textpieces="0" top="230" width="359">SAT works by repeatedly selecting a random violated clause</text>
<text font="3" height="12" left="81" textpieces="0" top="245" width="359">and &#8220;&#64257;xing&#8221; it by &#64258;ipping (i.e., changing the truth value of)</text>
<text font="3" height="12" left="81" textpieces="0" top="261" width="359">an atom in it (see Appendix A.4). As with any heuristic</text>
<text font="3" height="12" left="81" textpieces="0" top="277" width="359">search, we cannot be sure that we have achieved the op-</text>
<text font="3" height="12" left="81" textpieces="0" top="293" width="359">timal, and so the goal of any system that executes such a</text>
<text font="3" height="12" left="81" textpieces="0" top="308" width="359">search procedure is: execute more search steps in the same</text>
<text font="3" height="12" left="81" textpieces="0" top="324" width="97">amount of time.</text>
<text font="5" height="14" left="81" textpieces="1" top="356" width="359">Problem Description. The primary challenge that we ad-</text>
<text font="3" height="12" left="81" textpieces="0" top="373" width="359">dress in this paper is scaling both phases of MAP inference</text>
<text font="3" height="12" left="81" textpieces="0" top="389" width="359">algorithms, grounding and search, using an RDBMS. Sec-</text>
<text font="3" height="12" left="81" textpieces="0" top="405" width="359">ond, our goal is to improve the number of (e&#64256;ective) steps</text>
<text font="3" height="12" left="81" textpieces="0" top="420" width="359">of the local search procedure using parallelism and partition-</text>
<text font="3" height="12" left="81" textpieces="0" top="436" width="359">ing &#8211; but only when it provably improves the search quality.</text>
<text font="3" height="12" left="81" textpieces="0" top="452" width="359">To achieve these goals, we attack three main technical chal-</text>
<text font="3" height="14" left="81" textpieces="0" top="467" width="359">lenges: (1) e&#64259;ciently grounding large MLNs, (2) e&#64259;ciently</text>
<text font="3" height="14" left="81" textpieces="0" top="483" width="359">performing inference (search) on large MLNs, and (3) de-</text>
<text font="3" height="12" left="81" textpieces="0" top="499" width="359">signing partitioning and partition-aware search algorithms</text>
<text font="3" height="12" left="81" textpieces="0" top="514" width="317">that preserve (or enhance) search quality and speed.</text>
<text font="2" height="16" left="81" textpieces="1" top="554" width="179">3.  TUFFY SYSTEMS</text>
<text font="3" height="12" left="94" textpieces="0" top="577" width="345">In this section, we describe our technical contributions: a</text>
<text font="3" height="12" left="81" textpieces="0" top="592" width="359">bottom-up grounding approach to fully leverage the RDBMS</text>
<text font="3" height="12" left="81" textpieces="0" top="608" width="359">(Section 3.1); a hybrid main-memory RDBMS architecture</text>
<text font="3" height="12" left="81" textpieces="0" top="624" width="359">to support e&#64259;cient end-to-end inference (Section 3.2); and</text>
<text font="3" height="14" left="81" textpieces="0" top="639" width="364">data partitioning which dramatically improves Tuffy&#8217;s space</text>
<text font="3" height="12" left="81" textpieces="0" top="655" width="296">and time e&#64259;ciency (Section 3.3 and Section 3.4).</text>
<text font="2" height="16" left="81" textpieces="1" top="683" width="344">3.1  Grounding with a Bottom-up Approach</text>
<text font="3" height="14" left="94" textpieces="0" top="706" width="345">We describe how Tuffy performs grounding. In con-</text>
<text font="3" height="12" left="81" textpieces="0" top="721" width="359">trast to top-down approaches (similar to Prolog) that em-</text>
<text font="3" height="12" left="81" textpieces="0" top="737" width="359">ploy nested loops and that is used by prior MLN systems</text>
<text font="3" height="14" left="81" textpieces="0" top="753" width="359">such as Alchemy, Tu&#64256;y takes a bottom-up approach (sim-</text>
<text font="3" height="12" left="81" textpieces="0" top="768" width="359">ilar to Datalog) by expressing grounding as a sequence of</text>
<text font="3" height="12" left="81" textpieces="0" top="784" width="359">SQL queries. Each SQL query is optimized by the RDBMS,</text>
<text font="3" height="14" left="81" textpieces="0" top="800" width="359">which allows Tuffy to complete the grounding process or-</text>
<text font="3" height="12" left="81" textpieces="0" top="816" width="332">ders of magnitude more quickly than prior approaches.</text>
<text font="3" height="12" left="94" textpieces="1" top="831" width="346">For each predicate P ( &#175; A) in the input MLN, Tuffy creates</text>
<text font="3" height="12" left="81" textpieces="3" top="847" width="359">a relation RP(aid, &#175;  A, truth) where each row ap represents</text>
<text font="3" height="12" left="81" textpieces="1" top="863" width="359">an atom, aid is a globally unique identi&#64257;er, &#175; A is the tuple</text>
<text font="3" height="12" left="81" textpieces="0" top="878" width="359">of arguments of P , and truth is a three-valued attribute</text>
<text font="3" height="12" left="81" textpieces="1" top="894" width="358">that indicates if apis true or false (in the evidence), or not</text>
<text font="3" height="12" left="81" textpieces="0" top="910" width="359">speci&#64257;ed in the evidence. These tables form the input to</text>
<text font="3" height="14" left="81" textpieces="0" top="925" width="359">grounding, and Tuffy constructs them using standard bulk-</text>
<text font="3" height="12" left="81" textpieces="0" top="941" width="115">loading techniques.</text>
<text font="3" height="14" left="94" textpieces="0" top="957" width="345">In Tuffy, we produce an output table C(cid, lits, weight)</text>
<text font="3" height="12" left="81" textpieces="0" top="972" width="359">where each row corresponds to a single ground clause. Here,</text>
<text font="3" height="12" left="81" textpieces="0" top="988" width="359">cid is the id of a ground clause, lits is an array that stores</text>
<text font="3" height="12" left="81" textpieces="0" top="1004" width="359">the atom id of each literal in this clause (and whether or</text>
<text font="3" height="12" left="81" textpieces="0" top="1020" width="359">not it is negated), and weight is the weight of this clause.</text>
<text font="3" height="12" left="81" textpieces="0" top="1035" width="359">We &#64257;rst consider a formula without existential quanti&#64257;ers.</text>
<text font="3" height="12" left="81" textpieces="2" top="1051" width="358">In this case, the formula F can be written as F (&#175; x) = l1&#8744;</text>
<text font="3" height="13" left="81" textpieces="2" top="1066" width="358">&#183; &#183; &#183; &#8744; lN where &#175; x are all variables in F . Tuffy produces a</text>
<text font="3" height="12" left="475" textpieces="0" top="86" width="359">SQL query Q for F that joins together the relations corre-</text>
<text font="3" height="12" left="475" textpieces="0" top="102" width="359">sponding to the predicates in F to produce the atom ids of</text>
<text font="3" height="12" left="475" textpieces="0" top="118" width="359">the ground clauses (and whether or not they are negated).</text>
<text font="3" height="12" left="475" textpieces="0" top="133" width="359">The join conditions in Q enforce variable equality inside</text>
<text font="3" height="12" left="475" textpieces="0" top="149" width="359">F , and incorporate the pruning strategies described in Ap-</text>
<text font="3" height="12" left="475" textpieces="0" top="165" width="359">pendix A.3. For more details on the compilation procedure</text>
<text font="3" height="12" left="475" textpieces="0" top="180" width="110">see Appendix B.1.</text>
<text font="2" height="16" left="475" textpieces="1" top="224" width="319">3.2  A Hybrid Architecture for Inference</text>
<text font="3" height="12" left="489" textpieces="0" top="247" width="345">Our initial prototype of Tu&#64256;y runs both grounding and</text>
<text font="3" height="12" left="475" textpieces="0" top="263" width="359">search in the RDBMS. While the grounding phase described</text>
<text font="3" height="12" left="475" textpieces="0" top="278" width="359">in the previous section has good performance and scalabil-</text>
<text font="3" height="12" left="475" textpieces="0" top="294" width="359">ity, we found that performing search in an RDBMS is often</text>
<text font="3" height="12" left="475" textpieces="0" top="310" width="359">a bottleneck. Thus, we design a hybrid architecture that</text>
<text font="3" height="12" left="475" textpieces="0" top="325" width="359">allows e&#64259;cient in-memory search (in Java) while retaining</text>
<text font="3" height="12" left="475" textpieces="0" top="341" width="359">the performance bene&#64257;ts of RDBMS-based grounding. To</text>
<text font="3" height="12" left="475" textpieces="0" top="357" width="359">see why in-memory search is critical, recall that WalkSAT</text>
<text font="3" height="12" left="475" textpieces="0" top="372" width="359">works by selecting an unsatis&#64257;ed clause C, selecting an atom</text>
<text font="3" height="12" left="475" textpieces="0" top="388" width="359">in C, and &#8220;&#64258;ipping&#8221; that atom to satisfy C. Thus, Walk-</text>
<text font="3" height="12" left="475" textpieces="0" top="404" width="359">SAT performs a large number of random accesses to the</text>
<text font="3" height="12" left="475" textpieces="0" top="420" width="359">data representing ground clauses and atoms. Moreover, the</text>
<text font="3" height="12" left="475" textpieces="0" top="435" width="359">data that is accessed in one iteration depends on the data</text>
<text font="3" height="12" left="475" textpieces="0" top="451" width="359">that is accessed in the previous iteration. And so, this ac-</text>
<text font="3" height="12" left="475" textpieces="0" top="467" width="359">cess pattern prevents both e&#64256;ective caching and parallelism,</text>
<text font="3" height="12" left="475" textpieces="0" top="482" width="359">which causes a high overhead per data access. Thus, we</text>
<text font="3" height="12" left="475" textpieces="0" top="498" width="359">implement a hybrid architecture where the RDBMS per-</text>
<text font="3" height="14" left="475" textpieces="0" top="514" width="359">forms grounding and Tuffy is able to read the result of</text>
<text font="3" height="12" left="475" textpieces="0" top="529" width="359">grounding from the RDBMS into memory and perform in-</text>
<text font="3" height="12" left="475" textpieces="0" top="545" width="359">ference. If the grounding result is too large to &#64257;t in memory,</text>
<text font="3" height="12" left="475" textpieces="0" top="563" width="359">Tuffy invokes an implementation of search directly inside</text>
<text font="3" height="12" left="475" textpieces="0" top="576" width="359">the RDBMS (Appendix B.2). This approach is much less ef-</text>
<text font="3" height="12" left="475" textpieces="0" top="592" width="359">&#64257;cient than in-memory search, but it runs on datasets larger</text>
<text font="3" height="12" left="475" textpieces="0" top="608" width="359">than main memory without crashing. Appendix B.3 illus-</text>
<text font="3" height="14" left="475" textpieces="0" top="624" width="291">trates the architecture of Tuffy in more detail.</text>
<text font="3" height="12" left="489" textpieces="0" top="639" width="345">While it is clear that this hybrid approach is at least</text>
<text font="3" height="12" left="475" textpieces="0" top="655" width="359">as scalable as a direct memory implementation (such as</text>
<text font="3" height="12" left="475" textpieces="0" top="673" width="359">Alchemy), there are in fact cases where Tuffy can run</text>
<text font="3" height="14" left="475" textpieces="0" top="686" width="359">in-memory search whereas Alchemy would crash. The rea-</text>
<text font="3" height="12" left="475" textpieces="0" top="702" width="359">son is that the space requirement of a purely in-memory im-</text>
<text font="3" height="12" left="475" textpieces="0" top="718" width="359">plementation is determined by the peak memory footprint</text>
<text font="3" height="14" left="475" textpieces="0" top="733" width="359">throughout grounding and search, whereas Tuffy needs</text>
<text font="3" height="12" left="475" textpieces="0" top="749" width="359">main memory only for search. For example, on a dataset</text>
<text font="3" height="14" left="475" textpieces="0" top="765" width="359">called Relational Classi&#64257;cation (RC), Alchemy allocated</text>
<text font="3" height="12" left="475" textpieces="0" top="780" width="359">2.8 GB of RAM only to produce 4.8 MB of ground clauses.</text>
<text font="3" height="14" left="475" textpieces="0" top="796" width="256">On RC, Tuffy uses only 19 MB of RAM.</text>
<text font="2" height="16" left="475" textpieces="1" top="840" width="326">3.3  Partitioning to Improve Performance</text>
<text font="3" height="12" left="489" textpieces="0" top="863" width="345">In the following two sections, we study how to further im-</text>
<text font="3" height="14" left="475" textpieces="0" top="878" width="359">prove Tuffy&#8217;s space and time e&#64259;ciency without sacri&#64257;cing</text>
<text font="3" height="12" left="475" textpieces="0" top="894" width="359">its scalability. The underlying idea is simple: we will try</text>
<text font="3" height="12" left="475" textpieces="0" top="910" width="359">to partition the data. By splitting the problem into smaller</text>
<text font="3" height="12" left="475" textpieces="0" top="925" width="359">pieces, we can reduce the memory footprint and introduce</text>
<text font="3" height="12" left="475" textpieces="0" top="941" width="359">parallelism, which conceptually breaks the sequential nature</text>
<text font="3" height="12" left="475" textpieces="0" top="957" width="359">of the search. These are expected bene&#64257;ts of partitioning.</text>
<text font="3" height="12" left="475" textpieces="0" top="972" width="359">An unexpected bene&#64257;t is an exponentially increase of the</text>
<text font="3" height="12" left="475" textpieces="0" top="988" width="333">e&#64256;ective search speed, a point that we return to below.</text>
<text font="3" height="14" left="489" textpieces="0" top="1004" width="345">First, observe that the logical forms of MLNs often re-</text>
<text font="3" height="12" left="475" textpieces="0" top="1020" width="359">sult in an MRF with multiple disjoint components (see Ap-</text>
<text font="3" height="12" left="475" textpieces="0" top="1035" width="359">pendix B.4). For example, on the RC dataset there are 489</text>
<text font="3" height="12" left="475" textpieces="1" top="1051" width="364">components. Let G be an MRF with components G1, &#183; &#183; &#183; , Gk;</text>
<text font="3" height="12" left="475" textpieces="1" top="1067" width="359">let I be a truth assignment to the atoms in G and Iiits pro-</text>
<text font="3" height="12" left="81" textpieces="0" top="86" width="239">jection over Gi. Then, it&#8217;s clear that &#8704;I</text>
<text font="3" height="12" left="176" textpieces="0" top="114" width="65">costG(I) =</text>
<text font="1" height="8" left="246" textpieces="0" top="133" width="33">1&#8804;i&#8804;k</text>
<text font="3" height="12" left="281" textpieces="1" top="114" width="63">costGi(Ii).</text>
<text font="3" height="12" left="81" textpieces="0" top="156" width="359">Hence, instead of minimizing costG(I) directly, it su&#64259;ces</text>
<text font="3" height="12" left="81" textpieces="1" top="171" width="359">to minimize each individual costGi(Ii). The bene&#64257;t is that,</text>
<text font="3" height="12" left="81" textpieces="0" top="187" width="359">even if G itself does not &#64257;t in memory, it is possible that</text>
<text font="3" height="12" left="81" textpieces="2" top="203" width="358">each Gidoes. As such, we can solve each Giwith in-memory</text>
<text font="3" height="12" left="81" textpieces="1" top="218" width="357">search one by one, and &#64257;nally merge the results together. 4</text>
<text font="3" height="12" left="94" textpieces="0" top="234" width="345">Component detection is done after the grounding phase</text>
<text font="3" height="12" left="81" textpieces="0" top="250" width="359">and before the search phase, as follows. We maintain an in-</text>
<text font="3" height="12" left="81" textpieces="0" top="266" width="359">memory union-&#64257;nd structure over the nodes, and scan the</text>
<text font="3" height="12" left="81" textpieces="0" top="281" width="359">clause table while updating this union-&#64257;nd structure. The</text>
<text font="3" height="12" left="81" textpieces="0" top="297" width="359">result is the set of connected components in the MRF. An</text>
<text font="3" height="12" left="81" textpieces="0" top="313" width="335">immediate issue raised by partitioning is I/O e&#64259;ciency.</text>
<text font="5" height="14" left="81" textpieces="1" top="345" width="359">Ef&#64257;cient Data Loading. Once an MRF is split into compo-</text>
<text font="3" height="12" left="81" textpieces="0" top="362" width="359">nents, loading in and running inference on each component</text>
<text font="3" height="12" left="81" textpieces="0" top="378" width="359">sequentially one by one may incur many I/O operations,</text>
<text font="3" height="12" left="81" textpieces="0" top="393" width="359">as there may be many partitions. For example, the MRF</text>
<text font="3" height="12" left="81" textpieces="0" top="409" width="359">of the Information Extraction (IE) dataset contains thou-</text>
<text font="3" height="12" left="81" textpieces="0" top="425" width="359">sands of 2-cliques and 3-cliques. One solution is to group</text>
<text font="3" height="12" left="81" textpieces="0" top="440" width="359">the components into batches. The goal is to minimize the</text>
<text font="3" height="12" left="81" textpieces="0" top="456" width="359">total number of batches (and thereby the I/O cost of load-</text>
<text font="3" height="12" left="81" textpieces="0" top="472" width="359">ing), and the constraint is that each batch cannot exceed the</text>
<text font="3" height="12" left="81" textpieces="0" top="487" width="359">memory budget. This is essentially the bin packing problem,</text>
<text font="3" height="12" left="81" textpieces="0" top="503" width="359">and we implement the First Fit Decreasing algorithm [26].</text>
<text font="3" height="12" left="81" textpieces="0" top="519" width="359">Once the partitions are in memory, we can take advantage</text>
<text font="3" height="12" left="81" textpieces="0" top="535" width="332">of parallelism. We use a round-robin scheduling policy.</text>
<text font="5" height="14" left="81" textpieces="1" top="566" width="358">Improving Search Speed using Partitioning. Although</text>
<text font="3" height="12" left="81" textpieces="0" top="584" width="359">processing each component individually produces solutions</text>
<text font="3" height="12" left="81" textpieces="0" top="600" width="359">that are no worse than processing the whole graph at once,</text>
<text font="3" height="12" left="81" textpieces="0" top="615" width="359">we give an example to illustrate that component-aware pro-</text>
<text font="3" height="12" left="81" textpieces="0" top="631" width="349">cessing may result in exponentially faster speed of search.</text>
<text font="3" height="12" left="81" textpieces="0" top="661" width="359">Example 1 Consider an MRF consisting of N identical</text>
<text font="3" height="12" left="81" textpieces="0" top="677" width="357">connected components each containing two atoms {Xi, Yi}</text>
<text font="3" height="12" left="81" textpieces="0" top="692" width="161">and three weighted clauses</text>
<text font="3" height="13" left="167" textpieces="2" top="716" width="185">{(Xi, 1), (Yi, 1), (Xi&#8744; Yi, &#8722;1)},</text>
<text font="3" height="12" left="81" textpieces="0" top="742" width="359">where i = 1 . . . N and the second component of each tuple</text>
<text font="3" height="12" left="81" textpieces="0" top="758" width="359">is the weight. Based on how WalkSAT works, it&#8217;s not hard</text>
<text font="3" height="12" left="81" textpieces="0" top="774" width="359">to show that, if N = 1, starting from a random state, the</text>
<text font="3" height="12" left="81" textpieces="3" top="789" width="359">expected hitting time5 of the optimal state, i.e. X1= Y1=</text>
<text font="3" height="12" left="81" textpieces="0" top="805" width="359">T rue, is no more than 4. Therefore, if we run WalkSAT on</text>
<text font="3" height="12" left="81" textpieces="0" top="821" width="359">each component separately, the expected runtime of reach-</text>
<text font="3" height="12" left="81" textpieces="0" top="836" width="359">ing the optimum is no more than 4N . Now consider the</text>
<text font="3" height="12" left="81" textpieces="0" top="852" width="359">case where we run WalkSAT on the whole MRF. Intuitively,</text>
<text font="3" height="12" left="81" textpieces="0" top="868" width="359">reaching the optimal state requires &#8220;&#64257;xing&#8221; suboptimal com-</text>
<text font="3" height="12" left="81" textpieces="0" top="883" width="359">ponents one by one. As the number of optimal components</text>
<text font="3" height="12" left="81" textpieces="0" top="899" width="359">increases, however, it becomes more and more likely that</text>
<text font="3" height="12" left="81" textpieces="0" top="915" width="359">one step of WalkSAT &#8220;breaks&#8221; an optimal component in-</text>
<text font="3" height="12" left="81" textpieces="0" top="930" width="359">stead of &#64257;xing a suboptimal component. Such check and</text>
<text font="3" height="12" left="81" textpieces="0" top="946" width="359">balance makes it very di&#64259;cult for WalkSAT to reach the</text>
<text font="1" height="8" left="81" textpieces="1" top="968" width="359">4Alchemy exploits knowledge-based model construction</text>
<text font="3" height="12" left="81" textpieces="0" top="984" width="359">(KBMC) [28] to &#64257;nd the minimal subgraph of the MRF that</text>
<text font="3" height="14" left="81" textpieces="0" top="997" width="359">is needed for a given query. Alchemy, however, does not</text>
<text font="3" height="12" left="81" textpieces="0" top="1011" width="359">use the fact that the MRF output by KBMC may contain</text>
<text font="3" height="12" left="81" textpieces="0" top="1024" width="121">several components.</text>
<text font="1" height="8" left="81" textpieces="1" top="1037" width="359">5The hitting time is a standard notion from Markov</text>
<text font="3" height="12" left="81" textpieces="0" top="1053" width="359">Chains [9], it is a random variable for the number of steps</text>
<text font="3" height="12" left="81" textpieces="0" top="1067" width="355">taken by WalkSAT to reach an optimum for the &#64257;rst time.</text>
<text font="3" height="12" left="475" textpieces="0" top="86" width="359">optimum. Indeed, Appendix B.5 shows that the expected</text>
<text font="3" height="12" left="475" textpieces="1" top="102" width="292">hitting time is at least 2N &#8211; an exponential gap!</text>
<text font="3" height="12" left="475" textpieces="1" top="134" width="363">Let G be an MRF with components G1, . . . , GN. Component-</text>
<text font="3" height="12" left="475" textpieces="0" top="150" width="358">aware WalkSAT runs WalkSAT except that for each Gi, it</text>
<text font="3" height="12" left="475" textpieces="0" top="166" width="359">keeps track of the lowest-cost state it has found so far on</text>
<text font="3" height="12" left="475" textpieces="0" top="181" width="358">that Gi. In contrast, regular WalkSAT simply keeps the</text>
<text font="3" height="12" left="475" textpieces="0" top="197" width="359">best overall solution it has seen so far. For i = 1, . . . , N ,</text>
<text font="3" height="12" left="475" textpieces="2" top="213" width="359">let Oi be the set of optimal states of Gi, and Si the set of</text>
<text font="3" height="12" left="475" textpieces="1" top="229" width="359">non-optimal states of Gi that di&#64256;er only by one bit from</text>
<text font="3" height="12" left="475" textpieces="2" top="244" width="359">some x&#8727;&#8712; Oi; let Pi(x &#8594; y) be the transition probability of</text>
<text font="3" height="12" left="475" textpieces="0" top="260" width="358">WalkSAT running on Gi, i.e., the probability that one step</text>
<text font="3" height="12" left="475" textpieces="1" top="276" width="359">of WalkSAT would take Gifrom x to y. Given x, a state of</text>
<text font="3" height="12" left="475" textpieces="1" top="291" width="359">Gi, denote by vi(x) the number of violated clauses in Gi at</text>
<text font="3" height="12" left="475" textpieces="0" top="307" width="85">state x; de&#64257;ne</text>
<text font="3" height="12" left="509" textpieces="0" top="336" width="46">&#945;i(x) =</text>
<text font="1" height="8" left="559" textpieces="0" top="355" width="26">y&#8712;Oi</text>
<text font="3" height="12" left="588" textpieces="0" top="336" width="115">Pi(x &#8594; y), &#946;i(x) =</text>
<text font="1" height="8" left="708" textpieces="0" top="355" width="24">y&#8712;Si</text>
<text font="3" height="12" left="736" textpieces="0" top="336" width="64">Pi(x &#8594; y).</text>
<text font="3" height="12" left="475" textpieces="0" top="378" width="301">For any non-empty subset H &#8838; {1, . . . , N }, de&#64257;ne</text>
<text font="3" height="12" left="544" textpieces="0" top="412" width="44">r(H) =</text>
<text font="3" height="12" left="596" textpieces="3" top="404" width="162">mini&#8712;Hminx&#8712;Oivi(x)&#946;i(x)</text>
<text font="3" height="12" left="594" textpieces="2" top="421" width="165">maxi&#8712;Hmaxx&#8712;Sivi(x)&#945;i(x)</text>
<text font="3" height="12" left="762" textpieces="0" top="412" width="4">.</text>
<text font="3" height="12" left="490" textpieces="0" top="455" width="344">Theorem 3.1. Let H be an arbitrary non-empty subset of</text>
<text font="3" height="13" left="475" textpieces="0" top="467" width="359">{1, . . . , N } s.t. |H| &#8805; 2 and r = r(H) &gt; 0. Then, in expec-</text>
<text font="3" height="12" left="475" textpieces="1" top="485" width="359">tation, WalkSAT on G takes at least 2|H|r/(2+r) more steps</text>
<text font="3" height="12" left="475" textpieces="0" top="501" width="359">to &#64257;nd an optimal solution than component-aware WalkSAT.</text>
<text font="3" height="12" left="475" textpieces="0" top="533" width="359">The proof is in Appendix B.5. In the worst case, there is</text>
<text font="3" height="12" left="475" textpieces="0" top="549" width="359">only one component, or r(H) = 0 for every subset of compo-</text>
<text font="3" height="12" left="475" textpieces="0" top="565" width="359">nents H (which happens only if there is a zero-cost solution),</text>
<text font="3" height="12" left="475" textpieces="0" top="580" width="359">and partitioning would become pure overhead (but negli-</text>
<text font="3" height="12" left="475" textpieces="1" top="596" width="358">gible in our experiments).  On an information extraction</text>
<text font="3" height="12" left="475" textpieces="0" top="612" width="359">(IE) benchmark dataset, there is some H with |H| = 1196</text>
<text font="3" height="12" left="475" textpieces="0" top="628" width="359">and r(H) = 0.5. Thus, the gap on this dataset is at least</text>
<text font="3" height="12" left="475" textpieces="1" top="643" width="358">2200&#8776; 1060. This explains why Tuffy produces lower cost</text>
<text font="3" height="12" left="475" textpieces="0" top="659" width="359">solutions in minutes than non-partition aware approaches</text>
<text font="3" height="14" left="475" textpieces="0" top="675" width="260">such as Alchemy produce even after days.</text>
<text font="2" height="16" left="475" textpieces="1" top="701" width="251">3.4  Further Partitioning MRFs</text>
<text font="3" height="12" left="489" textpieces="0" top="724" width="345">Although our algorithms are more scalable than prior ap-</text>
<text font="3" height="12" left="475" textpieces="0" top="739" width="359">proaches, if the largest component does not &#64257;t in memory</text>
<text font="3" height="12" left="475" textpieces="0" top="755" width="359">then we are forced to run the in-RDBMS version of inference,</text>
<text font="3" height="12" left="475" textpieces="0" top="771" width="359">which is ine&#64259;cient. Intuitively, if the graph is only weakly</text>
<text font="3" height="12" left="475" textpieces="0" top="786" width="359">connected, then we should still be able to get the exponential</text>
<text font="3" height="12" left="475" textpieces="0" top="802" width="352">speed up of partitioning. Consider the following example.</text>
<text font="7" height="15" left="722" textpieces="1" top="852" width="91">G1         G2 </text>
<text font="3" height="13" left="763" textpieces="0" top="845" width="10">e </text>
<text font="3" height="13" left="744" textpieces="1" top="857" width="48">a      b </text>
<text font="3" height="12" left="713" textpieces="0" top="903" width="106">Figure 2: Ex. 2</text>
<text font="3" height="12" left="475" textpieces="0" top="831" width="187">Example 2 Consider an MRF</text>
<text font="3" height="12" left="475" textpieces="0" top="847" width="187">consisting of two equally sized</text>
<text font="3" height="12" left="475" textpieces="1" top="863" width="186">subgraphs G1 and G2, plus an</text>
<text font="3" height="12" left="475" textpieces="0" top="878" width="187">edge e = (a, b) between them</text>
<text font="3" height="12" left="475" textpieces="1" top="894" width="188">(Figure 2).  Suppose that the</text>
<text font="3" height="12" left="475" textpieces="0" top="910" width="187">expected hitting time of Walk-</text>
<text font="3" height="12" left="475" textpieces="2" top="925" width="187">SAT on Giis Hi. Since H1and</text>
<text font="3" height="12" left="475" textpieces="1" top="941" width="359">H2are essentially independent, the hitting time of WalkSAT</text>
<text font="3" height="12" left="475" textpieces="0" top="957" width="169">on G could be roughly H1H</text>
<text font="1" height="8" left="646" textpieces="0" top="961" width="187">2. On the other hand, consider</text>
<text font="3" height="12" left="475" textpieces="0" top="972" width="359">the following scheme: enumerate all possible truth assign-</text>
<text font="3" height="12" left="475" textpieces="0" top="988" width="359">ments to one of the boundary variables {a, b}, say a &#8211; of</text>
<text font="3" height="12" left="475" textpieces="0" top="1004" width="359">which there are two &#8211; and conditioning on each assignment,</text>
<text font="3" height="12" left="475" textpieces="2" top="1020" width="359">run WalkSAT on G1 and G2 independently. Clearly, the</text>
<text font="3" height="12" left="475" textpieces="1" top="1035" width="358">overall hitting time is no more than 2(H1+ H2), which is</text>
<text font="3" height="12" left="475" textpieces="0" top="1051" width="186">a huge improvement over H1H</text>
<text font="1" height="8" left="663" textpieces="2" top="1056" width="171">2  since Hi is usually a high-</text>
<text font="3" height="12" left="475" textpieces="0" top="1067" width="331">order polynomial or even exponential in the size of Gi.</text>
<text font="3" height="12" left="94" textpieces="0" top="86" width="345">To capitalize on this idea, we need to address two chal-</text>
<text font="3" height="12" left="81" textpieces="0" top="102" width="359">lenges: 1) designing an e&#64259;cient MRF partitioning algo-</text>
<text font="3" height="12" left="81" textpieces="0" top="118" width="359">rithm; and 2) designing an e&#64256;ective partition-aware search</text>
<text font="3" height="12" left="81" textpieces="0" top="133" width="270">algorithm. We address each of them in turn.</text>
<text font="5" height="14" left="81" textpieces="1" top="165" width="359">MRF Partitioning. Intuitively, to maximally utilize the</text>
<text font="3" height="12" left="81" textpieces="0" top="183" width="359">memory budget, we want to partition the MRF into roughly</text>
<text font="3" height="12" left="81" textpieces="0" top="198" width="359">equal sizes; to minimize information loss, we want to min-</text>
<text font="3" height="12" left="81" textpieces="0" top="214" width="359">imize total weight of clauses that span over multiple parti-</text>
<text font="3" height="12" left="81" textpieces="0" top="230" width="359">tions, i.e., the cut size. To capture this notion, we de&#64257;ne a</text>
<text font="3" height="12" left="81" textpieces="0" top="245" width="359">balanced bisection of a hypergraph G = (V, E) as a partition</text>
<text font="3" height="12" left="81" textpieces="3" top="261" width="358">of V = V1&#8746; V2 such that |V1| = |V2|. The cost of a bisection</text>
<text font="3" height="12" left="81" textpieces="0" top="277" width="32">(V1, V</text>
<text font="1" height="8" left="115" textpieces="2" top="281" width="247">2) is |{e &#8712; E|e &#8745; V1 = &#8709; and e &#8745; V2 = &#8709;}|.</text>
<text font="3" height="12" left="96" textpieces="0" top="308" width="344">Theorem 3.2. Consider the MLN &#915; given by the single</text>
<text font="3" height="12" left="81" textpieces="0" top="321" width="359">rule p(x), r(x, y) &#8594; p(y) where r is an evidence predicate.</text>
<text font="3" height="12" left="81" textpieces="0" top="337" width="359">Then, the problem of &#64257;nding a minimum-cost balanced bi-</text>
<text font="3" height="12" left="81" textpieces="0" top="353" width="359">section of the MRF that results from &#915; is NP-hard in the</text>
<text font="3" height="12" left="81" textpieces="0" top="368" width="162">size of the evidence (data).</text>
<text font="3" height="12" left="81" textpieces="0" top="397" width="359">The proof (Appendix B.6) is by reduction to the graph min-</text>
<text font="3" height="12" left="81" textpieces="0" top="413" width="359">imum bisection problem [14], which is hard to approximate</text>
<text font="3" height="12" left="81" textpieces="0" top="429" width="359">(unless P = NP, there is no PTAS). In fact, the problem</text>
<text font="3" height="12" left="81" textpieces="0" top="444" width="359">we are facing (multi-way hypergraph partitioning) is more</text>
<text font="3" height="12" left="81" textpieces="0" top="460" width="359">challenging than graph bisection, and has been extensively</text>
<text font="3" height="12" left="81" textpieces="0" top="476" width="359">studied [12, 24]. And so, we design a simple, greedy parti-</text>
<text font="3" height="12" left="81" textpieces="0" top="492" width="359">tioning algorithm: it assigns each clause to a bin in descend-</text>
<text font="3" height="12" left="81" textpieces="0" top="507" width="359">ing order by clause weight, subject to the constraint that no</text>
<text font="3" height="12" left="81" textpieces="0" top="523" width="359">component in the resulting graph is larger than an input</text>
<text font="3" height="12" left="81" textpieces="0" top="539" width="334">parameter &#946;. We include pseudocode in Appendix B.7.</text>
<text font="5" height="14" left="81" textpieces="1" top="571" width="358">Partition-aware Search. We need to re&#64257;ne the search pro-</text>
<text font="3" height="12" left="81" textpieces="0" top="588" width="359">cedure to be aware of partitions: the central challenge is</text>
<text font="3" height="12" left="81" textpieces="0" top="604" width="359">that a clause in the cut may depend on atoms in two dis-</text>
<text font="3" height="12" left="81" textpieces="0" top="619" width="359">tinct partitions. Hence, there are dependencies between the</text>
<text font="3" height="12" left="81" textpieces="0" top="635" width="359">partitions. We exploit the idea in Example 2 to design the</text>
<text font="3" height="12" left="81" textpieces="0" top="651" width="359">following partition-aware search scheme &#8211; which is an in-</text>
<text font="3" height="12" left="81" textpieces="0" top="666" width="359">stance of the Gauss-Seidel method from nonlinear optimiza-</text>
<text font="3" height="12" left="81" textpieces="1" top="682" width="358">tion [3, pg. 219]. Denote by X1, . . . , Xkthe states (i.e., truth</text>
<text font="3" height="12" left="81" textpieces="0" top="698" width="359">assignments to the atoms) of the partitions. First initialize</text>
<text font="3" height="12" left="81" textpieces="1" top="713" width="49">Xi = x0</text>
<text font="1" height="8" left="125" textpieces="0" top="719" width="4">i</text>
<text font="3" height="12" left="136" textpieces="0" top="713" width="303">for i = 1 . . . k. For t = 1 . . . T , for i = 1 . . . k, run</text>
<text font="3" height="12" left="81" textpieces="0" top="729" width="106">WalkSAT on xt&#8722;1</text>
<text font="1" height="8" left="169" textpieces="0" top="736" width="4">i</text>
<text font="3" height="12" left="192" textpieces="0" top="729" width="111">conditioned on {xt</text>
<text font="1" height="8" left="299" textpieces="1" top="735" width="103">j|1 &#8804; j &lt; i} &#8746; {xt&#8722;1      j</text>
<text font="3" height="13" left="416" textpieces="0" top="728" width="23">|i &lt;</text>
<text font="3" height="12" left="81" textpieces="0" top="748" width="116">j &#8804; k} to obtain xt</text>
<text font="1" height="8" left="192" textpieces="1" top="753" width="126">i. Finally, return {xT  i</text>
<text font="3" height="13" left="323" textpieces="0" top="747" width="71">|1 &#8804; i &#8804; k}.</text>
<text font="5" height="14" left="81" textpieces="1" top="780" width="362">Tradeoffs. Although &#64257;ne-grained partitioning improves per-</text>
<text font="3" height="12" left="81" textpieces="0" top="797" width="359">partition search speed (Theorem 3.1) and space e&#64259;ciency, it</text>
<text font="3" height="12" left="81" textpieces="0" top="813" width="359">also increases cut sizes &#8211; especially for dense graphs &#8211; which</text>
<text font="3" height="12" left="81" textpieces="0" top="828" width="359">would in turn slow down the Gauss-Seidel inference scheme.</text>
<text font="3" height="12" left="81" textpieces="0" top="844" width="359">Thus, there is an interesting tradeo&#64256; of partitioning gran-</text>
<text font="3" height="12" left="81" textpieces="0" top="860" width="359">ularity. In Section B.8, we describe a basic heuristic that</text>
<text font="3" height="12" left="81" textpieces="0" top="876" width="318">combines Theorem 3.1 and the Gauss-Seidel scheme.</text>
<text font="2" height="16" left="81" textpieces="1" top="908" width="163">4.  EXPERIMENTS</text>
<text font="3" height="14" left="94" textpieces="0" top="931" width="345">In this section, we validate &#64257;rst that our system Tuffy is</text>
<text font="3" height="12" left="81" textpieces="0" top="946" width="359">orders of magnitude more scalable and e&#64259;cient than prior</text>
<text font="3" height="12" left="81" textpieces="0" top="962" width="359">approaches. We then validate that each of our techniques</text>
<text font="3" height="12" left="81" textpieces="0" top="978" width="141">contributes to the goal.</text>
<text font="5" height="14" left="81" textpieces="1" top="1002" width="359">Experimental Setup. We select Alchemy, the currently</text>
<text font="3" height="14" left="81" textpieces="0" top="1020" width="359">most widely used MLN system, as our comparison point.</text>
<text font="3" height="12" left="81" textpieces="0" top="1038" width="359">Alchemy and Tuffy are implemented in C++ and Java,</text>
<text font="3" height="14" left="81" textpieces="0" top="1051" width="359">respectively. The RDBMS used by Tuffy is PostgreSQL</text>
<text font="3" height="12" left="81" textpieces="0" top="1067" width="359">8.4. Unless speci&#64257;ed otherwise, all experiments are run on an</text>
<text font="8" height="10" left="661" textpieces="0" top="135" width="32">1.0E+03</text>
<text font="8" height="10" left="661" textpieces="0" top="111" width="32">2.0E+03</text>
<text font="8" height="10" left="661" textpieces="0" top="87" width="32">3.0E+03</text>
<text font="8" height="10" left="699" textpieces="2" top="147" width="98">0          20          40</text>
<text font="9" height="11" left="656" textpieces="0" top="116" width="0">cost </text>
<text font="10" height="12" left="759" textpieces="0" top="87" width="12">IE </text>
<text font="11" height="12" left="769" textpieces="0" top="98" width="45">Alchemy </text>
<text font="11" height="12" left="722" textpieces="0" top="127" width="28">Tuffy </text>
<text font="8" height="10" left="494" textpieces="0" top="135" width="32">0.0E+00</text>
<text font="8" height="10" left="494" textpieces="0" top="119" width="32">1.0E+04</text>
<text font="8" height="10" left="494" textpieces="0" top="103" width="32">2.0E+04</text>
<text font="8" height="10" left="494" textpieces="0" top="87" width="32">3.0E+04</text>
<text font="8" height="10" left="532" textpieces="2" top="147" width="116">0             50           100</text>
<text font="9" height="11" left="490" textpieces="0" top="117" width="0">cost </text>
<text font="10" height="12" left="568" textpieces="0" top="90" width="14">LP </text>
<text font="11" height="12" left="595" textpieces="1" top="105" width="-15">Alchemy                  Tuffy </text>
<text font="8" height="10" left="660" textpieces="0" top="210" width="32">0.0E+00</text>
<text font="8" height="10" left="660" textpieces="0" top="187" width="32">1.0E+05</text>
<text font="8" height="10" left="660" textpieces="0" top="163" width="32">2.0E+05</text>
<text font="8" height="10" left="698" textpieces="1" top="223" width="124">0    2000 4000 6000 8000</text>
<text font="9" height="11" left="656" textpieces="0" top="192" width="0">cost </text>
<text font="10" height="12" left="756" textpieces="0" top="162" width="15">ER </text>
<text font="12" height="10" left="707" textpieces="0" top="178" width="118">Alchemy grounding took 7 hr. </text>
<text font="11" height="12" left="743" textpieces="0" top="189" width="28">Tuffy </text>
<text font="8" height="10" left="494" textpieces="0" top="210" width="32">0.0E+00</text>
<text font="8" height="10" left="494" textpieces="0" top="195" width="32">2.0E+03</text>
<text font="8" height="10" left="494" textpieces="0" top="179" width="32">4.0E+03</text>
<text font="8" height="10" left="494" textpieces="0" top="163" width="32">6.0E+03</text>
<text font="8" height="10" left="532" textpieces="3" top="223" width="104">0     2000   4000  6000</text>
<text font="9" height="11" left="490" textpieces="0" top="192" width="0">cost </text>
<text font="10" height="12" left="567" textpieces="0" top="165" width="16">RC </text>
<text font="11" height="12" left="599" textpieces="0" top="178" width="45">Alchemy </text>
<text font="11" height="12" left="559" textpieces="0" top="186" width="28">Tuffy </text>
<text font="3" height="12" left="475" textpieces="0" top="252" width="359">Figure 3: Time-cost plots of Alchemy vs. Tu&#64256;y; the</text>
<text font="3" height="12" left="475" textpieces="0" top="268" width="145">x axes are time (sec)</text>
<text font="3" height="12" left="475" textpieces="0" top="299" width="359">Intel Core2 at 2.4GHz with 4 GB of RAM running Red Hat</text>
<text font="3" height="12" left="475" textpieces="0" top="315" width="359">Enterprise Linux 5. For fair comparison, in all experiments</text>
<text font="3" height="12" left="475" textpieces="0" top="333" width="311">Tuffy runs a single thread unless otherwise noted.</text>
<text font="5" height="14" left="475" textpieces="1" top="363" width="359">Datasets. We run Alchemy and Tuffy on four datasets;</text>
<text font="3" height="14" left="475" textpieces="0" top="380" width="359">three of them (including their MLNs) are taken directly</text>
<text font="3" height="14" left="475" textpieces="0" top="396" width="359">from the Alchemy website [7]: Link Prediction (LP), given</text>
<text font="3" height="12" left="475" textpieces="0" top="412" width="359">an administrative database of a CS department, the goal</text>
<text font="3" height="12" left="475" textpieces="0" top="427" width="359">is to predict student-adviser relationships; Information Ex-</text>
<text font="3" height="12" left="475" textpieces="0" top="443" width="359">traction (IE), given a set of Citeseer citations, the goal is</text>
<text font="3" height="12" left="475" textpieces="0" top="459" width="359">to extract from them structured records; and Entity Resolu-</text>
<text font="3" height="12" left="475" textpieces="0" top="474" width="359">tion (ER), which is to deduplicate citation records based on</text>
<text font="3" height="12" left="475" textpieces="0" top="490" width="359">word similarity. These tasks have been extensively used in</text>
<text font="3" height="12" left="475" textpieces="0" top="506" width="359">prior work. The last task, Relational Classi&#64257;cation (RC),</text>
<text font="3" height="12" left="475" textpieces="0" top="521" width="359">performs classi&#64257;cation on the Cora dataset [15]; RC con-</text>
<text font="3" height="12" left="475" textpieces="0" top="537" width="359">tains all the rules in Figure 1. Table 1 contains statistics</text>
<text font="3" height="12" left="475" textpieces="0" top="553" width="94">about the data.</text>
<text font="4" height="11" left="634" textpieces="3" top="582" width="164">LP        IE      RC    ER</text>
<text font="4" height="11" left="512" textpieces="4" top="596" width="286">#relations               22        18         4      10</text>
<text font="4" height="11" left="512" textpieces="4" top="609" width="285">#rules                    94       1K        15   3.8K</text>
<text font="4" height="11" left="512" textpieces="4" top="623" width="285">#entities               302     2.6K      51K     510</text>
<text font="4" height="11" left="512" textpieces="4" top="636" width="285">#evidence tuples      731   0.25M   0.43M     676</text>
<text font="4" height="11" left="512" textpieces="4" top="650" width="286">#query atoms        4.6K   0.34M      10K    16K</text>
<text font="4" height="11" left="512" textpieces="4" top="663" width="285">#components            1     5341       489        1</text>
<text font="3" height="12" left="563" textpieces="0" top="692" width="183">Table 1: Dataset statistics</text>
<text font="2" height="16" left="475" textpieces="1" top="722" width="223">4.1  High-level Performance</text>
<text font="3" height="14" left="489" textpieces="0" top="744" width="345">We empirically demonstrate that Tuffy with all the tech-</text>
<text font="3" height="12" left="475" textpieces="0" top="760" width="359">niques we have described has faster grounding, higher search</text>
<text font="3" height="12" left="475" textpieces="0" top="775" width="359">speed, lower memory usage, and in some cases produces</text>
<text font="3" height="12" left="475" textpieces="0" top="791" width="359">much better solutions than a competitor main memory ap-</text>
<text font="3" height="14" left="475" textpieces="0" top="807" width="359">proach, Alchemy. Recall that the name of the game is</text>
<text font="3" height="12" left="475" textpieces="0" top="823" width="359">to produce low-cost solutions quickly. With this in mind,</text>
<text font="3" height="14" left="475" textpieces="0" top="838" width="359">we run Tuffy and Alchemy on each dataset for 7500 sec-</text>
<text font="3" height="12" left="475" textpieces="0" top="854" width="359">onds, and track the cost of the best solution found up to</text>
<text font="3" height="12" left="475" textpieces="0" top="870" width="359">any moment; on datasets that have multiple components,</text>
<text font="3" height="12" left="475" textpieces="0" top="885" width="359">namely IE and RC, we apply the partitioning strategy in</text>
<text font="3" height="14" left="475" textpieces="0" top="901" width="359">Section 3.3 on Tuffy. As shown in Figure 3, Tuffy of-</text>
<text font="3" height="12" left="475" textpieces="0" top="917" width="359">ten reaches a best solution within orders of magnitude less</text>
<text font="3" height="14" left="475" textpieces="0" top="932" width="359">time than Alchemy; secondly, the result quality of Tuffy</text>
<text font="3" height="12" left="475" textpieces="0" top="948" width="359">is at least as good as &#8211; sometimes substantially better (e.g.,</text>
<text font="3" height="14" left="475" textpieces="0" top="964" width="359">on IE and RC) than &#8211; Alchemy. Here, we have zoomed</text>
<text font="3" height="12" left="475" textpieces="0" top="979" width="359">the time axes into interesting areas. Since &#8220;solution cost&#8221;</text>
<text font="3" height="12" left="475" textpieces="0" top="995" width="359">is unde&#64257;ned during grounding, each curve begins only when</text>
<text font="3" height="12" left="475" textpieces="0" top="1011" width="358">grounding is completed6. We analyze the experiment results</text>
<text font="3" height="12" left="475" textpieces="0" top="1027" width="238">in more detail in the following sections.</text>
<text font="1" height="8" left="476" textpieces="0" top="1051" width="357">6The L-shaped curves indicate that search converges very</text>
<text font="3" height="12" left="475" textpieces="0" top="1067" width="224">quickly compared to grounding time.</text>
<text font="4" height="11" left="223" textpieces="3" top="82" width="148">LP   IE     RC      ER</text>
<text font="4" height="10" left="149" textpieces="4" top="99" width="222">Alchemy      48    13   3,913   23,891</text>
<text font="4" height="10" left="157" textpieces="4" top="113" width="214">Tuffy         6    13       40       106</text>
<text font="3" height="12" left="154" textpieces="0" top="141" width="212">Table 2: Grounding time (sec)</text>
<text font="12" height="10" left="99" textpieces="0" top="227" width="31">0.0E+00</text>
<text font="12" height="10" left="99" textpieces="0" top="201" width="31">1.0E+04</text>
<text font="12" height="10" left="99" textpieces="0" top="175" width="31">2.0E+04</text>
<text font="12" height="10" left="137" textpieces="2" top="240" width="98">0        1000      2000</text>
<text font="9" height="11" left="94" textpieces="0" top="200" width="0">cost </text>
<text font="9" height="11" left="171" textpieces="0" top="254" width="47">time (sec) </text>
<text font="10" height="12" left="191" textpieces="0" top="164" width="14">LP </text>
<text font="13" height="11" left="175" textpieces="0" top="192" width="70">Alchemy (solid) </text>
<text font="13" height="11" left="165" textpieces="0" top="180" width="64">Tuffy-p (dash) </text>
<text font="13" height="11" left="201" textpieces="0" top="205" width="46">Tuffy-mm </text>
<text font="12" height="10" left="268" textpieces="0" top="227" width="31">0.0E+00</text>
<text font="12" height="10" left="268" textpieces="0" top="198" width="31">2.0E+05</text>
<text font="12" height="10" left="268" textpieces="0" top="168" width="31">4.0E+05</text>
<text font="12" height="10" left="306" textpieces="2" top="240" width="123">0            4000          8000</text>
<text font="9" height="11" left="263" textpieces="0" top="200" width="0">cost </text>
<text font="9" height="11" left="341" textpieces="0" top="254" width="47">time (sec) </text>
<text font="10" height="12" left="360" textpieces="0" top="167" width="16">RC </text>
<text font="13" height="11" left="385" textpieces="0" top="195" width="40">Alchemy </text>
<text font="13" height="11" left="323" textpieces="0" top="216" width="34">Tuffy-p </text>
<text font="13" height="11" left="316" textpieces="0" top="180" width="46">Tuffy-mm </text>
<text font="3" height="12" left="81" textpieces="0" top="282" width="359">Figure 4: Time-cost plots of Alchemy vs. Tu&#64256;y-p</text>
<text font="3" height="12" left="81" textpieces="0" top="297" width="359">(i.e., Tu&#64256;y without partitioning) vs. Tu&#64256;y-mm (i.e.,</text>
<text font="3" height="12" left="81" textpieces="0" top="313" width="237">Tu&#64256;y with RDBMS-based search)</text>
<text font="2" height="16" left="81" textpieces="1" top="351" width="281">4.2  Effect of Bottom-up Grounding</text>
<text font="3" height="12" left="94" textpieces="0" top="373" width="345">We validate that the RDBMS-based grounding approach</text>
<text font="3" height="14" left="81" textpieces="0" top="389" width="359">in Tuffy allows us to complete the grounding process orders</text>
<text font="3" height="14" left="81" textpieces="0" top="405" width="359">of magnitude more e&#64259;ciently than Alchemy. To make this</text>
<text font="3" height="14" left="81" textpieces="0" top="420" width="359">point, we run Tuffy and Alchemy on the four datasets,</text>
<text font="3" height="12" left="81" textpieces="0" top="436" width="359">and show their grounding time in Table 2. We can see that</text>
<text font="3" height="12" left="81" textpieces="0" top="454" width="359">Tuffy outperforms Alchemy by orders of magnitude at</text>
<text font="3" height="12" left="81" textpieces="0" top="467" width="359">run time in the grounding phase (a factor of 225 on the</text>
<text font="3" height="12" left="81" textpieces="0" top="483" width="359">ER dataset). To understand the di&#64256;erences, we dug deeper</text>
<text font="3" height="12" left="81" textpieces="0" top="499" width="359">with a lesion study (i.e., disabling one aspect of a system at</text>
<text font="3" height="12" left="81" textpieces="0" top="514" width="359">a time), and found that sort join and hash join algorithms</text>
<text font="3" height="12" left="81" textpieces="0" top="530" width="359">(along with predicate pushdown) are the key components of</text>
<text font="3" height="14" left="81" textpieces="0" top="546" width="359">the RDBMS that speeds up the grounding process of Tuffy</text>
<text font="3" height="14" left="81" textpieces="0" top="561" width="359">(Appendix C.2). Tuffy obviates the need for Alchemy to</text>
<text font="3" height="12" left="81" textpieces="0" top="577" width="342">reimplement the optimization techniques in an RDBMS.</text>
<text font="2" height="16" left="81" textpieces="1" top="605" width="267">4.3  Effect of Hybrid Architecture</text>
<text font="3" height="12" left="94" textpieces="0" top="627" width="345">We validate two technical claims: (1) the hybrid memory</text>
<text font="3" height="14" left="81" textpieces="0" top="643" width="359">management strategy of Tuffy (even without our parti-</text>
<text font="3" height="12" left="81" textpieces="0" top="659" width="359">tioning optimizations) has comparable search rates to exist-</text>
<text font="3" height="12" left="81" textpieces="0" top="674" width="359">ing main memory implementations (and much faster than</text>
<text font="3" height="14" left="81" textpieces="0" top="690" width="359">RDBMS-based implementation) and (2) Tuffy maintains</text>
<text font="3" height="12" left="81" textpieces="0" top="706" width="359">a much smaller memory footprint (again without partition-</text>
<text font="3" height="14" left="81" textpieces="0" top="721" width="359">ing). Thus, we compare three approaches: (1) Tuffy with-</text>
<text font="3" height="14" left="81" textpieces="0" top="737" width="359">out the partitioning optimizations, called Tuffy-p (read:</text>
<text font="3" height="14" left="81" textpieces="0" top="753" width="359">Tu&#64256;y minus p), (2) a version of Tuffy (also without parti-</text>
<text font="3" height="12" left="81" textpieces="0" top="768" width="359">tioning) that implements RDBMS-based WalkSAT (detailed</text>
<text font="3" height="14" left="81" textpieces="0" top="784" width="305">in Appendix B.2), Tuffy-mm, and (3) Alchemy.</text>
<text font="3" height="12" left="94" textpieces="0" top="800" width="345">Figure 4 illustrates the time-cost plots on LP and RC of</text>
<text font="3" height="14" left="81" textpieces="0" top="816" width="359">all three approaches. We see from RC that Tuffy-p is able</text>
<text font="3" height="14" left="81" textpieces="0" top="831" width="359">to ground much more quickly than Alchemy (40 sec com-</text>
<text font="3" height="12" left="81" textpieces="0" top="847" width="359">pared to 3913 sec). Additionally, we see that, compared to</text>
<text font="3" height="12" left="81" textpieces="0" top="865" width="359">Tuffy-mm, Tuffy-p&#8217;s in-memory search is orders of mag-</text>
<text font="3" height="12" left="81" textpieces="0" top="878" width="359">nitude faster at getting to their best reported solution (both</text>
<text font="3" height="12" left="81" textpieces="0" top="894" width="359">approaches &#64257;nish grounding at the same time, and so start</text>
<text font="3" height="12" left="81" textpieces="0" top="910" width="359">search at the same time). To understand why, we measure</text>
<text font="3" height="12" left="81" textpieces="0" top="925" width="359">the &#64258;ipping rate, which is the number of steps performed by</text>
<text font="3" height="12" left="81" textpieces="0" top="941" width="359">WalkSAT per second. As shown in Table 3, the reason is</text>
<text font="3" height="14" left="81" textpieces="0" top="957" width="359">that Tuffy-mm has a dramatically lower &#64258;ipping rate. We</text>
<text font="3" height="12" left="81" textpieces="0" top="972" width="359">discuss the performance bound of any RDBMS-based search</text>
<text font="3" height="12" left="81" textpieces="0" top="988" width="203">implementation in Appendix C.1.</text>
<text font="3" height="14" left="94" textpieces="0" top="1004" width="345">To validate our second claim, that Tuffy-p has a smaller</text>
<text font="3" height="12" left="81" textpieces="0" top="1020" width="359">memory footprint, we see in Table 4, that on all datasets,</text>
<text font="3" height="14" left="81" textpieces="0" top="1035" width="359">the memory footprint of Tuffy is no more than 5% of</text>
<text font="3" height="12" left="81" textpieces="0" top="1053" width="359">Alchemy. Drilling down, the reason is that the interme-</text>
<text font="3" height="14" left="81" textpieces="0" top="1067" width="359">diate state size of Alchemy&#8217;s grounding process may be</text>
<text font="4" height="11" left="622" textpieces="3" top="82" width="160">LP       IE      RC    ER</text>
<text font="4" height="10" left="528" textpieces="4" top="99" width="257">Alchemy     0.20M       1M     1.9K   0.9K</text>
<text font="4" height="10" left="524" textpieces="4" top="113" width="262">Tuffy-mm        0.9        13       0.9    0.03</text>
<text font="4" height="10" left="531" textpieces="4" top="127" width="254">Tuffy-p      0.11M   0.39M   0.17M   7.9K</text>
<text font="3" height="12" left="531" textpieces="0" top="155" width="248">Table 3: Flipping rates (#&#64258;ips/sec)</text>
<text font="4" height="11" left="622" textpieces="3" top="171" width="199">LP          IE        RC         ER</text>
<text font="4" height="11" left="501" textpieces="4" top="185" width="319">clause table       5.2 MB    0.6 MB   4.8 MB   164 MB</text>
<text font="4" height="10" left="489" textpieces="4" top="201" width="331">Alchemy RAM    411 MB   206 MB    2.8 GB    3.5 GB</text>
<text font="4" height="10" left="491" textpieces="4" top="216" width="329">Tuffy-p RAM        9 MB      8 MB    19 MB   184 MB</text>
<text font="3" height="12" left="475" textpieces="1" top="243" width="359">Table 4: Space e&#64259;ciency of Alchemy vs.  Tu&#64256;y-p</text>
<text font="3" height="12" left="475" textpieces="0" top="258" width="154">(without partitioning)</text>
<text font="3" height="12" left="475" textpieces="0" top="289" width="359">larger than the size of grounding results. For example, on</text>
<text font="3" height="14" left="475" textpieces="0" top="305" width="359">the RC dataset, Alchemy allocated 2.8 GB of RAM only to</text>
<text font="3" height="14" left="475" textpieces="0" top="321" width="359">produce 4.8 MB of ground clauses. While Alchemy has to</text>
<text font="3" height="14" left="475" textpieces="0" top="336" width="359">hold everything in memory, Tuffy only needs to load the</text>
<text font="3" height="12" left="475" textpieces="0" top="352" width="359">grounding result from the RDBMS at the end of grounding.</text>
<text font="3" height="14" left="475" textpieces="0" top="368" width="359">It follows that, given the same resources, there are MLNs</text>
<text font="3" height="14" left="475" textpieces="0" top="384" width="359">that Tuffy can handle e&#64259;ciently while Alchemy would</text>
<text font="3" height="12" left="475" textpieces="0" top="399" width="359">crash. Indeed, on a dataset called &#8220;ER+&#8221; which is twice</text>
<text font="3" height="14" left="475" textpieces="0" top="415" width="359">as large as ER, Alchemy exhausts all 4GB of RAM and</text>
<text font="3" height="14" left="475" textpieces="0" top="431" width="359">crashes soon after launching, whereas Tuffy runs normally</text>
<text font="3" height="12" left="475" textpieces="0" top="446" width="238">with peak RAM usage of roughly 2GB.</text>
<text font="3" height="12" left="489" textpieces="0" top="462" width="345">From these experiments, we conclude that the hybrid ar-</text>
<text font="3" height="14" left="475" textpieces="0" top="478" width="297">chitecture is crucial to Tuffy&#8217;s overall e&#64259;ciency.</text>
<text font="2" height="16" left="475" textpieces="1" top="503" width="203">4.4  Effect of Partitioning</text>
<text font="3" height="12" left="489" textpieces="0" top="525" width="345">In this section, we validate that, when there are multi-</text>
<text font="3" height="12" left="475" textpieces="0" top="541" width="359">ple components in the data, partitioning not only improves</text>
<text font="3" height="12" left="475" textpieces="0" top="559" width="359">Tuffy&#8217;s space e&#64259;ciency, but &#8211; due to Theorem 3.1 &#8211; may</text>
<text font="3" height="14" left="475" textpieces="0" top="572" width="359">actually enable Tuffy to &#64257;nd substantially higher quality</text>
<text font="3" height="14" left="475" textpieces="0" top="588" width="359">results. We compare Tuffy&#8217;s performance (with partition-</text>
<text font="3" height="14" left="475" textpieces="0" top="604" width="359">ing enabled) against Tuffy-p: a version of Tuffy with</text>
<text font="3" height="12" left="475" textpieces="0" top="620" width="129">partitioning disabled.</text>
<text font="3" height="12" left="489" textpieces="0" top="635" width="345">We run the search phase on each of the four datasets using</text>
<text font="3" height="14" left="475" textpieces="0" top="651" width="359">three approaches: Alchemy, Tuffy-p, and Tuffy (with</text>
<text font="3" height="14" left="475" textpieces="0" top="667" width="359">partitioning). Tuffy-p and Alchemy run WalkSAT on the</text>
<text font="3" height="12" left="475" textpieces="1" top="682" width="359">whole MRF for 107 steps. Tuffy runs WalkSAT on each</text>
<text font="3" height="12" left="475" textpieces="0" top="698" width="358">component in the MRF independently, each component Gi</text>
<text font="3" height="12" left="475" textpieces="1" top="714" width="358">receiving 107|Gi|/|G| steps, where |Gi| and |G| are the num-</text>
<text font="3" height="12" left="475" textpieces="0" top="729" width="359">bers of atoms in this component and the MRF, respectively.</text>
<text font="3" height="12" left="475" textpieces="0" top="745" width="246">This is weighted round-robin scheduling.</text>
<text font="4" height="11" left="626" textpieces="3" top="776" width="169">LP      IE      RC        ER</text>
<text font="4" height="11" left="518" textpieces="4" top="790" width="276">#components         1    5341      489           1</text>
<text font="4" height="10" left="514" textpieces="4" top="806" width="281">Tuffy-p RAM    9MB   8MB   19MB   184MB</text>
<text font="4" height="10" left="520" textpieces="4" top="820" width="275">Tuffy RAM     9MB   8MB   15MB   184MB</text>
<text font="4" height="10" left="519" textpieces="4" top="834" width="276">Tuffy-p cost     2534    1933     1943     18717</text>
<text font="4" height="10" left="524" textpieces="4" top="848" width="271">Tuffy cost       2534    1635     1281     18717</text>
<text font="3" height="12" left="475" textpieces="2" top="876" width="359">Table 5:  Performance of Tu&#64256;y vs.  Tu&#64256;y-p (i.e.,</text>
<text font="3" height="12" left="475" textpieces="0" top="892" width="190">Tu&#64256;y without partitioning)</text>
<text font="3" height="12" left="489" textpieces="0" top="925" width="345">As shown in Table 5, when there are multiple compo-</text>
<text font="3" height="14" left="475" textpieces="0" top="941" width="359">nents in the MRF, partitioning allows Tuffy to use less</text>
<text font="3" height="14" left="475" textpieces="0" top="957" width="359">memory than Tuffy-p. (The IE dataset is too small to</text>
<text font="3" height="14" left="475" textpieces="0" top="972" width="359">yield notable di&#64256;erences). We see that Tuffy&#8217;s component-</text>
<text font="3" height="12" left="475" textpieces="0" top="988" width="359">aware inference can produce signi&#64257;cantly better results than</text>
<text font="3" height="12" left="475" textpieces="0" top="1006" width="359">Tuffy-p. We then extend the run time of all systems. As</text>
<text font="3" height="12" left="475" textpieces="0" top="1020" width="359">shown in Figure 5, there continues to be a gap between</text>
<text font="3" height="12" left="475" textpieces="0" top="1038" width="359">Tuffy&#8217;s component-aware search approach and the original</text>
<text font="3" height="12" left="475" textpieces="0" top="1051" width="359">WalkSAT running on the whole MRF. This gap is predicted</text>
<text font="3" height="12" left="475" textpieces="0" top="1067" width="359">by our theoretical analysis in Section 3.3. Thus, we have</text>
<text font="12" height="10" left="100" textpieces="0" top="143" width="20">1000</text>
<text font="12" height="10" left="100" textpieces="0" top="126" width="20">1400</text>
<text font="12" height="10" left="100" textpieces="0" top="110" width="20">1800</text>
<text font="12" height="10" left="100" textpieces="0" top="94" width="20">2200</text>
<text font="12" height="10" left="100" textpieces="0" top="78" width="20">2600</text>
<text font="12" height="10" left="126" textpieces="4" top="155" width="138">0       20      40      60      80</text>
<text font="9" height="11" left="95" textpieces="0" top="116" width="0">cost </text>
<text font="9" height="11" left="172" textpieces="0" top="169" width="47">time (sec) </text>
<text font="14" height="14" left="201" textpieces="0" top="77" width="14">IE </text>
<text font="12" height="10" left="204" textpieces="0" top="135" width="22">Tuffy </text>
<text font="12" height="10" left="132" textpieces="0" top="84" width="64">Tuffy-p (dotted) </text>
<text font="12" height="10" left="132" textpieces="0" top="96" width="62">Alchemy (solid) </text>
<text font="12" height="10" left="280" textpieces="0" top="143" width="5">0</text>
<text font="12" height="10" left="266" textpieces="0" top="121" width="20">1000</text>
<text font="12" height="10" left="266" textpieces="0" top="99" width="20">2000</text>
<text font="12" height="10" left="266" textpieces="0" top="78" width="20">3000</text>
<text font="12" height="10" left="292" textpieces="3" top="155" width="133">0        100       200       300</text>
<text font="9" height="11" left="261" textpieces="0" top="116" width="0">cost </text>
<text font="9" height="11" left="334" textpieces="0" top="169" width="47">time (sec) </text>
<text font="14" height="14" left="336" textpieces="0" top="77" width="19">RC </text>
<text font="12" height="10" left="363" textpieces="0" top="106" width="22">Tuffy </text>
<text font="12" height="10" left="370" textpieces="0" top="83" width="30">Tuffy-p </text>
<text font="1" height="8" left="304" textpieces="0" top="135" width="121">Alchemy grounding took over 1 hr. </text>
<text font="3" height="12" left="81" textpieces="0" top="195" width="359">Figure 5: Time-cost plots of Tu&#64256;y vs Tu&#64256;y-p (i.e.,</text>
<text font="3" height="12" left="81" textpieces="0" top="211" width="190">Tu&#64256;y without partitioning)</text>
<text font="1" height="9" left="98" textpieces="0" top="304" width="17">0E+0</text>
<text font="1" height="9" left="98" textpieces="0" top="280" width="17">1E+3</text>
<text font="1" height="9" left="98" textpieces="0" top="256" width="17">2E+3</text>
<text font="1" height="9" left="98" textpieces="0" top="232" width="17">3E+3</text>
<text font="1" height="9" left="121" textpieces="3" top="315" width="76">0    100   200   300</text>
<text font="8" height="10" left="94" textpieces="0" top="273" width="0">cost </text>
<text font="8" height="10" left="137" textpieces="0" top="327" width="42">time (sec) </text>
<text font="8" height="10" left="154" textpieces="0" top="233" width="13">RC </text>
<text font="12" height="10" left="173" textpieces="0" top="243" width="23">15MB</text>
<text font="12" height="10" left="173" textpieces="0" top="254" width="23">13MB</text>
<text font="12" height="10" left="173" textpieces="0" top="266" width="23">12MB</text>
<text font="1" height="9" left="210" textpieces="0" top="304" width="24">2.4E+3</text>
<text font="1" height="9" left="210" textpieces="0" top="280" width="24">2.6E+3</text>
<text font="1" height="9" left="210" textpieces="0" top="256" width="24">2.8E+3</text>
<text font="1" height="9" left="210" textpieces="0" top="232" width="24">3.0E+3</text>
<text font="1" height="9" left="240" textpieces="2" top="315" width="71">0     50   100 150</text>
<text font="8" height="10" left="206" textpieces="0" top="273" width="0">cost </text>
<text font="8" height="10" left="254" textpieces="0" top="327" width="42">time (sec) </text>
<text font="9" height="11" left="264" textpieces="0" top="234" width="13">LP </text>
<text font="12" height="10" left="286" textpieces="0" top="249" width="18">9MB</text>
<text font="12" height="10" left="286" textpieces="0" top="260" width="18">5MB</text>
<text font="12" height="10" left="286" textpieces="0" top="271" width="26">3.5MB</text>
<text font="1" height="9" left="326" textpieces="0" top="304" width="17">0E+0</text>
<text font="1" height="9" left="326" textpieces="0" top="286" width="17">2E+4</text>
<text font="1" height="9" left="326" textpieces="0" top="268" width="17">4E+4</text>
<text font="1" height="9" left="326" textpieces="0" top="250" width="17">6E+4</text>
<text font="1" height="9" left="326" textpieces="0" top="232" width="17">8E+4</text>
<text font="1" height="9" left="349" textpieces="1" top="315" width="77">0    500 1000 1500</text>
<text font="8" height="10" left="322" textpieces="0" top="273" width="0">cost </text>
<text font="8" height="10" left="364" textpieces="0" top="327" width="42">time (sec) </text>
<text font="9" height="11" left="379" textpieces="0" top="234" width="14">ER </text>
<text font="12" height="10" left="399" textpieces="0" top="247" width="28">200MB</text>
<text font="12" height="10" left="399" textpieces="0" top="260" width="28">100MB</text>
<text font="12" height="10" left="399" textpieces="0" top="272" width="23">50MB</text>
<text font="3" height="12" left="81" textpieces="1" top="352" width="358">Figure 6:  Time-cost plots of Tu&#64256;y with di&#64256;erent</text>
<text font="3" height="12" left="81" textpieces="0" top="368" width="116">memory budgets</text>
<text font="3" height="14" left="81" textpieces="0" top="392" width="359">veri&#64257;ed that partitioning makes Tuffy substantially more</text>
<text font="3" height="12" left="81" textpieces="0" top="408" width="297">e&#64259;cient in terms of both space and search speed.</text>
<text font="3" height="14" left="94" textpieces="0" top="423" width="345">We also validate that Tuffy&#8217;s loading and parallelism</text>
<text font="3" height="12" left="81" textpieces="0" top="439" width="359">makes a substantial di&#64256;erence: without our batch loading</text>
<text font="3" height="14" left="81" textpieces="1" top="455" width="359">technique, Tuffy takes 448s to perform 106 search steps</text>
<text font="3" height="12" left="81" textpieces="0" top="470" width="359">per component on RC, while 117s to perform the same op-</text>
<text font="3" height="12" left="81" textpieces="0" top="486" width="359">eration with batch loading. With the addition of 8 threads</text>
<text font="3" height="12" left="81" textpieces="0" top="502" width="359">(on 8 cores), we further reduce the runtime to 28s. Addi-</text>
<text font="3" height="12" left="81" textpieces="0" top="517" width="359">tional loading and parallelism experiments in Appendix C.3</text>
<text font="3" height="12" left="81" textpieces="0" top="533" width="359">support our claim that our loading algorithm and partition-</text>
<text font="3" height="12" left="81" textpieces="0" top="549" width="339">ing algorithm contribute to improving processing speed.</text>
<text font="2" height="16" left="81" textpieces="1" top="573" width="267">4.5  Effect of Further Partitioning</text>
<text font="3" height="12" left="94" textpieces="0" top="596" width="345">To validate our claim that splitting MRF components can</text>
<text font="3" height="12" left="81" textpieces="0" top="611" width="359">further improve both space e&#64259;ciency and sometimes also</text>
<text font="3" height="14" left="81" textpieces="0" top="627" width="359">search quality (Section 3.4), we run Tuffy on RC, ER, and</text>
<text font="3" height="12" left="81" textpieces="0" top="643" width="359">LP with di&#64256;erent memory budgets &#8211; which are fed to the</text>
<text font="3" height="12" left="81" textpieces="0" top="658" width="359">partitioning algorithm as the bound of partition size. On</text>
<text font="3" height="14" left="81" textpieces="0" top="674" width="359">each dataset, we give Tuffy three memory budgets, with</text>
<text font="3" height="12" left="81" textpieces="0" top="690" width="359">the largest one corresponding to the case when no compo-</text>
<text font="3" height="12" left="81" textpieces="0" top="706" width="359">nents are split. Figure 6 shows the experiment results. On</text>
<text font="3" height="12" left="81" textpieces="0" top="721" width="359">RC, we see another improvement of the result quality (cf.</text>
<text font="3" height="12" left="81" textpieces="0" top="737" width="359">Figure 5). Similar to Example 2, we believe the reason to be</text>
<text font="3" height="12" left="81" textpieces="0" top="753" width="359">graph sparsity: &#8220;13MB&#8221; cuts only about 420 out of the to-</text>
<text font="3" height="12" left="81" textpieces="0" top="768" width="359">tal 10K clauses. In contrast, while MRF partitioning lowers</text>
<text font="3" height="12" left="81" textpieces="0" top="784" width="359">RAM usage considerably on ER, it also leads to slower con-</text>
<text font="3" height="12" left="81" textpieces="0" top="800" width="359">vergence &#8211; which correlates with poor partitioning quality:</text>
<text font="3" height="12" left="81" textpieces="0" top="815" width="359">the MRF of ER is quite dense and even 2-way partition-</text>
<text font="3" height="12" left="81" textpieces="0" top="831" width="359">ing (&#8220;100MB&#8221;) would cut over 1.4M out of the total 2M</text>
<text font="3" height="12" left="81" textpieces="0" top="847" width="359">clauses. The dataset LP illustrates the interesting tradeo&#64256;</text>
<text font="3" height="12" left="81" textpieces="0" top="862" width="359">where a coarse partition is bene&#64257;cial whereas &#64257;ner grained</text>
<text font="3" height="12" left="81" textpieces="0" top="878" width="359">partitions would be detrimental. We discuss this tradeo&#64256; in</text>
<text font="3" height="12" left="81" textpieces="0" top="894" width="88">Appendix B.8.</text>
<text font="2" height="16" left="81" textpieces="1" top="919" width="153">5.  CONCLUSION</text>
<text font="3" height="12" left="94" textpieces="0" top="941" width="345">Motivated by a large set of data-rich applications, we</text>
<text font="3" height="14" left="81" textpieces="0" top="957" width="359">study how to push MLN inference inside an RDBMS. We</text>
<text font="3" height="14" left="81" textpieces="0" top="972" width="359">&#64257;nd that the grounding phase of MLN inference performs</text>
<text font="3" height="12" left="81" textpieces="0" top="988" width="359">many relational operations and that these operations are a</text>
<text font="3" height="14" left="81" textpieces="0" top="1004" width="359">substantial bottleneck in state-of-the-art MLN implementa-</text>
<text font="3" height="14" left="81" textpieces="0" top="1020" width="359">tions such as Alchemy. Using an RDBMS, Tuffy not only</text>
<text font="3" height="12" left="81" textpieces="0" top="1035" width="359">achieves scalability, but also speeds up the grounding phase</text>
<text font="3" height="12" left="81" textpieces="0" top="1051" width="359">by orders of magnitude. We then develop a hybrid solution</text>
<text font="3" height="12" left="81" textpieces="0" top="1067" width="359">with RDBMS-based grounding and in-memory search. To</text>
<text font="3" height="14" left="475" textpieces="0" top="86" width="359">improve the space and time e&#64259;ciency of Tuffy, we study a</text>
<text font="3" height="12" left="475" textpieces="0" top="102" width="359">partitioning approach that allows for in-memory search even</text>
<text font="3" height="12" left="475" textpieces="0" top="118" width="359">when the dataset does not &#64257;t in memory. We showed that</text>
<text font="3" height="14" left="475" textpieces="0" top="133" width="359">further partitioning allows Tuffy to produce higher quality</text>
<text font="3" height="12" left="475" textpieces="0" top="149" width="215">results in a shorter amount of time.</text>
<text font="2" height="16" left="475" textpieces="1" top="171" width="153">6.  REFERENCES</text>
<text font="13" height="9" left="481" textpieces="0" top="188" width="321">[1] L. Antova, T. Jansen, C. Koch, and D. Olteanu. Fast and</text>
<text font="13" height="9" left="501" textpieces="0" top="200" width="326">simple relational processing of uncertain data. In ICDE, 2008.</text>
<text font="13" height="9" left="481" textpieces="0" top="213" width="308">[2] O. Benjelloun, A. Sarma, A. Halevy, M. Theobald, and</text>
<text font="13" height="9" left="501" textpieces="0" top="225" width="326">J. Widom. Databases with uncertainty and lineage. In VLDB,</text>
<text font="13" height="9" left="501" textpieces="0" top="237" width="109">pages 243&#8211;264, 2008.</text>
<text font="13" height="9" left="481" textpieces="0" top="251" width="341">[3] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed</text>
<text font="13" height="9" left="501" textpieces="0" top="263" width="294">Computation: Numerical Methods. Prentice-Hall, 1989.</text>
<text font="13" height="9" left="481" textpieces="0" top="276" width="310">[4] N. N. Dalvi and D. Suciu. E&#64259;cient query evaluation on</text>
<text font="13" height="9" left="501" textpieces="0" top="288" width="292">probabilistic databases. In VLDB, pages 864&#8211;875, 2004.</text>
<text font="13" height="9" left="481" textpieces="0" top="302" width="300">[5] A. Deshpande and S. Madden. MauveDB: Supporting</text>
<text font="13" height="9" left="501" textpieces="0" top="313" width="307">model-based user views in database systems. In SIGMOD,</text>
<text font="13" height="9" left="501" textpieces="0" top="325" width="97">pages 73&#8211;84, 2006.</text>
<text font="13" height="9" left="481" textpieces="0" top="339" width="349">[6] P. Domingos and D. Lowd. Markov Logic: An Interface Layer</text>
<text font="13" height="9" left="501" textpieces="0" top="351" width="266">for Arti&#64257;cial Intelligence. Morgan Claypool, 2009.</text>
<text font="13" height="10" left="481" textpieces="0" top="364" width="307">[7] P. Domingos et al. http://alchemy.cs.washington.edu/.</text>
<text font="13" height="9" left="481" textpieces="0" top="378" width="350">[8] R. Fagin, J. Y. Halpern, and N. Megiddo. A logic for reasoning</text>
<text font="13" height="9" left="501" textpieces="0" top="390" width="304">about probabilities. Information and Computation, 1990.</text>
<text font="13" height="9" left="481" textpieces="0" top="403" width="322">[9] W. Feller. An Introduction to Probability Theory and its</text>
<text font="13" height="9" left="501" textpieces="0" top="415" width="248">Applications. Vol. I. John Wiley &amp; Sons, 1950.</text>
<text font="13" height="9" left="475" textpieces="0" top="429" width="348">[10] R. Jampani, F. Xu, M. Wu, L. L. Perez, C. M. Jermaine, and</text>
<text font="13" height="9" left="501" textpieces="0" top="441" width="299">P. J. Haas. MCDB: A monte carlo approach to managing</text>
<text font="13" height="9" left="501" textpieces="0" top="452" width="266">uncertain data. In SIGMOD, pages 687&#8211;700, 2008.</text>
<text font="13" height="9" left="475" textpieces="0" top="466" width="356">[11] B. Kanagal and A. Deshpande. Online &#64257;ltering, smoothing and</text>
<text font="13" height="9" left="501" textpieces="0" top="478" width="300">probabilistic modeling of streaming data. In ICDE, 2008.</text>
<text font="13" height="9" left="475" textpieces="0" top="491" width="305">[12] G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar.</text>
<text font="13" height="9" left="501" textpieces="0" top="503" width="301">Multilevel hypergraph partitioning: Applications in VLSI</text>
<text font="13" height="9" left="501" textpieces="0" top="515" width="285">domain. VLSI Systems, IEEE Transactions on, 2002.</text>
<text font="13" height="9" left="475" textpieces="0" top="529" width="322">[13] H. Kautz, B. Selman, and Y. Jiang. A general stochastic</text>
<text font="13" height="9" left="501" textpieces="0" top="541" width="319">approach to solving problems with hard and soft constraints.</text>
<text font="13" height="9" left="501" textpieces="0" top="553" width="292">Satis&#64257;ability Problem: Theory and Applications, 1997.</text>
<text font="13" height="9" left="475" textpieces="0" top="566" width="335">[14] S. Khot. Ruling out PTAS for graph min-bisection, densest</text>
<text font="13" height="9" left="501" textpieces="0" top="578" width="315">subgraph and bipartite clique. In Foundations of Computer</text>
<text font="13" height="9" left="501" textpieces="0" top="590" width="317">Science, 2004. Proceedings. 45th Annual IEEE Symposium</text>
<text font="13" height="9" left="501" textpieces="0" top="602" width="165">on, pages 136&#8211;145. IEEE, 2004.</text>
<text font="13" height="9" left="475" textpieces="0" top="615" width="305">[15] A. McCallum, K. Nigam, J. Rennie, and K. Seymore.</text>
<text font="13" height="9" left="501" textpieces="0" top="627" width="324">Automating the construction of internet portals with machine</text>
<text font="13" height="9" left="501" textpieces="0" top="639" width="320">learning. Information Retrieval Journal, 3(2):127&#8211;163, 2000.</text>
<text font="13" height="9" left="475" textpieces="1" top="653" width="329">[16] F. Niu, C. R&#180; e, A. Doan, and J. Shavlik. Tu&#64256;y: Scaling up</text>
<text font="13" height="9" left="501" textpieces="0" top="665" width="293">Statistical Inference in Markov Logic Networks using an</text>
<text font="13" height="10" left="501" textpieces="0" top="677" width="316">RDBMS, http://www.cs.wisc.edu/hazy/papers/tuffy-tr.pdf.</text>
<text font="13" height="9" left="501" textpieces="0" top="689" width="116">Working Paper, 2010.</text>
<text font="13" height="9" left="475" textpieces="0" top="702" width="324">[17] J. Pearl. Probabilistic Reasoning in Intelligent Systems:</text>
<text font="13" height="9" left="501" textpieces="0" top="714" width="205">Networks of Plausible Inference. 1988.</text>
<text font="13" height="9" left="475" textpieces="0" top="727" width="324">[18] H. Poon and P. Domingos. Joint inference in information</text>
<text font="13" height="9" left="501" textpieces="0" top="739" width="223">extraction. In AAAI, pages 913&#8211;918, 2007.</text>
<text font="13" height="9" left="475" textpieces="1" top="753" width="314">[19] C. R&#180; e, N. N. Dalvi, and D. Suciu. E&#64259;cient top-k query</text>
<text font="13" height="9" left="501" textpieces="0" top="765" width="333">evaluation on probabilistic data. In ICDE, pages 886&#8211;895, 2007.</text>
<text font="13" height="9" left="475" textpieces="1" top="778" width="356">[20] C. R&#180; e, J. Letchner, M. Balazinska, and D. Suciu. Event queries</text>
<text font="13" height="9" left="501" textpieces="0" top="790" width="289">on correlated probabilistic streams. In SIGMOD, 2008.</text>
<text font="13" height="9" left="475" textpieces="0" top="804" width="323">[21] M. Richardson and P. Domingos. Markov logic networks.</text>
<text font="13" height="9" left="501" textpieces="0" top="816" width="196">Machine Learning, 62:107&#8211;136, 2006.</text>
<text font="13" height="9" left="475" textpieces="0" top="829" width="343">[22] S. Riedel and I. Meza-Ruiz. Collective semantic role labeling</text>
<text font="13" height="9" left="501" textpieces="0" top="841" width="273">with Markov logic. In CoNLL, pages 193&#8211;197, 2008.</text>
<text font="13" height="9" left="475" textpieces="0" top="854" width="340">[23] P. Sen, A. Deshpande, and L. Getoor. PrDB: Managing and</text>
<text font="13" height="9" left="501" textpieces="0" top="866" width="319">exploiting rich correlations in probabilistic databases. VLDB</text>
<text font="13" height="9" left="501" textpieces="0" top="878" width="121">J., 18:1065&#8211;1090, 2009.</text>
<text font="13" height="9" left="475" textpieces="0" top="892" width="355">[24] H. Simon and S. Teng. How good is recursive bisection? SIAM</text>
<text font="13" height="9" left="501" textpieces="0" top="904" width="287">Journal on Scienti&#64257;c Computing, 18:1436&#8211;1445, 1997.</text>
<text font="13" height="9" left="475" textpieces="0" top="917" width="331">[25] P. Singla and P. Domingos. Entity resolution with Markov</text>
<text font="13" height="9" left="501" textpieces="0" top="929" width="194">logic. In ICDE, pages 572&#8211;582, 2006.</text>
<text font="13" height="9" left="475" textpieces="0" top="943" width="358">[26] V. Vazirani. Approximation Algorithms. Springer Verlag, 2001.</text>
<text font="13" height="9" left="475" textpieces="0" top="956" width="337">[27] D. Wang, E. Michelakis, M. Garofalakis, and J. Hellerstein.</text>
<text font="13" height="9" left="501" textpieces="0" top="968" width="319">Bayesstore: Managing large, uncertain data repositories with</text>
<text font="13" height="9" left="501" textpieces="0" top="980" width="300">probabilistic graphical models. Proceedings of the VLDB</text>
<text font="13" height="9" left="501" textpieces="0" top="992" width="171">Endowment, 1(1):340&#8211;351, 2008.</text>
<text font="13" height="9" left="475" textpieces="0" top="1005" width="330">[28] M. Wellman, J. Breese, and R. Goldman. From knowledge</text>
<text font="13" height="9" left="501" textpieces="0" top="1017" width="331">bases to decision models. The Knowledge Engineering Review,</text>
<text font="13" height="9" left="501" textpieces="0" top="1029" width="95">7(01):35&#8211;53, 1992.</text>
<text font="13" height="9" left="475" textpieces="0" top="1043" width="331">[29] F. Wu and D. Weld. Automatically re&#64257;ning the Wikipedia</text>
<text font="13" height="9" left="501" textpieces="0" top="1055" width="303">infobox ontology. In Proceeding of the 17th international</text>
<text font="13" height="9" left="501" textpieces="0" top="1067" width="323">conference on World Wide Web, pages 635&#8211;644. ACM, 2008.</text>
<text font="2" height="16" left="81" textpieces="0" top="83" width="93">APPENDIX</text>
<text font="2" height="16" left="81" textpieces="1" top="110" width="325">A.  MATERIAL FOR PRELIMINARIES</text>
<text font="2" height="16" left="81" textpieces="1" top="141" width="314">A.1  More Details on the MLN Program</text>
<text font="3" height="14" left="94" textpieces="0" top="162" width="345">Rules in MLNs are expressive and may involve data in</text>
<text font="3" height="12" left="81" textpieces="0" top="175" width="261">non-trivial ways. For example, consider F2:</text>
<text font="4" height="10" left="103" textpieces="2" top="201" width="315">wrote(x, p1), wrote(x, p2), cat(p1, c) =&gt; cat(p2, c)  (F2)</text>
<text font="3" height="12" left="87" textpieces="0" top="226" width="352">Intuitively, this rule says that all the papers written by</text>
<text font="3" height="12" left="81" textpieces="0" top="242" width="359">a particular person are likely to be in the same category.</text>
<text font="3" height="12" left="81" textpieces="1" top="257" width="359">Rules may also have existential quanti&#64257;ers: F4 in Figure 1</text>
<text font="3" height="12" left="81" textpieces="0" top="273" width="359">says &#8220;any paper in our database must have at least one au-</text>
<text font="3" height="12" left="81" textpieces="0" top="289" width="359">thor.&#8221; It is also a hard rule, which is indicated by the in&#64257;nite</text>
<text font="3" height="12" left="81" textpieces="0" top="304" width="359">weight, and so no possible world may violate this rule. The</text>
<text font="3" height="12" left="81" textpieces="0" top="320" width="359">weight of a formula may also be negative, which e&#64256;ectively</text>
<text font="3" height="12" left="81" textpieces="0" top="336" width="359">means that the negation of the formula is likely to hold. For</text>
<text font="3" height="12" left="81" textpieces="1" top="352" width="358">example, F5 models our belief that none or very few of the</text>
<text font="3" height="14" left="81" textpieces="0" top="367" width="359">unlabeled papers belong to &#8216;Networking&#8217;. Tuffy supports</text>
<text font="3" height="14" left="81" textpieces="0" top="383" width="359">all of these features. If the input MLN contains hard rules</text>
<text font="3" height="12" left="81" textpieces="0" top="399" width="359">(indicated by a weight of +&#8734; or &#8722;&#8734;), then we insist that</text>
<text font="3" height="12" left="81" textpieces="0" top="414" width="359">the set of possible worlds (Inst) only contain worlds that</text>
<text font="3" height="12" left="81" textpieces="0" top="430" width="359">satisfy every hard rule with +&#8734; and violate every rule with</text>
<text font="3" height="13" left="81" textpieces="0" top="445" width="28">&#8722;&#8734;.</text>
<text font="2" height="16" left="81" textpieces="1" top="470" width="219">A.2  Markov Random Field</text>
<text font="3" height="12" left="94" textpieces="0" top="493" width="345">A Boolean Markov Random Field (or Boolean Markov net-</text>
<text font="3" height="12" left="81" textpieces="0" top="508" width="359">work) is a model of the joint distribution of a set of Boolean</text>
<text font="3" height="12" left="81" textpieces="3" top="524" width="358">random variables &#175;  X = (X1, . . . , XN).  It is de&#64257;ned by a</text>
<text font="3" height="12" left="81" textpieces="0" top="540" width="359">hypergraph G = (X, E); for each hyperedge e &#8712; E there</text>
<text font="3" height="12" left="81" textpieces="0" top="555" width="358">is a potential function (aka &#8220;feature&#8221;) denoted &#966;e, which is</text>
<text font="3" height="12" left="81" textpieces="0" top="571" width="359">a function from the values of the set of variables in e to</text>
<text font="3" height="12" left="81" textpieces="0" top="587" width="359">non-negative real numbers. This de&#64257;nes a joint distribution</text>
<text font="3" height="12" left="81" textpieces="2" top="602" width="130">Pr( &#175;  X = &#175; x) as follows:</text>
<text font="3" height="12" left="179" textpieces="2" top="634" width="79">Pr( &#175;  X = &#175; x) =</text>
<text font="3" height="12" left="266" textpieces="0" top="625" width="7">1</text>
<text font="3" height="12" left="264" textpieces="0" top="643" width="9">Z</text>
<text font="1" height="8" left="278" textpieces="0" top="652" width="21">e&#8712;E</text>
<text font="3" height="12" left="302" textpieces="1" top="634" width="38">&#966;e(&#175; xe)</text>
<text font="3" height="12" left="81" textpieces="3" top="674" width="357">where &#175; x &#8712; {0, 1}N, Z is a normalization constant and &#175; xe</text>
<text font="3" height="12" left="81" textpieces="0" top="690" width="238">denotes the values of the variables in e.</text>
<text font="3" height="12" left="94" textpieces="2" top="706" width="346">Fix a set of constants C = {c1, . . . , cM}. An MLN de&#64257;nes</text>
<text font="3" height="12" left="81" textpieces="0" top="722" width="359">a Boolean Markov Random Field as follows: for each possi-</text>
<text font="3" height="12" left="81" textpieces="0" top="737" width="359">ble grounding of each predicate (i.e., atom), create a node</text>
<text font="3" height="12" left="81" textpieces="0" top="753" width="359">(and so a Boolean random variable). For example, there will</text>
<text font="3" height="12" left="81" textpieces="0" top="769" width="359">be a node refers(p1, p2) for each pair of papers p1, p2. For</text>
<text font="3" height="12" left="81" textpieces="1" top="784" width="358">each formula Fiwe ground it in all possible ways, then we</text>
<text font="3" height="12" left="81" textpieces="0" top="800" width="359">create a hyperedge e that contains the nodes corresponding</text>
<text font="3" height="12" left="81" textpieces="0" top="816" width="359">to all terms in the formula. For example, the key constraint</text>
<text font="3" height="12" left="81" textpieces="0" top="831" width="359">creates hyperedges for each paper and all of its potential</text>
<text font="3" height="12" left="81" textpieces="0" top="847" width="63">categories.</text>
<text font="2" height="16" left="81" textpieces="1" top="872" width="328">A.3  Optimizing MLN Grounding Process</text>
<text font="3" height="14" left="94" textpieces="0" top="894" width="345">Conceptually, we might ground an MLN formula by enu-</text>
<text font="3" height="12" left="81" textpieces="0" top="910" width="359">merating all possible assignments to its free variables. How-</text>
<text font="3" height="12" left="81" textpieces="0" top="925" width="359">ever, this is both impractical and unnecessary. For example,</text>
<text font="3" height="12" left="81" textpieces="1" top="941" width="358">if we ground F2exhaustively this way, the result would con-</text>
<text font="3" height="12" left="81" textpieces="1" top="957" width="358">tain |D|4 ground clauses. Fortunately, in practice a vast</text>
<text font="3" height="12" left="81" textpieces="0" top="972" width="359">majority of ground clauses are satis&#64257;ed by evidence regard-</text>
<text font="3" height="12" left="81" textpieces="0" top="988" width="359">less of the assignments to unknown truth values; we can</text>
<text font="3" height="12" left="81" textpieces="0" top="1004" width="359">safely discard such clauses [40]. Consider the ground clause</text>
<text font="3" height="12" left="81" textpieces="0" top="1020" width="12">g&#175;</text>
<text font="1" height="8" left="87" textpieces="0" top="1026" width="6">d</text>
<text font="3" height="12" left="100" textpieces="2" top="1020" width="339">of F2 where &#175; d =(&#8216;Joe&#8217;, &#8216;P2&#8217;, &#8216;P3&#8217;, &#8216;DB&#8217;). Suppose that</text>
<text font="3" height="11" left="81" textpieces="1" top="1036" width="279">wrote(&#8216;Joe&#8217;, &#8216;P3&#8217;) is known to be false, then g&#175;</text>
<text font="1" height="8" left="353" textpieces="0" top="1041" width="6">d</text>
<text font="3" height="12" left="364" textpieces="0" top="1035" width="76">will be satis-</text>
<text font="3" height="12" left="81" textpieces="0" top="1051" width="284">&#64257;ed no matter how the other atoms are set (g&#175;</text>
<text font="1" height="8" left="359" textpieces="0" top="1057" width="6">d</text>
<text font="3" height="12" left="371" textpieces="0" top="1051" width="69">is an impli-</text>
<text font="3" height="12" left="81" textpieces="1" top="1067" width="198">cation). Hence, we can ignore g&#175;</text>
<text font="1" height="8" left="272" textpieces="0" top="1073" width="6">d</text>
<text font="3" height="12" left="283" textpieces="0" top="1067" width="147">during the search phase.</text>
<text font="3" height="12" left="489" textpieces="0" top="86" width="345">Pushing this idea further, [39] proposes a method called</text>
<text font="3" height="14" left="475" textpieces="0" top="102" width="359">&#8220;lazy inference&#8221; which is implemented by Alchemy. Specif-</text>
<text font="3" height="14" left="475" textpieces="0" top="118" width="359">ically, Alchemy works under the more aggressive hypoth-</text>
<text font="3" height="12" left="475" textpieces="0" top="133" width="359">esis that most atoms will be false in the &#64257;nal solution, and</text>
<text font="3" height="12" left="475" textpieces="0" top="149" width="359">in fact throughout the entire execution. To make this idea</text>
<text font="3" height="12" left="475" textpieces="0" top="165" width="359">precise, call a ground clause active if it can be violated by</text>
<text font="3" height="12" left="475" textpieces="0" top="180" width="359">&#64258;ipping zero or more active atoms, where an atom is ac-</text>
<text font="3" height="12" left="475" textpieces="0" top="196" width="359">tive if its value &#64258;ips at any point during execution. Observe</text>
<text font="3" height="12" left="475" textpieces="0" top="212" width="312">that in the preceding example the ground clause g</text>
<text font="1" height="8" left="789" textpieces="2" top="216" width="45">&#175;  d  is not</text>
<text font="3" height="14" left="475" textpieces="0" top="228" width="359">active. Alchemy keeps only active ground clauses in mem-</text>
<text font="3" height="12" left="475" textpieces="0" top="243" width="359">ory, which can be much smaller than the full set of ground</text>
<text font="3" height="12" left="475" textpieces="0" top="259" width="359">clauses. Furthermore, as on-the-&#64258;y incremental grounding</text>
<text font="3" height="14" left="475" textpieces="0" top="275" width="359">is more expensive than batch grounding, Alchemy uses the</text>
<text font="3" height="12" left="475" textpieces="0" top="290" width="359">following one-step look-ahead strategy: assume all atoms</text>
<text font="3" height="12" left="475" textpieces="0" top="306" width="359">are inactive and compute active clauses; activate the atoms</text>
<text font="3" height="12" left="475" textpieces="0" top="322" width="359">in the grounding result and recompute active clauses. This</text>
<text font="3" height="12" left="475" textpieces="0" top="337" width="359">&#8220;look-ahead&#8221; procedure could be repeatedly applied until</text>
<text font="3" height="14" left="475" textpieces="0" top="353" width="359">convergence, resulting in an active closure. Tuffy imple-</text>
<text font="3" height="12" left="475" textpieces="0" top="369" width="176">ments this closure algorithm.</text>
<text font="2" height="16" left="475" textpieces="1" top="394" width="237">A.4  The WalkSAT Algorithm</text>
<text font="3" height="12" left="489" textpieces="0" top="416" width="342">We list the pseudocode of WalkSAT [13] in Algorithm 1.</text>
<text font="3" height="14" left="475" textpieces="0" top="450" width="250">Algorithm 1 The WalkSAT Algorithm</text>
<text font="3" height="12" left="475" textpieces="0" top="469" width="163">Input: A: an set of atoms</text>
<text font="3" height="12" left="475" textpieces="0" top="484" width="273">Input: C: an set of weighted ground clauses</text>
<text font="3" height="12" left="475" textpieces="0" top="500" width="169">Input: MaxFlips, MaxTries</text>
<text font="3" height="12" left="475" textpieces="0" top="516" width="232">Output: &#963;&#8727;: a truth assignment to A</text>
<text font="3" height="12" left="481" textpieces="0" top="532" width="113">1: lowCost &#8592; +&#8734;</text>
<text font="3" height="12" left="481" textpieces="0" top="547" width="188">2: for try = 1 to MaxTries do</text>
<text font="3" height="12" left="481" textpieces="1" top="563" width="259">3:   &#963; &#8592; a random truth assignment to A</text>
<text font="3" height="12" left="481" textpieces="1" top="579" width="209">4:   for flip = 1 to MaxFlips do</text>
<text font="3" height="12" left="481" textpieces="1" top="594" width="268">5:      pick a random c &#8712; C that is violated</text>
<text font="3" height="12" left="481" textpieces="1" top="610" width="309">6:      rand &#8592; a random &#64258;oat between 0.0 and 1.0</text>
<text font="3" height="12" left="481" textpieces="1" top="626" width="161">7:      if rand &#8804; 0.5 then</text>
<text font="3" height="12" left="481" textpieces="1" top="641" width="204">8:        &#64258;ip a random atom in c</text>
<text font="3" height="12" left="481" textpieces="1" top="657" width="70">9:      else</text>
<text font="3" height="12" left="475" textpieces="1" top="673" width="338">10:        &#64258;ip an atom in c s.t. the cost decreases most</text>
<text font="3" height="12" left="475" textpieces="1" top="688" width="199">11:      if cost &lt; lowCost then</text>
<text font="3" height="12" left="475" textpieces="2" top="704" width="219">12:        lowCost &#8592; cost, &#963;&#8727;&#8592; &#963;</text>
<text font="2" height="16" left="475" textpieces="2" top="752" width="258">A.5  Marginal Inference ofMLNs</text>
<text font="3" height="12" left="489" textpieces="0" top="774" width="345">In marginal inference, we estimate the marginal proba-</text>
<text font="3" height="12" left="475" textpieces="0" top="790" width="359">bility of atoms. Since this problem is generally intractable,</text>
<text font="3" height="12" left="475" textpieces="0" top="806" width="359">we usually resort to sampling methods. The state-of-the-art</text>
<text font="3" height="12" left="475" textpieces="0" top="821" width="359">marginal inference algorithm is MC-SAT [38], which is im-</text>
<text font="3" height="14" left="475" textpieces="0" top="837" width="359">plemented in both Alchemy and Tuffy. In MC-SAT, each</text>
<text font="3" height="12" left="475" textpieces="0" top="853" width="359">sampling step consists of a call to a heuristic SAT sampler</text>
<text font="3" height="12" left="475" textpieces="0" top="868" width="359">named SampleSAT [44]. Essentially, SampleSAT is a combi-</text>
<text font="3" height="14" left="475" textpieces="0" top="884" width="359">nation of simulated annealing and WalkSAT. And so, Tuffy</text>
<text font="3" height="12" left="475" textpieces="0" top="900" width="359">is able to perform marginal inference more e&#64259;ciently as well.</text>
<text font="3" height="12" left="475" textpieces="0" top="918" width="359">Alchemy also implements a lifted algorithm for marginal</text>
<text font="3" height="12" left="475" textpieces="0" top="931" width="359">inference [42]; it is future work to extend our study to lifted</text>
<text font="3" height="12" left="475" textpieces="0" top="947" width="70">approaches.</text>
<text font="2" height="16" left="475" textpieces="1" top="981" width="262">B.  MATERIAL FOR SYSTEMS</text>
<text font="2" height="16" left="475" textpieces="1" top="1013" width="357">B.1  A Compilation Algorithm for Grounding</text>
<text font="3" height="12" left="489" textpieces="0" top="1035" width="345">Algorithm 2 is a basic algorithm of expressing the ground-</text>
<text font="3" height="14" left="475" textpieces="0" top="1051" width="359">ing process of an MLN formula in SQL. To support existen-</text>
<text font="3" height="12" left="475" textpieces="0" top="1067" width="359">tial quanti&#64257;ers, we used PostgreSQL&#8217;s array aggregate fea-</text>
<text font="3" height="12" left="81" textpieces="0" top="86" width="359">ture. The ideas in Appendix A.3 can be easily implemented</text>
<text font="3" height="12" left="81" textpieces="0" top="102" width="149">on top of this algorithm.</text>
<text font="3" height="12" left="81" textpieces="0" top="132" width="238">Algorithm 2 MLN Grounding in SQL</text>
<text font="3" height="14" left="81" textpieces="0" top="154" width="196">Input: an MLN formula &#966; = &#8744;k</text>
<text font="1" height="8" left="270" textpieces="2" top="159" width="169">i=1li where each li is a literal</text>
<text font="3" height="12" left="104" textpieces="0" top="169" width="204">supported by predicate table r(li)</text>
<text font="3" height="12" left="81" textpieces="0" top="185" width="250">Output: a SQL query Q that grounds &#966;</text>
<text font="3" height="12" left="87" textpieces="0" top="201" width="342">1: FROM clause of Q includes &#8216;r(li) ti&#8217; for each literal li</text>
<text font="3" height="12" left="87" textpieces="0" top="216" width="348">2: SELECT clause of Q contains &#8216;ti.aid&#8217; for each literal li</text>
<text font="3" height="12" left="87" textpieces="0" top="232" width="352">3: For each positive (resp. negative) literal li, there is a</text>
<text font="3" height="12" left="104" textpieces="1" top="248" width="336">WHERE predicate &#8216;ti.truth = true&#8217; (resp. &#8216;ti.truth =</text>
<text font="3" height="12" left="104" textpieces="0" top="263" width="36">false&#8217;)</text>
<text font="3" height="12" left="87" textpieces="0" top="279" width="353">4: For each variable x in &#966;, there is a WHERE predicate</text>
<text font="3" height="12" left="104" textpieces="0" top="295" width="334">that equates the corresponding columns of ti&#8217;s with li</text>
<text font="3" height="12" left="104" textpieces="0" top="310" width="75">containing x</text>
<text font="3" height="12" left="87" textpieces="0" top="326" width="352">5: For each constant argument of li, there is an equal-</text>
<text font="3" height="12" left="104" textpieces="0" top="342" width="239">constant WHERE predicate for table ti</text>
<text font="3" height="12" left="87" textpieces="0" top="358" width="353">6: Form a conjunction with the above WHERE predicates</text>
<text font="2" height="16" left="81" textpieces="1" top="402" width="318">B.2  Implementing WalkSAT in RDBMS</text>
<text font="3" height="12" left="94" textpieces="0" top="424" width="345">WalkSAT is a stochastic local search algorithm; its ran-</text>
<text font="3" height="12" left="81" textpieces="0" top="440" width="359">dom access patterns pose considerable challenges to the de-</text>
<text font="3" height="14" left="81" textpieces="0" top="456" width="359">sign of Tuffy. More speci&#64257;cally, the following operations</text>
<text font="3" height="12" left="81" textpieces="0" top="471" width="359">are di&#64259;cult to implement e&#64259;ciently with on-disk data: 1)</text>
<text font="3" height="12" left="81" textpieces="0" top="487" width="359">uniformly sample an unsatis&#64257;ed clause; 2) random access</text>
<text font="3" height="12" left="81" textpieces="0" top="503" width="359">(read/write) to per-atom or per-clause data structures; and</text>
<text font="3" height="12" left="81" textpieces="0" top="518" width="359">3) traverse clauses involving a given atom. Atoms are cached</text>
<text font="3" height="12" left="81" textpieces="0" top="534" width="359">as in-memory arrays, while the per-clause data structures</text>
<text font="3" height="12" left="81" textpieces="0" top="550" width="359">are read-only. Each step of WalkSAT involves a scan over</text>
<text font="3" height="12" left="81" textpieces="0" top="565" width="318">the clauses and many random accesses to the atoms.</text>
<text font="3" height="12" left="94" textpieces="0" top="581" width="345">Although our design process iterated over numerous com-</text>
<text font="3" height="12" left="81" textpieces="0" top="597" width="359">binations of various design choices, we were still unable to</text>
<text font="3" height="12" left="81" textpieces="0" top="612" width="359">reduce the gap as reported in Section 4.2. For example, com-</text>
<text font="3" height="12" left="81" textpieces="0" top="628" width="359">pared to clause table scans, one might suspect that index-</text>
<text font="3" height="12" left="81" textpieces="0" top="644" width="359">ing could improve search speed by reading less data at each</text>
<text font="3" height="12" left="81" textpieces="0" top="660" width="359">step. However, we actually found that the cost of maintain-</text>
<text font="3" height="12" left="81" textpieces="0" top="675" width="359">ing indices often outweighs the bene&#64257;t provided by indexing.</text>
<text font="3" height="12" left="81" textpieces="0" top="691" width="359">Moreover, we found it very di&#64259;cult to get around RDBMS</text>
<text font="3" height="12" left="81" textpieces="0" top="707" width="309">overhead such as PostgreSQL&#8217;s mandatory MVCC.</text>
<text font="2" height="16" left="81" textpieces="1" top="730" width="349">B.3  Illustrating Tuffy&#8217;s Hybrid Architecture</text>
<text font="3" height="12" left="94" textpieces="0" top="753" width="345">Figure 7 illustrates the hybrid memory management ap-</text>
<text font="3" height="14" left="81" textpieces="0" top="768" width="359">proach of Tuffy. Alchemy is a representative of prior art</text>
<text font="3" height="12" left="81" textpieces="0" top="786" width="359">MLN systems, which uses RAM for both grounding and</text>
<text font="3" height="14" left="81" textpieces="0" top="800" width="359">search; Tuffy-mm is a version of Tuffy we developed that</text>
<text font="3" height="14" left="81" textpieces="0" top="815" width="359">uses an RDBMS for all memory management; and Tuffy</text>
<text font="3" height="12" left="81" textpieces="0" top="831" width="304">is the hybrid approach as discussed in Section 3.2.</text>
<text font="6" height="7" left="271" textpieces="1" top="904" width="-43">RDBMS                                 RAM </text>
<text font="6" height="7" left="213" textpieces="2" top="948" width="139">RAM                  RDBMS                  RAM </text>
<text font="6" height="7" left="334" textpieces="1" top="904" width="-135">RDBMS                                                                                   Grounding </text>
<text font="13" height="11" left="159" textpieces="0" top="946" width="32">Search </text>
<text font="9" height="11" left="201" textpieces="2" top="863" width="157">Alchemy     Tuffy-mm       Tuffy </text>
<text font="3" height="12" left="127" textpieces="0" top="991" width="267">Figure 7: Comparison of architectures</text>
<text font="2" height="16" left="81" textpieces="1" top="1028" width="323">B.4  MLNs Causing MRF Fragmentation</text>
<text font="3" height="12" left="94" textpieces="0" top="1053" width="345">MLN rules usually model the interaction of relationships</text>
<text font="3" height="12" left="81" textpieces="0" top="1067" width="359">and attributes of some underlying entities. As such, one can</text>
<text font="3" height="12" left="475" textpieces="0" top="86" width="359">de&#64257;ne entity-based transitive closures, which directly cor-</text>
<text font="3" height="12" left="475" textpieces="0" top="102" width="359">responds to components in the MRF. Since in real world</text>
<text font="3" height="12" left="475" textpieces="0" top="118" width="359">data the interactions are usually sparse, one can expect to</text>
<text font="3" height="12" left="475" textpieces="0" top="133" width="359">see multiple components in the MRF. A concrete example is</text>
<text font="3" height="12" left="475" textpieces="0" top="149" width="359">the paper classi&#64257;cation running example, where the primary</text>
<text font="3" height="12" left="475" textpieces="0" top="165" width="359">entities are papers, and the interactions are de&#64257;ned by cita-</text>
<text font="3" height="12" left="475" textpieces="0" top="180" width="359">tions and common authors. Indeed, our RC dataset yields</text>
<text font="3" height="12" left="475" textpieces="0" top="196" width="310">hundreds of components in the MRF (see Table 5).</text>
<text font="2" height="16" left="475" textpieces="1" top="220" width="140">B.5  Theorem 3.1</text>
<text font="3" height="12" left="490" textpieces="0" top="251" width="344">Proof of Theorem 3.1. We follow the notations of the</text>
<text font="3" height="12" left="475" textpieces="0" top="264" width="359">theorem. Without loss of generality and for ease of notation,</text>
<text font="3" height="12" left="475" textpieces="0" top="280" width="359">suppose H = {1, . . . , N }. Denote by &#8486; the state space of G.</text>
<text font="3" height="12" left="475" textpieces="1" top="296" width="359">Let Qk&#8838; &#8486; be the set of states of G where there are exactly</text>
<text font="3" height="12" left="475" textpieces="0" top="311" width="359">k non-optimal components. For any state x &#8712; &#8486;, de&#64257;ne</text>
<text font="3" height="12" left="475" textpieces="1" top="327" width="359">H(x) = E[Hx(Q0)], i.e., the expected hitting time of an</text>
<text font="3" height="12" left="475" textpieces="1" top="343" width="359">optimal state from x when running WalkSAT. De&#64257;ne fk=</text>
<text font="3" height="12" left="475" textpieces="3" top="358" width="359">minx&#8712;QkH(x); in particular, f0 = 0, and f1 corresponds</text>
<text font="3" height="12" left="475" textpieces="0" top="374" width="359">to some state that di&#64256;ers from an optimal by only one bit.</text>
<text font="3" height="12" left="475" textpieces="2" top="390" width="358">De&#64257;ne gk= fk+1&#8722; fk. For any x, y &#8712; &#8486;, let Pr(x &#8594; y) be</text>
<text font="3" height="12" left="475" textpieces="0" top="405" width="359">the transition probability of WalkSAT, i.e., the probability</text>
<text font="3" height="12" left="475" textpieces="0" top="421" width="359">that next state will be y given current state x. Note that</text>
<text font="3" height="12" left="475" textpieces="0" top="437" width="359">Pr(x &#8594; y) &gt; 0 only if y &#8712; N (x), where N (x) is the set of</text>
<text font="3" height="12" left="475" textpieces="0" top="452" width="359">states that di&#64256;er from x by at most one bit. For any A &#8838; &#8486;,</text>
<text font="3" height="12" left="475" textpieces="0" top="468" width="120">de&#64257;ne Pr(x &#8594; A) =</text>
<text font="1" height="8" left="613" textpieces="0" top="475" width="22">y&#8712;A</text>
<text font="3" height="12" left="638" textpieces="0" top="468" width="66">Pr(x &#8594; y).</text>
<text font="3" height="12" left="489" textpieces="0" top="484" width="149">For any x &#8712; Qk, we have</text>
<text font="3" height="12" left="512" textpieces="2" top="510" width="90">H(x)  =  1 +</text>
<text font="1" height="8" left="604" textpieces="0" top="529" width="21">y&#8712;&#8486;</text>
<text font="3" height="12" left="628" textpieces="0" top="510" width="93">Pr(x &#8594; y)H(y)</text>
<text font="3" height="12" left="556" textpieces="1" top="549" width="46">=  1 +</text>
<text font="1" height="8" left="604" textpieces="0" top="569" width="98">t&#8712;{&#8722;1,0,1} y&#8712;Qk+t</text>
<text font="3" height="12" left="705" textpieces="0" top="549" width="93">Pr(x &#8594; y)H(y)</text>
<text font="3" height="13" left="556" textpieces="1" top="589" width="46">&#8805;  1 +</text>
<text font="1" height="8" left="604" textpieces="0" top="610" width="98">t&#8712;{&#8722;1,0,1} y&#8712;Qk+t</text>
<text font="3" height="12" left="705" textpieces="0" top="590" width="92">Pr(x &#8594; y)fk+t.</text>
<text font="3" height="12" left="475" textpieces="0" top="630" width="38">De&#64257;ne</text>
<text font="3" height="12" left="519" textpieces="1" top="654" width="17">Px</text>
<text font="1" height="8" left="528" textpieces="2" top="659" width="143">+ = Pr(x &#8594; Qk+1),     P</text>
<text font="1" height="8" left="673" textpieces="0" top="651" width="6">x</text>
<text font="1" height="8" left="671" textpieces="1" top="659" width="117">&#8722; = Pr(x &#8594; Qk&#8722;1),</text>
<text font="3" height="12" left="475" textpieces="1" top="678" width="163">then Pr(x &#8594; Qk) = 1 &#8722; Px</text>
<text font="1" height="8" left="630" textpieces="0" top="684" width="8">+</text>
<text font="3" height="13" left="643" textpieces="0" top="677" width="29">&#8722; Px</text>
<text font="1" height="8" left="665" textpieces="0" top="683" width="40">&#8722;, and</text>
<text font="3" height="12" left="503" textpieces="1" top="702" width="133">H(x) &#8805; 1 + fk(1 &#8722; Px</text>
<text font="1" height="8" left="628" textpieces="0" top="708" width="8">+</text>
<text font="3" height="13" left="640" textpieces="1" top="701" width="31">&#8722; Px</text>
<text font="1" height="8" left="663" textpieces="0" top="707" width="67">&#8722;) + fk&#8722;1P</text>
<text font="1" height="8" left="733" textpieces="0" top="699" width="6">x</text>
<text font="1" height="8" left="731" textpieces="1" top="707" width="62">&#8722; + fk+1P</text>
<text font="1" height="8" left="795" textpieces="0" top="699" width="6">x</text>
<text font="1" height="8" left="793" textpieces="0" top="708" width="12">+.</text>
<text font="3" height="12" left="489" textpieces="0" top="726" width="345">Since this inequality holds for any x &#8712; Qk, we can &#64257;x it</text>
<text font="3" height="12" left="475" textpieces="4" top="742" width="341">to be some x&#8727; &#8712; Qk s.t. H(x&#8727;) = fk. Then gk&#8722;1Px&#8727;</text>
<text font="1" height="8" left="802" textpieces="0" top="747" width="9">&#8722;</text>
<text font="3" height="13" left="823" textpieces="0" top="741" width="11">&#8805;</text>
<text font="3" height="12" left="475" textpieces="1" top="760" width="60">1 + gkPx&#8727;</text>
<text font="1" height="8" left="521" textpieces="1" top="765" width="137">+  , which implies gk&#8722;1</text>
<text font="3" height="13" left="663" textpieces="1" top="759" width="51">&#8805; gkPx&#8727;</text>
<text font="1" height="8" left="700" textpieces="1" top="765" width="37">+  /Px</text>
<text font="6" height="7" left="739" textpieces="0" top="754" width="6">&#8727;</text>
<text font="1" height="8" left="731" textpieces="1" top="765" width="19">&#8722;  .</text>
<text font="3" height="12" left="489" textpieces="1" top="776" width="357">Now without loss of generality assume that in x&#8727;, G1, . . . , Gk</text>
<text font="3" height="12" left="475" textpieces="1" top="791" width="340">are non-optimal while Gk+1, . . . , GN are optimal. Let x&#8727;</text>
<text font="1" height="8" left="809" textpieces="0" top="797" width="4">i</text>
<text font="3" height="12" left="820" textpieces="0" top="791" width="14">be</text>
<text font="3" height="12" left="475" textpieces="1" top="807" width="233">the projection of x&#8727;on Gi. Then since</text>
<text font="3" height="12" left="490" textpieces="0" top="843" width="9">P</text>
<text font="1" height="8" left="501" textpieces="0" top="839" width="12">x&#8727;</text>
<text font="1" height="8" left="499" textpieces="1" top="848" width="30">&#8722;   =</text>
<text font="1" height="8" left="549" textpieces="0" top="830" width="6">k</text>
<text font="1" height="8" left="549" textpieces="0" top="840" width="5">1</text>
<text font="3" height="12" left="558" textpieces="0" top="833" width="30">vi(x&#8727;</text>
<text font="1" height="8" left="583" textpieces="0" top="839" width="4">i</text>
<text font="3" height="12" left="589" textpieces="0" top="833" width="37">)&#945;i(x&#8727;</text>
<text font="1" height="8" left="621" textpieces="0" top="839" width="4">i</text>
<text font="3" height="12" left="628" textpieces="0" top="833" width="5">)</text>
<text font="1" height="8" left="567" textpieces="0" top="850" width="9">N</text>
<text font="1" height="8" left="567" textpieces="0" top="861" width="5">1</text>
<text font="3" height="12" left="579" textpieces="0" top="854" width="30">vi(x&#8727;</text>
<text font="1" height="8" left="604" textpieces="0" top="860" width="4">i</text>
<text font="3" height="12" left="610" textpieces="0" top="854" width="5">)</text>
<text font="3" height="12" left="635" textpieces="1" top="843" width="29">,  P</text>
<text font="1" height="8" left="666" textpieces="0" top="839" width="12">x&#8727;</text>
<text font="1" height="8" left="664" textpieces="1" top="848" width="30">+   =</text>
<text font="1" height="8" left="714" textpieces="0" top="829" width="9">N</text>
<text font="1" height="8" left="714" textpieces="0" top="839" width="20">k+1</text>
<text font="3" height="12" left="737" textpieces="0" top="832" width="31">vj(x&#8727;</text>
<text font="1" height="8" left="763" textpieces="3" top="837" width="50">j)&#946;j(x&#8727;  j)</text>
<text font="1" height="8" left="739" textpieces="0" top="850" width="9">N</text>
<text font="1" height="8" left="739" textpieces="0" top="861" width="5">1</text>
<text font="3" height="12" left="752" textpieces="0" top="854" width="30">vi(x&#8727;</text>
<text font="1" height="8" left="776" textpieces="0" top="860" width="4">i</text>
<text font="3" height="12" left="783" textpieces="0" top="854" width="5">)</text>
<text font="3" height="12" left="815" textpieces="0" top="843" width="4">,</text>
<text font="3" height="12" left="475" textpieces="0" top="877" width="48">we have</text>
<text font="3" height="12" left="519" textpieces="1" top="910" width="59">gk&#8722;1&#8805; gk</text>
<text font="1" height="8" left="595" textpieces="0" top="896" width="9">N</text>
<text font="1" height="8" left="595" textpieces="0" top="907" width="20">k+1</text>
<text font="3" height="12" left="618" textpieces="0" top="899" width="31">vj(x&#8727;</text>
<text font="1" height="8" left="644" textpieces="0" top="905" width="23">j)&#946;j</text>
<text font="3" height="12" left="669" textpieces="0" top="899" width="19">(x&#8727;</text>
<text font="1" height="8" left="683" textpieces="0" top="905" width="10">j)</text>
<text font="1" height="8" left="603" textpieces="0" top="918" width="6">k</text>
<text font="1" height="8" left="603" textpieces="0" top="929" width="5">1</text>
<text font="3" height="12" left="612" textpieces="0" top="921" width="30">vi(x&#8727;</text>
<text font="1" height="8" left="636" textpieces="0" top="928" width="4">i</text>
<text font="3" height="12" left="643" textpieces="0" top="921" width="37">)&#945;i(x&#8727;</text>
<text font="1" height="8" left="675" textpieces="0" top="928" width="4">i</text>
<text font="3" height="12" left="682" textpieces="0" top="921" width="5">)</text>
<text font="3" height="13" left="700" textpieces="0" top="909" width="27">&#8805; gk</text>
<text font="3" height="12" left="730" textpieces="0" top="901" width="54">r(N &#8722; k)</text>
<text font="3" height="12" left="753" textpieces="0" top="919" width="7">k</text>
<text font="3" height="12" left="786" textpieces="0" top="910" width="4">,</text>
<text font="3" height="12" left="475" textpieces="0" top="945" width="359">where the second inequality follows from the de&#64257;nition of r.</text>
<text font="3" height="12" left="489" textpieces="2" top="960" width="345">For all k &#8804; rN/(r + 2), we have gk&#8722;1&#8805; 2gk. Since gk &#8805;</text>
<text font="3" height="12" left="475" textpieces="2" top="978" width="359">1 for any k, f1 = g0 &#8805; 2rN/(r+2). That is, not aware of</text>
<text font="3" height="12" left="475" textpieces="0" top="993" width="359">components, WalkSAT would take an exponential number</text>
<text font="3" height="12" left="475" textpieces="0" top="1009" width="359">of steps in expectation to correct the last bit to reach an</text>
<text font="3" height="12" left="475" textpieces="0" top="1025" width="58">optimum.</text>
<text font="3" height="12" left="475" textpieces="0" top="1051" width="359">According to this theorem, the gap on Example 1 is at least</text>
<text font="3" height="12" left="475" textpieces="0" top="1067" width="358">2N/3; in fact, a more detailed analysis reveals that the gap</text>
<text font="1" height="9" left="186" textpieces="0" top="155" width="13">500</text>
<text font="1" height="9" left="182" textpieces="0" top="132" width="18">1000</text>
<text font="1" height="9" left="182" textpieces="0" top="108" width="18">1500</text>
<text font="1" height="9" left="182" textpieces="0" top="85" width="18">2000</text>
<text font="1" height="9" left="205" textpieces="4" top="167" width="140">0         20        40        60        80</text>
<text font="10" height="12" left="177" textpieces="0" top="125" width="0">cost </text>
<text font="10" height="12" left="248" textpieces="0" top="180" width="54">time (sec) </text>
<text font="8" height="10" left="219" textpieces="0" top="91" width="113">Performance on Example 1 </text>
<text font="1" height="9" left="255" textpieces="0" top="114" width="69">Tuffy-p (diamonds) </text>
<text font="1" height="9" left="255" textpieces="0" top="125" width="70">Alchemy (triangles) </text>
<text font="1" height="9" left="255" textpieces="0" top="144" width="20">Tuffy </text>
<text font="3" height="12" left="102" textpieces="0" top="201" width="317">Figure 8: E&#64256;ect of partitioning on Example 1</text>
<text font="3" height="12" left="81" textpieces="1" top="229" width="94">is at least  N &#8722;1</text>
<text font="6" height="7" left="159" textpieces="0" top="235" width="8">N</text>
<text font="6" height="7" left="161" textpieces="0" top="242" width="5">2</text>
<text font="3" height="13" left="186" textpieces="0" top="229" width="253">&#8776; &#920;(2N/&#8730;N ). Figure 8 shows the exper-</text>
<text font="3" height="14" left="81" textpieces="0" top="249" width="359">iment results of running Alchemy, Tuffy, and Tuffy-p</text>
<text font="3" height="14" left="81" textpieces="0" top="265" width="359">(i.e., Tuffy without partitioning) on Example 1 with 1000</text>
<text font="3" height="12" left="81" textpieces="0" top="280" width="359">components. Note that the analysis of Theorem 3.1 actually</text>
<text font="3" height="12" left="81" textpieces="0" top="296" width="359">applies to not only WalkSAT, but stochastic local search in</text>
<text font="3" height="12" left="81" textpieces="0" top="312" width="359">general. Since stochastic local search algorithms are used</text>
<text font="3" height="12" left="81" textpieces="0" top="328" width="359">in many statistical models, we believe that our observation</text>
<text font="3" height="12" left="81" textpieces="0" top="343" width="359">here and corresponding techniques have much wider impli-</text>
<text font="3" height="14" left="81" textpieces="0" top="359" width="174">cations than MLN inference.</text>
<text font="2" height="16" left="81" textpieces="1" top="383" width="277">B.6  Hardness of MRF Partitioning</text>
<text font="3" height="12" left="94" textpieces="0" top="406" width="345">A bisection of a graph G = (V, E) with an even number</text>
<text font="3" height="12" left="81" textpieces="1" top="421" width="359">of vertices is a pair of disjoint subsets V1, V2 &#8834; V of equal</text>
<text font="3" height="12" left="81" textpieces="0" top="437" width="359">size. The cost of a bisection is the number of edges adjacent</text>
<text font="3" height="12" left="81" textpieces="1" top="453" width="357">to both V1 and V2. The problem of Minimum Graph Bisec-</text>
<text font="3" height="12" left="81" textpieces="0" top="468" width="359">tion (MGB) is to &#64257;nd a bisection with minimum cost. This</text>
<text font="3" height="12" left="81" textpieces="0" top="484" width="359">problem admits no PTAS [14]. The hardness of MGB di-</text>
<text font="3" height="12" left="81" textpieces="0" top="500" width="359">rectly implies the hardness of partitioning MRFs. As such,</text>
<text font="3" height="12" left="81" textpieces="0" top="515" width="359">one may wonder if it still holds w.r.t. the domain size for</text>
<text font="3" height="12" left="81" textpieces="0" top="531" width="359">a given MLN program (hence of size O(1)). The following</text>
<text font="3" height="12" left="81" textpieces="0" top="547" width="229">theorem shows that the answer is yes.</text>
<text font="3" height="12" left="96" textpieces="0" top="574" width="344">Theorem B.1. MGB can be reduced to the problem of</text>
<text font="3" height="12" left="81" textpieces="0" top="588" width="359">&#64257;nding a minimum bisection of the MRF generated an MLN</text>
<text font="3" height="12" left="81" textpieces="0" top="603" width="76">of size O(1).</text>
<text font="3" height="12" left="96" textpieces="0" top="628" width="344">Proof. Consider the MLN that contains a single formula</text>
<text font="3" height="12" left="81" textpieces="0" top="642" width="130">of the following form:</text>
<text font="3" height="12" left="200" textpieces="0" top="666" width="121">p(x), r(x, y) &#8594; p(y),</text>
<text font="3" height="12" left="81" textpieces="0" top="690" width="359">where p is query and r is evidence. For any graph G =</text>
<text font="3" height="12" left="81" textpieces="0" top="706" width="359">(V, E), we can set the domain of the predicates to be V ,</text>
<text font="3" height="12" left="81" textpieces="0" top="721" width="359">and let r = E. The MRF generated by the above MLN</text>
<text font="3" height="12" left="81" textpieces="0" top="737" width="321">(using techniques in Appendix A.3) is identical to G.</text>
<text font="2" height="16" left="81" textpieces="1" top="762" width="265">B.7  MRF Partitioning Algorithm</text>
<text font="3" height="12" left="94" textpieces="0" top="784" width="345">We provide a very simple MRF partitioning algorithm</text>
<text font="3" height="12" left="81" textpieces="0" top="800" width="359">(Algorithm 3) that is inspired by Kruskal&#8217;s minimum span-</text>
<text font="3" height="12" left="81" textpieces="0" top="816" width="359">ning tree algorithm. It agglomeratively merges atoms into</text>
<text font="3" height="12" left="81" textpieces="0" top="831" width="359">partitions with one scan of the clauses sorted in the (de-</text>
<text font="3" height="12" left="81" textpieces="0" top="847" width="359">scending) absolute values of weights. The hope is to avoid</text>
<text font="3" height="12" left="81" textpieces="0" top="863" width="359">cutting high-weighted clauses, thereby (heuristically) mini-</text>
<text font="3" height="12" left="81" textpieces="0" top="878" width="151">mizing weighted cut size.</text>
<text font="3" height="12" left="94" textpieces="0" top="894" width="345">To explain the partitioning procedure, we provide the fol-</text>
<text font="3" height="12" left="81" textpieces="0" top="910" width="359">lowing de&#64257;nitions. Each clause c in the MRF G = (V, E) is</text>
<text font="3" height="12" left="81" textpieces="0" top="925" width="359">assigned to an atom in c. A partition of the MRF is a sub-</text>
<text font="3" height="12" left="81" textpieces="2" top="941" width="358">graph Gi= (Vi, Ei) de&#64257;ned by a subset of atoms Vi &#8838; V ; Ei</text>
<text font="3" height="12" left="81" textpieces="0" top="957" width="358">is the set of clauses assigned to some atom in Vi. The size of</text>
<text font="3" height="12" left="81" textpieces="1" top="972" width="358">Gias referred to by Algorithm 3 can be any monotone func-</text>
<text font="3" height="12" left="81" textpieces="0" top="988" width="358">tion in Gi; in practice, it is de&#64257;ned to be the total number</text>
<text font="3" height="12" left="81" textpieces="0" top="1004" width="358">of literals and atoms in Gi. Note that when the parameter</text>
<text font="3" height="12" left="81" textpieces="0" top="1020" width="359">&#946; is set to +&#8734;, the output is the connected components of</text>
<text font="3" height="12" left="81" textpieces="0" top="1035" width="15">G.</text>
<text font="3" height="12" left="94" textpieces="0" top="1051" width="345">Our implementation of Algorithm 3 only uses RAM to</text>
<text font="3" height="12" left="81" textpieces="0" top="1067" width="359">maintain a union-&#64257;nd structure of the nodes, and performs</text>
<text font="3" height="12" left="475" textpieces="0" top="85" width="326">Algorithm 3 A Simple MRF Partitioning Algorithm</text>
<text font="3" height="14" left="475" textpieces="0" top="104" width="359">Input: an MRF G = (V, E) with clause weights w : E &#8594; R</text>
<text font="3" height="12" left="475" textpieces="0" top="120" width="184">Input: partition size bound &#946;</text>
<text font="3" height="12" left="475" textpieces="0" top="136" width="359">Output: a partitioning of V s.t. the size of each partition</text>
<text font="3" height="12" left="499" textpieces="0" top="151" width="112">is no larger than &#946;</text>
<text font="3" height="12" left="481" textpieces="0" top="167" width="288">1: Initialize hypergraph H = (V, F ) with F = &#8709;</text>
<text font="3" height="12" left="481" textpieces="0" top="183" width="264">2: for all e &#8712; E in |w|-descending order do</text>
<text font="3" height="12" left="481" textpieces="1" top="198" width="353">3:   F &#8592; F &#8746; e if afterwards no component in H is larger</text>
<text font="3" height="12" left="513" textpieces="0" top="214" width="40">than &#946;</text>
<text font="3" height="12" left="481" textpieces="0" top="230" width="352">4: return the collection of per-component atom sets in H</text>
<text font="3" height="12" left="475" textpieces="0" top="266" width="359">all other operations in the RDBMS. For example, we use</text>
<text font="3" height="12" left="475" textpieces="0" top="282" width="359">SQL queries to &#8220;assign&#8221; clauses to atoms and to compute</text>
<text font="3" height="12" left="475" textpieces="0" top="298" width="301">the partition of clauses from a partition of atoms.</text>
<text font="2" height="16" left="475" textpieces="1" top="323" width="272">B.8  Tradeoff of MRF Partitioning</text>
<text font="3" height="12" left="489" textpieces="0" top="346" width="345">Clearly, partitioning might be detrimental to search speed</text>
<text font="3" height="12" left="475" textpieces="0" top="361" width="359">if the cut size is large. Furthermore, given multiple parti-</text>
<text font="3" height="12" left="475" textpieces="0" top="377" width="359">tioning options, how do we decide which one is better? As</text>
<text font="3" height="12" left="475" textpieces="0" top="393" width="359">a baseline, we provide the following formula to (roughly) es-</text>
<text font="3" height="12" left="475" textpieces="0" top="409" width="359">timate the bene&#64257;t (if positive) or detriment (if negative) of</text>
<text font="3" height="12" left="475" textpieces="0" top="424" width="87">a partitioning:</text>
<text font="3" height="12" left="567" textpieces="0" top="457" width="40">W = 2</text>
<text font="6" height="7" left="609" textpieces="0" top="451" width="8">N</text>
<text font="6" height="7" left="611" textpieces="1" top="458" width="34">3  &#8722; T</text>
<text font="3" height="13" left="649" textpieces="0" top="447" width="88">|#cut clauses|</text>
<text font="3" height="13" left="684" textpieces="0" top="465" width="19">|E|</text>
<text font="3" height="12" left="739" textpieces="0" top="457" width="4">,</text>
<text font="3" height="12" left="475" textpieces="0" top="490" width="359">where N is the estimated number of components with posi-</text>
<text font="3" height="12" left="475" textpieces="0" top="506" width="359">tive lowest cost, T is the total number of WalkSAT steps in</text>
<text font="3" height="12" left="475" textpieces="0" top="521" width="359">one round of Gauss-Seidel, and |E| is the total number of</text>
<text font="3" height="12" left="475" textpieces="0" top="537" width="359">clauses. The &#64257;rst term roughly captures the speed-up as a</text>
<text font="3" height="12" left="475" textpieces="0" top="553" width="359">result of Theorem 3.1, and the second term roughly captures</text>
<text font="3" height="12" left="475" textpieces="0" top="569" width="224">the slow-down caused by cut clauses.</text>
<text font="3" height="12" left="489" textpieces="0" top="584" width="345">Empirically however, we &#64257;nd this formula to be rather</text>
<text font="3" height="12" left="475" textpieces="0" top="600" width="359">conservative compared to experimental results that gener-</text>
<text font="3" height="12" left="475" textpieces="0" top="616" width="359">ally favor much more aggressive partitioning. In the techni-</text>
<text font="3" height="12" left="475" textpieces="0" top="631" width="359">cal report [16] (Section 5), we present a much more detailed</text>
<text font="3" height="12" left="475" textpieces="0" top="647" width="359">discussion. The main idea is to &#64257;nely model the elements</text>
<text font="3" height="12" left="475" textpieces="0" top="663" width="359">of the tradeo&#64256; by taking into account connectivity and the</text>
<text font="3" height="12" left="475" textpieces="0" top="678" width="178">in&#64258;uence of individual atoms.</text>
<text font="2" height="16" left="475" textpieces="1" top="713" width="311">C.  MATERIAL FOR EXPERIMENTS</text>
<text font="2" height="16" left="475" textpieces="1" top="745" width="279">C.1  Alternative Search Algorithms</text>
<text font="3" height="12" left="489" textpieces="0" top="768" width="345">As shown in Section 4.3, RDBMS-based implementation</text>
<text font="3" height="12" left="475" textpieces="0" top="783" width="359">of WalkSAT is several orders of magnitude slower than the</text>
<text font="3" height="12" left="475" textpieces="0" top="799" width="359">in-memory counter part. This gap is consistent with the</text>
<text font="3" height="12" left="475" textpieces="2" top="815" width="359">I/O performance of disk vs.  main memory.  One might</text>
<text font="3" height="12" left="475" textpieces="0" top="830" width="359">imagine some clever caching schemes for WalkSAT, but even</text>
<text font="3" height="12" left="475" textpieces="0" top="846" width="359">assuming that a &#64258;ip incurs only one random I/O operation</text>
<text font="3" height="12" left="475" textpieces="0" top="862" width="359">(which is usually on the order of 10 ms), the &#64258;ipping rate</text>
<text font="3" height="12" left="475" textpieces="0" top="877" width="359">of RDBMS-based search is still no more than 100 &#64258;ips/sec.</text>
<text font="3" height="12" left="475" textpieces="0" top="893" width="359">Thus, it is highly unlikely that disk-based search implemen-</text>
<text font="3" height="12" left="475" textpieces="0" top="909" width="333">tations could catch up to their in-memory counterpart.</text>
<text font="2" height="16" left="475" textpieces="1" top="934" width="299">C.2  Lesion Study of Tuffy Grounding</text>
<text font="3" height="12" left="489" textpieces="0" top="957" width="345">To understand which part of the RDBMS contributes the</text>
<text font="3" height="14" left="475" textpieces="0" top="972" width="359">most to Tuffy&#8217;s fast grounding speed, we conduct a lesion</text>
<text font="3" height="12" left="475" textpieces="0" top="988" width="359">study by comparing the grounding time in three settings: 1)</text>
<text font="3" height="12" left="475" textpieces="0" top="1004" width="359">full optimizer, where the RDBMS is free to optimize SQL</text>
<text font="3" height="12" left="475" textpieces="0" top="1020" width="359">queries in all ways; 2) &#64257;xed join order, where we force</text>
<text font="3" height="14" left="475" textpieces="0" top="1035" width="359">the RDBMS to use the same join order as Alchemy does;</text>
<text font="3" height="12" left="475" textpieces="0" top="1051" width="359">3) &#64257;xed join algorithm, where we force the RDBMS to</text>
<text font="3" height="12" left="475" textpieces="0" top="1067" width="359">use nested loop join only. The results are shown in Table 6.</text>
<text font="4" height="11" left="238" textpieces="3" top="82" width="179">LP    IE        RC         ER</text>
<text font="4" height="11" left="122" textpieces="4" top="96" width="294">Full optimizer           6     13          40         106</text>
<text font="4" height="11" left="116" textpieces="4" top="111" width="300">Fixed join order          7     13          43         111</text>
<text font="4" height="11" left="104" textpieces="4" top="125" width="312">Fixed join algorithm    112   306   &gt;36,000   &gt;16,000</text>
<text font="3" height="12" left="135" textpieces="0" top="154" width="251">Table 6: Grounding time in seconds</text>
<text font="4" height="11" left="295" textpieces="1" top="184" width="53">IE   RC</text>
<text font="4" height="11" left="189" textpieces="2" top="198" width="159">Tu&#64256;y-batch        448    133</text>
<text font="4" height="11" left="207" textpieces="2" top="212" width="141">Tu&#64256;y            117     77</text>
<text font="4" height="11" left="172" textpieces="2" top="226" width="176">Tu&#64256;y+parallelism      28     42</text>
<text font="3" height="12" left="86" textpieces="0" top="256" width="349">Table 7: Comparison of execution time in seconds</text>
<text font="3" height="12" left="81" textpieces="0" top="285" width="359">Clearly, being able to use various join algorithms is the key</text>
<text font="3" height="14" left="81" textpieces="0" top="301" width="202">to Tuffy&#8217;s fast grounding speed.</text>
<text font="2" height="16" left="81" textpieces="1" top="325" width="273">C.3  Data Loading and Parallelism</text>
<text font="3" height="12" left="94" textpieces="0" top="348" width="345">To validate the importance of batch data loading and</text>
<text font="3" height="14" left="81" textpieces="0" top="364" width="359">parallelism (Section 3.3), we run three versions of Tuffy</text>
<text font="3" height="12" left="81" textpieces="0" top="379" width="359">on the IE and RC datasets: 1) Tu&#64256;y, which has batch</text>
<text font="3" height="12" left="81" textpieces="0" top="395" width="359">loading but no parallelism; 2) Tu&#64256;y-batch, which loads</text>
<text font="3" height="12" left="81" textpieces="0" top="411" width="359">components one by one and does not use parallelism; and</text>
<text font="3" height="12" left="81" textpieces="0" top="426" width="359">3) Tu&#64256;y+parallelism, which has both batch loading and</text>
<text font="3" height="12" left="81" textpieces="0" top="442" width="359">parallelism. We use the same WalkSAT parameters on each</text>
<text font="3" height="12" left="81" textpieces="1" top="458" width="358">component (up to 106&#64258;ips per component) and run all three</text>
<text font="3" height="12" left="81" textpieces="0" top="473" width="359">settings on the same machine with an 8-core Xeon CPU. Ta-</text>
<text font="3" height="12" left="81" textpieces="0" top="489" width="341">ble 7 shows the end-to-end running time of each setting.</text>
<text font="3" height="12" left="94" textpieces="0" top="505" width="345">Clearly, loading the components one by one incurs signif-</text>
<text font="3" height="12" left="81" textpieces="0" top="520" width="359">icant I/O cost on both datasets. The grounding + parti-</text>
<text font="3" height="12" left="81" textpieces="0" top="536" width="359">tioning time of IE and RC are 11 seconds and 35 seconds,</text>
<text font="3" height="12" left="81" textpieces="0" top="552" width="359">respectively. Hence, Tu&#64256;y+parallelism achieved roughly 6-</text>
<text font="3" height="12" left="81" textpieces="0" top="568" width="195">time speed up on both datasets.</text>
<text font="2" height="16" left="81" textpieces="1" top="588" width="287">D.  EXTENDED RELATED WORK</text>
<text font="3" height="12" left="94" textpieces="0" top="610" width="345">The idea of using the stochastic local search algorithm</text>
<text font="3" height="12" left="81" textpieces="0" top="628" width="359">WalkSAT to &#64257;nd the most likely world is due to Kautz et</text>
<text font="3" height="12" left="81" textpieces="0" top="641" width="359">al. [13]. Singla and Domingos [41] proposed lazy grounding</text>
<text font="3" height="14" left="81" textpieces="0" top="657" width="359">and applies it to WalkSAT, resulting in an algorithm called</text>
<text font="3" height="12" left="81" textpieces="0" top="675" width="359">LazySAT that is implemented in Alchemy. The idea of ig-</text>
<text font="3" height="12" left="81" textpieces="0" top="688" width="359">noring ground clauses that are satis&#64257;ed by evidence is high-</text>
<text font="3" height="14" left="81" textpieces="0" top="704" width="359">lighted as an e&#64256;ective way of speeding up the MLN ground-</text>
<text font="3" height="12" left="81" textpieces="0" top="720" width="359">ing process in Shavlik and Natarajan [40], which formulates</text>
<text font="3" height="12" left="81" textpieces="0" top="736" width="359">the grounding process as nested loops and provides heuris-</text>
<text font="3" height="12" left="81" textpieces="0" top="751" width="359">tics to approximate the optimal looping order. Mihalkova</text>
<text font="3" height="12" left="81" textpieces="0" top="767" width="359">and Mooney [35] also employ a bottom-up approach, but</text>
<text font="3" height="12" left="81" textpieces="0" top="783" width="359">they address structure learning of MLNs whereas we focus</text>
<text font="3" height="12" left="81" textpieces="0" top="798" width="359">on inference. As an orthogonal approach to scaling MLN in-</text>
<text font="3" height="12" left="81" textpieces="0" top="814" width="359">ference, Mihalkova and Richardson [36] study how to avoid</text>
<text font="3" height="12" left="81" textpieces="0" top="830" width="359">redundant computation by clustering similar query literals.</text>
<text font="3" height="12" left="81" textpieces="0" top="845" width="359">It is an interesting problem to incorporate their techniques</text>
<text font="3" height="14" left="81" textpieces="0" top="861" width="359">into Tuffy. Lifted inference (e.g., [42]) involves performing</text>
<text font="3" height="14" left="81" textpieces="0" top="877" width="359">inference in MLNs without completely grounding them into</text>
<text font="3" height="14" left="81" textpieces="0" top="892" width="359">MRFs. It is interesting future work to extend Tuffy to</text>
<text font="3" height="12" left="81" textpieces="0" top="908" width="359">perform lifted inference. Knowledge-based model construc-</text>
<text font="3" height="12" left="81" textpieces="0" top="924" width="359">tion [28] is a technique that, given a query, &#64257;nds the minimal</text>
<text font="3" height="12" left="81" textpieces="0" top="940" width="359">relevant portion of a graph; although the resulting subgraph</text>
<text font="3" height="12" left="81" textpieces="0" top="955" width="359">may contain multiple components, the downstream inference</text>
<text font="3" height="12" left="81" textpieces="0" top="971" width="359">algorithm may not be aware of it and thereby cannot bene&#64257;t</text>
<text font="3" height="12" left="81" textpieces="0" top="987" width="185">from the speedup in Thm. 3.1.</text>
<text font="3" height="14" left="489" textpieces="0" top="86" width="345">While Tuffy employs the simple WalkSAT algorithm,</text>
<text font="3" height="12" left="475" textpieces="0" top="102" width="359">there are more advanced techniques for MAP inference [31,</text>
<text font="3" height="12" left="475" textpieces="0" top="118" width="359">33]; we plan to integrate them into upcoming versions of</text>
<text font="3" height="12" left="475" textpieces="0" top="136" width="359">Tuffy. For hypergraph partitioning, there are established</text>
<text font="3" height="12" left="475" textpieces="0" top="149" width="359">solutions such as hMETIS [12]. However, existing imple-</text>
<text font="3" height="12" left="475" textpieces="0" top="161" width="359">mentations of them are limited by memory size, and it is</text>
<text font="3" height="12" left="475" textpieces="0" top="177" width="359">future work to adapt these algorithms to on-disk data; this</text>
<text font="3" height="12" left="475" textpieces="0" top="193" width="359">motivated us to design Algorithm 3. The technique of cut-</text>
<text font="3" height="12" left="475" textpieces="0" top="208" width="359">set conditioning [17] from the SAT and probabilistic infer-</text>
<text font="3" height="12" left="475" textpieces="0" top="224" width="359">ence literature is closely related to our partitioning tech-</text>
<text font="3" height="12" left="475" textpieces="0" top="240" width="359">nique [30, 37]. Cutset conditioning recursively conditions on</text>
<text font="3" height="12" left="475" textpieces="0" top="256" width="359">cutsets of graphical models, and at each step exhaustively</text>
<text font="3" height="12" left="475" textpieces="0" top="271" width="359">enumerates all con&#64257;gurations of the cut, which is imprac-</text>
<text font="3" height="12" left="475" textpieces="0" top="287" width="359">tical in our scenario: even for small datasets, the cut size</text>
<text font="3" height="12" left="475" textpieces="0" top="303" width="359">can easily be thousands, making exhaustive enumeration in-</text>
<text font="3" height="12" left="475" textpieces="0" top="318" width="359">feasible. Instead, we use a Gauss-Seidel strategy, which we</text>
<text font="3" height="12" left="475" textpieces="0" top="334" width="359">show is e&#64259;cient and e&#64256;ective. Additionally, our conceptual</text>
<text font="3" height="12" left="475" textpieces="0" top="350" width="359">goals are di&#64256;erent: our goal is to &#64257;nd an analytic formula</text>
<text font="3" height="12" left="475" textpieces="0" top="365" width="359">that quanti&#64257;es the e&#64256;ect of partitioning and then, we use</text>
<text font="3" height="12" left="475" textpieces="0" top="381" width="359">this formula to optimize the IO and scheduling behavior of</text>
<text font="3" height="12" left="475" textpieces="0" top="397" width="359">a class of local search algorithms; in contrast, prior work</text>
<text font="3" height="12" left="475" textpieces="0" top="412" width="281">focuses on designing new inference algorithms.</text>
<text font="3" height="14" left="489" textpieces="0" top="428" width="345">There are statistical-logical frameworks similar to MLNs,</text>
<text font="3" height="12" left="475" textpieces="0" top="444" width="359">such as Probabilistic Relational Models [32] and Relational</text>
<text font="3" height="12" left="475" textpieces="0" top="460" width="359">Markov Models [43]. Inference on those models also requires</text>
<text font="3" height="12" left="475" textpieces="0" top="475" width="359">grounding and search, and we are optimistic that the lessons</text>
<text font="3" height="12" left="475" textpieces="0" top="491" width="305">we learned with MLNs carry over to these models.</text>
<text font="2" height="16" left="475" textpieces="1" top="516" width="156">E.  REFERENCES</text>
<text font="13" height="9" left="475" textpieces="0" top="537" width="322">[30] D. Allen and A. Darwiche. New advances in inference by</text>
<text font="13" height="9" left="501" textpieces="0" top="549" width="258">recursive conditioning. In UAI, pages 2&#8211;10, 2003.</text>
<text font="13" height="9" left="475" textpieces="0" top="563" width="303">[31] J. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using</text>
<text font="13" height="9" left="501" textpieces="0" top="575" width="281">combinatorial optimization within max-product belief</text>
<text font="13" height="9" left="501" textpieces="0" top="587" width="230">propagation. In NIPS, pages 369&#8211;376, 2007.</text>
<text font="13" height="9" left="475" textpieces="0" top="600" width="339">[32] N. Friedman, L. Getoor, D. Koller, and A. Pfe&#64256;er. Learning</text>
<text font="13" height="9" left="501" textpieces="0" top="612" width="311">probabilistic relational models. In IJCAI, pages 1300&#8211;1309,</text>
<text font="13" height="9" left="501" textpieces="0" top="624" width="27">1999.</text>
<text font="13" height="9" left="475" textpieces="0" top="637" width="350">[33] R. Gupta, A. Diwan, and S. Sarawagi. E&#64259;cient inference with</text>
<text font="13" height="9" left="501" textpieces="0" top="649" width="268">cardinality-based clique potentials. In ICML, 2007.</text>
<text font="13" height="9" left="475" textpieces="0" top="663" width="322">[34] H. Kautz, B. Selman, and Y. Jiang. A general stochastic</text>
<text font="13" height="9" left="501" textpieces="0" top="675" width="319">approach to solving problems with hard and soft constraints.</text>
<text font="13" height="9" left="501" textpieces="0" top="687" width="292">Satis&#64257;ability Problem: Theory and Applications, 1997.</text>
<text font="13" height="9" left="475" textpieces="0" top="700" width="344">[35] L. Mihalkova and R. Mooney. Bottom-up learning of Markov</text>
<text font="13" height="9" left="501" textpieces="0" top="712" width="291">logic network structure. In ICML, pages 625&#8211;632, 2007.</text>
<text font="13" height="9" left="475" textpieces="0" top="726" width="333">[36] L. Mihalkova and M. Richardson. Speeding up inference in</text>
<text font="13" height="9" left="501" textpieces="0" top="737" width="294">statistical relational learning by clustering similar query</text>
<text font="13" height="9" left="501" textpieces="0" top="749" width="195">literals. In ILP, pages 110&#8211;122, 2010.</text>
<text font="13" height="9" left="475" textpieces="0" top="763" width="305">[37] T. Park and A. Van Gelder. Partitioning methods for</text>
<text font="13" height="9" left="501" textpieces="0" top="775" width="324">satis&#64257;ability testing on large formulas. Automated Deduction,</text>
<text font="13" height="9" left="501" textpieces="0" top="787" width="109">pages 748&#8211;762, 1996.</text>
<text font="13" height="9" left="475" textpieces="0" top="800" width="346">[38] H. Poon and P. Domingos. Sound and e&#64259;cient inference with</text>
<text font="13" height="9" left="501" textpieces="0" top="812" width="321">probabilistic and deterministic dependencies. In AAAI, 2006.</text>
<text font="13" height="9" left="475" textpieces="0" top="826" width="349">[39] H. Poon, P. Domingos, and M. Sumner. A general method for</text>
<text font="13" height="9" left="501" textpieces="0" top="838" width="283">reducing the complexity of relational inference and its</text>
<text font="13" height="9" left="501" textpieces="0" top="850" width="298">application to MCMC. In AAAI, pages 1075&#8211;1080, 2008.</text>
<text font="13" height="9" left="475" textpieces="0" top="863" width="350">[40] J. Shavlik and S. Natarajan. Speeding up inference in Markov</text>
<text font="13" height="9" left="501" textpieces="0" top="875" width="295">logic networks by preprocessing to reduce the size of the</text>
<text font="13" height="9" left="501" textpieces="0" top="887" width="327">resulting grounded network. In IJCAI, pages 1951&#8211;1956, 2009.</text>
<text font="13" height="9" left="475" textpieces="0" top="900" width="324">[41] P. Singla and P. Domingos. Memory-e&#64259;cient inference in</text>
<text font="13" height="9" left="501" textpieces="0" top="912" width="267">relational domains. In AAAI, pages 488&#8211;493, 2006.</text>
<text font="13" height="9" left="475" textpieces="0" top="926" width="291">[42] P. Singla and P. Domingos. Lifted &#64257;rst-order belief</text>
<text font="13" height="9" left="501" textpieces="0" top="938" width="245">propagation. In AAAI, pages 1094&#8211;1099, 2008.</text>
<text font="13" height="9" left="475" textpieces="0" top="951" width="296">[43] B. Taskar, P. Abbeel, and D. Koller. Discriminative</text>
<text font="13" height="9" left="501" textpieces="0" top="963" width="333">probabilistic models for relational data. In UAI, pages 485&#8211;492,</text>
<text font="13" height="9" left="501" textpieces="0" top="975" width="27">2002.</text>
<text font="13" height="9" left="475" textpieces="0" top="989" width="309">[44] W. Wei, J. Erenrich, and B. Selman. Towards e&#64259;cient</text>
<text font="13" height="9" left="501" textpieces="0" top="1000" width="323">sampling: Exploiting random walk strategies. In AAAI, 2004.</text>
