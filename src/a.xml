<text font="0" height="23" left="123" textpieces="0" top="172" width="672">Tu&#64256;y: Scaling up Statistical Inference in Markov Logic Networks</text>
<text font="0" height="23" left="368" textpieces="0" top="205" width="182">using an RDBMS</text>
<text font="1" height="16" left="195" textpieces="4" top="256" width="528">Feng Niu      Christopher R&#180; e      AnHai Doan      Jude Shavlik</text>
<text font="1" height="16" left="341" textpieces="0" top="283" width="253">University of Wisconsin-Madison</text>
<text font="1" height="17" left="310" textpieces="0" top="303" width="317">{leonn,chrisre,anhai,shavlik}@cs.wisc.edu</text>
<text font="1" height="16" left="391" textpieces="0" top="339" width="136">December 7, 2010</text>
<text font="2" height="13" left="426" textpieces="0" top="402" width="66">Abstract</text>
<text font="2" height="16" left="171" textpieces="0" top="429" width="598">Markov Logic Networks (MLNs) have emerged as a powerful framework that combines sta-</text>
<text font="2" height="13" left="149" textpieces="0" top="447" width="620">tistical and logical reasoning; they have been applied to many data intensive problems including</text>
<text font="2" height="16" left="149" textpieces="0" top="465" width="620">information extraction, entity resolution, and text mining. Current implementations of MLNs</text>
<text font="2" height="13" left="149" textpieces="0" top="483" width="620">do not scale to real-world data sets, which is preventing their wide-spread adoption. We present</text>
<text font="2" height="13" left="149" textpieces="0" top="503" width="620">Tuffy that achieves scalability via three novel contributions: (1) a bottom-up approach to</text>
<text font="2" height="13" left="149" textpieces="0" top="519" width="620">grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hy-</text>
<text font="2" height="13" left="149" textpieces="0" top="537" width="620">brid architecture that allows us to perform AI-style local search e&#64259;ciently using an RDBMS,</text>
<text font="2" height="13" left="149" textpieces="0" top="554" width="620">and (3) a theoretical insight that shows when one can (exponentially) improve the e&#64259;ciency</text>
<text font="2" height="13" left="149" textpieces="0" top="572" width="620">of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel</text>
<text font="2" height="13" left="149" textpieces="0" top="590" width="620">algorithms. We show that our approach outperforms state-of-the-art implementations in both</text>
<text font="2" height="13" left="149" textpieces="0" top="608" width="366">quality and speed on several publicly available datasets.</text>
<text font="3" height="19" left="108" textpieces="1" top="655" width="169">1  Introduction</text>
<text font="4" height="17" left="108" textpieces="0" top="695" width="702">Over the past few years, Markov Logic Networks (MLNs) have emerged as a powerful and popular</text>
<text font="4" height="17" left="108" textpieces="0" top="716" width="702">framework that combines logical and probabilistic reasoning. MLNs have been successfully applied</text>
<text font="4" height="15" left="108" textpieces="0" top="736" width="702">to a wide variety of data management problems, including information extraction, entity resolution,</text>
<text font="4" height="15" left="108" textpieces="0" top="756" width="702">and text mining. In contrast to probability models like factor graphs [24] that require complex</text>
<text font="4" height="17" left="108" textpieces="0" top="777" width="702">distributions to be speci&#64257;ed in tedious detail, MLNs allow us to declare a rigorous statistical model</text>
<text font="4" height="15" left="108" textpieces="0" top="797" width="702">at a much higher level using essentially &#64257;rst-order logic. For example, in a problem where we try</text>
<text font="4" height="15" left="108" textpieces="0" top="817" width="702">to classify papers by research area, one could write a rule such as &#8220;it is likely that if one paper cites</text>
<text font="4" height="15" left="108" textpieces="0" top="838" width="318">another they are in the same research area.&#8221;</text>
<text font="4" height="17" left="133" textpieces="0" top="858" width="677">Our interest in MLNs stems from our involvement in a DARPA project called &#8220;Machine Read-</text>
<text font="4" height="15" left="108" textpieces="0" top="878" width="702">ing.&#8221; The grand challenge is to build software that can read the Web, i.e., extract and integrate</text>
<text font="4" height="15" left="108" textpieces="0" top="899" width="702">structured data (e.g., entities, relationships) from Web data, then use this structured data to answer</text>
<text font="4" height="17" left="108" textpieces="0" top="919" width="702">user queries. The current approach is to use MLNs as a lingua franca to combine many di&#64256;erent</text>
<text font="4" height="17" left="108" textpieces="0" top="939" width="702">kinds of extractions into one coherent picture. To accomplish this goal, it is critical that MLNs</text>
<text font="4" height="15" left="108" textpieces="0" top="960" width="615">scale to large data sets, and we have been put in charge of investigating this problem.</text>
<text font="4" height="17" left="133" textpieces="0" top="980" width="677">Unfortunately, none of the current MLN implementations scale beyond relatively small data</text>
<text font="4" height="15" left="108" textpieces="0" top="1000" width="702">sets (and even on many of these data sets, existing implementations routinely take hours to run).</text>
<text font="4" height="15" left="108" textpieces="0" top="1021" width="702">The &#64257;rst obvious reason is that these are in-memory implementations: when manipulating large</text>
<text font="4" height="15" left="455" textpieces="0" top="1069" width="8">1</text>
<text font="4" height="15" left="108" textpieces="0" top="113" width="702">intermediate data structures that over&#64258;ow main memory, they either crash or thrash badly. Con-</text>
<text font="4" height="17" left="108" textpieces="0" top="133" width="702">sequently, there is an emerging e&#64256;ort across several research groups to scale up MLNs. In this</text>
<text font="4" height="17" left="108" textpieces="0" top="154" width="702">paper, we describe our system, Tuffy, that leverages an RDBMS to address the above scalability</text>
<text font="4" height="15" left="108" textpieces="0" top="174" width="63">problem.</text>
<text font="4" height="17" left="133" textpieces="0" top="194" width="677">Reasoning with MLNs can be classi&#64257;ed as either learning or inference [21]. We focus on infer-</text>
<text font="4" height="15" left="108" textpieces="0" top="215" width="702">ence, since typically a model is learned once, and then an application may perform inference many</text>
<text font="4" height="15" left="108" textpieces="0" top="235" width="702">times using the same model; hence inference is an on-line process, which must be fast. Conceptu-</text>
<text font="4" height="17" left="108" textpieces="0" top="255" width="702">ally, inference in MLNs has two phases: a grounding phase, which constructs a large, weighted SAT</text>
<text font="4" height="15" left="108" textpieces="0" top="276" width="702">formula, and a search phase, which searches for a low cost (weight) assignment (called a solution)</text>
<text font="4" height="17" left="108" textpieces="1" top="296" width="702">to the SAT formula from grounding (using WalkSAT [14], a local search procedure).1 Grounding</text>
<text font="4" height="15" left="108" textpieces="0" top="316" width="702">is a non-trivial portion of the overall inference e&#64256;ort: on a classi&#64257;cation benchmark (called RC)</text>
<text font="4" height="17" left="108" textpieces="0" top="337" width="702">the state-of-the-art MLN inference engine, Alchemy [7], spends over 96% of its execution time in</text>
<text font="4" height="17" left="108" textpieces="0" top="357" width="702">grounding. The state-of-the-art strategy for the grounding phase (and the one used by Alchemy)</text>
<text font="4" height="15" left="108" textpieces="0" top="377" width="702">is a top-down procedure (similar to the proof strategy in Prolog). In contrast, we propose a bottom-</text>
<text font="4" height="17" left="108" textpieces="0" top="398" width="702">up grounding strategy. Intuitively, bottom-up grounding allows Tuffy to fully exploit the RDBMS</text>
<text font="4" height="17" left="108" textpieces="0" top="418" width="702">optimizer, and thereby signi&#64257;cantly speed up the grounding phase of MLN inference. On an entity</text>
<text font="4" height="17" left="108" textpieces="0" top="438" width="702">resolution task, Alchemy takes over 7 hours to complete grounding, while Tuffy&#8217;s grounding</text>
<text font="4" height="15" left="108" textpieces="0" top="459" width="219">&#64257;nishes in less than 2 minutes.</text>
<text font="4" height="15" left="133" textpieces="0" top="479" width="677">But not all phases are well-optimized by the RDBMS: during the search phase, we found that</text>
<text font="4" height="15" left="108" textpieces="0" top="499" width="702">the RDBMS implementation performed poorly. The underlying reason is a fundamental problem</text>
<text font="4" height="15" left="108" textpieces="0" top="520" width="702">for pushing local search procedures into an RDBMS: search procedures often perform inherently</text>
<text font="4" height="15" left="108" textpieces="0" top="540" width="702">sequential, random data accesses. Consequently, any RDBMS-based solution must execute a large</text>
<text font="4" height="15" left="108" textpieces="0" top="560" width="702">number of disk accesses, each of which has a substantial overhead (due to the RDBMS) versus direct</text>
<text font="4" height="15" left="108" textpieces="0" top="581" width="702">main-memory access. Not surprisingly, given the same amount of time, an in-memory solution can</text>
<text font="4" height="15" left="108" textpieces="0" top="601" width="702">execute between three and &#64257;ve orders of magnitude more search steps than an approach that uses</text>
<text font="4" height="15" left="108" textpieces="0" top="621" width="702">an RDBMS. Thus, to achieve competitive performance, we are forced to develop a novel hybrid</text>
<text font="4" height="15" left="108" textpieces="0" top="642" width="702">architecture that supports local search procedures in main memory whenever possible. This is our</text>
<text font="4" height="15" left="108" textpieces="0" top="662" width="214">second technical contribution.</text>
<text font="4" height="17" left="133" textpieces="0" top="682" width="677">Our third contribution is a simple partitioning technique that allows Tuffy to introduce paral-</text>
<text font="4" height="15" left="108" textpieces="0" top="702" width="702">lelism and use less memory than state-of-the-art approaches. Surprisingly, this same technique often</text>
<text font="4" height="17" left="108" textpieces="0" top="723" width="702">allows Tuffy to speed up the search phase exponentially. The underlying idea is simple: in many</text>
<text font="4" height="15" left="108" textpieces="0" top="743" width="702">cases, a local search problem can be divided into multiple independent subproblems. For example,</text>
<text font="4" height="15" left="108" textpieces="0" top="763" width="702">the formula that is output by the grounding phase may consist of multiple connected components.</text>
<text font="4" height="15" left="108" textpieces="0" top="784" width="702">On such datasets, we derive a su&#64259;cient condition under which solving the subproblems indepen-</text>
<text font="4" height="15" left="108" textpieces="0" top="804" width="702">dently results in exponentially faster search than running the larger global problem (Thm. 3.1). An</text>
<text font="4" height="15" left="108" textpieces="0" top="824" width="702">application of our theorem shows that on an information extraction testbed, a system that is not</text>
<text font="4" height="17" left="108" textpieces="1" top="845" width="702">aware of the partitioning phenomenon (such as Alchemy) must take at least 2200more steps than</text>
<text font="4" height="14" left="108" textpieces="0" top="868" width="702">Tuffy&#8217;s approach. Empirically we found that, on some real-world datasets, solutions found by</text>
<text font="4" height="14" left="108" textpieces="0" top="888" width="702">Tuffy within one minute has higher quality than those found by non-partitioning systems (such</text>
<text font="4" height="17" left="108" textpieces="0" top="906" width="303">as Alchemy) even after running for days.</text>
<text font="4" height="15" left="133" textpieces="0" top="926" width="677">The exponential di&#64256;erence in running time for independent subproblems versus the larger global</text>
<text font="4" height="15" left="108" textpieces="0" top="946" width="702">problem suggests that in some cases, further decomposing the search space may improve the overall</text>
<text font="6" height="8" left="127" textpieces="0" top="975" width="682">1We discuss maximum a posteriori inference (highest probability world) which is critical for many integration</text>
<text font="7" height="14" left="108" textpieces="0" top="994" width="702">tasks. Our system, Tuffy, supports marginal probabilistic inference as well: the algorithms are similar, and in &#167;A.5,</text>
<text font="7" height="12" left="108" textpieces="0" top="1011" width="259">we apply our results to marginal inference.</text>
<text font="4" height="15" left="455" textpieces="0" top="1069" width="8">2</text>
<text font="4" height="17" left="108" textpieces="0" top="113" width="702">runtime. To implement this idea for MLNs, we must address two di&#64259;cult problems: (1) partitioning</text>
<text font="4" height="15" left="108" textpieces="0" top="133" width="702">the formula from grounding (and so the search space) to minimize the number of formula that are</text>
<text font="4" height="15" left="108" textpieces="0" top="154" width="702">split between partitions, and (2) augmenting the search algorithm to be aware of partitioning.</text>
<text font="4" height="15" left="108" textpieces="0" top="174" width="702">We show that the &#64257;rst problem is NP-hard (even to approximate), and design a scalable heuristic</text>
<text font="4" height="15" left="108" textpieces="0" top="194" width="702">partitioning algorithm. For the second problem, we apply a technique from non-linear optimization</text>
<text font="4" height="15" left="108" textpieces="0" top="215" width="702">to leverage the insights gained from our characterization of the phenomenon described above. The</text>
<text font="4" height="15" left="108" textpieces="0" top="235" width="702">e&#64256;ect of such partitioning is dramatic. As an example, on a classi&#64257;cation benchmark (called RC),</text>
<text font="4" height="14" left="108" textpieces="0" top="258" width="702">Tuffy (using 15MB of RAM) produces much better result quality in minutes than Alchemy</text>
<text font="4" height="17" left="108" textpieces="0" top="276" width="702">(using 2.8GB of RAM) even after days of running. In fact, Tuffy is able to answer queries on a</text>
<text font="4" height="17" left="108" textpieces="0" top="296" width="702">version of the RC dataset that is over 2 orders of magnitude larger. (We estimate that Alchemy</text>
<text font="4" height="15" left="108" textpieces="0" top="316" width="315">would need 280GB+ of RAM to process it.)</text>
<text font="4" height="15" left="108" textpieces="1" top="360" width="702">Related Work  MLNs are an integral part of state-of-the-art approaches in a variety of applica-</text>
<text font="4" height="15" left="108" textpieces="0" top="380" width="702">tions: natural language processing [22], ontology matching [30], information extraction [18], entity</text>
<text font="4" height="17" left="108" textpieces="0" top="400" width="702">resolution [26], data mining [27], etc. And so, there is an application push to support MLNs. In</text>
<text font="4" height="15" left="108" textpieces="0" top="421" width="702">contrast to machine learning research that has focused on quality and algorithmic e&#64259;ciency, we</text>
<text font="4" height="15" left="108" textpieces="0" top="441" width="631">apply fundamental data management principles to improve scalability and performance.</text>
<text font="4" height="15" left="133" textpieces="0" top="461" width="680">Pushing statistical reasoning models inside a database system has been a goal of many projects [5,</text>
<text font="4" height="17" left="108" textpieces="0" top="481" width="702">11, 12, 20, 29]. Most closely related is the BayesStore project, in which the database essentially</text>
<text font="4" height="15" left="108" textpieces="0" top="502" width="702">stores Bayes Nets [17] and allows these networks to be retrieved for inference by an external pro-</text>
<text font="4" height="17" left="108" textpieces="0" top="522" width="702">gram. In contrast, Tuffy uses an RDBMS to optimize the inference procedure. The Monte-Carlo</text>
<text font="4" height="17" left="108" textpieces="0" top="542" width="702">database [11] made sampling a &#64257;rst-class citizen inside an RDBMS. In contrast, in Tuffy our</text>
<text font="4" height="15" left="108" textpieces="0" top="563" width="702">approach can be viewed as pushing classical search inside the database engine. One way to view</text>
<text font="4" height="17" left="108" textpieces="0" top="583" width="702">an MLN is a compact speci&#64257;cation of factor graphs [23, 24]. Sen et al. proposed new algorithms;</text>
<text font="4" height="15" left="108" textpieces="0" top="603" width="702">in contrast, we take an existing, widely used class of algorithms (local search), and our focus is to</text>
<text font="4" height="15" left="108" textpieces="0" top="624" width="332">leverage the RDBMS to improve performance.</text>
<text font="4" height="15" left="133" textpieces="0" top="644" width="677">There has also been an extensive amount of work on probabilistic databases [1, 2, 4, 19] that</text>
<text font="4" height="15" left="108" textpieces="0" top="664" width="702">deal with simpler probabilistic models. Finding the most likely world is trivial in these models;</text>
<text font="4" height="17" left="108" textpieces="0" top="685" width="702">in contrast, it is highly non-trivial in MLNs (in fact, it is NP-hard [6]). MLNs also provide an</text>
<text font="4" height="15" left="108" textpieces="0" top="705" width="702">appealing answer for one of the most pressing questions in the area of probabilistic databases:</text>
<text font="4" height="15" left="108" textpieces="0" top="725" width="702">&#8220;Where do those probabilities come from?&#8221; Finally, none of these prior approaches deal with the</text>
<text font="4" height="17" left="108" textpieces="0" top="746" width="702">core technical challenge Tuffy addresses, which is handling AI-style search inside a database.</text>
<text font="4" height="15" left="108" textpieces="0" top="766" width="318">Additional related work can be found in &#167;D.</text>
<text font="4" height="15" left="108" textpieces="1" top="809" width="702">Contributions, Validation, and Outline  To summarize, we make the following contributions:</text>
<text font="4" height="18" left="133" textpieces="0" top="842" width="677">&#8226; In &#167;3.1, we design a solution that pushes MLNs into RDBMSes. The key idea is to use</text>
<text font="4" height="15" left="149" textpieces="0" top="863" width="661">bottom-up grounding that allows us to leverage the RDBMS optimizer; this idea improves</text>
<text font="4" height="15" left="149" textpieces="0" top="884" width="516">the performance of the grounding phase by several orders of magnitude.</text>
<text font="4" height="15" left="133" textpieces="0" top="907" width="677">&#8226; In &#167;3.2, we devise a novel hybrid architecture to support e&#64259;cient grounding and in-memory</text>
<text font="4" height="15" left="149" textpieces="0" top="928" width="661">inference. By itself, this architecture is orders of magnitude more scalable and, given the</text>
<text font="4" height="15" left="149" textpieces="0" top="948" width="614">same amount of time, performs orders of magnitude more search steps than prior art.</text>
<text font="4" height="15" left="133" textpieces="0" top="972" width="677">&#8226; In &#167;3.3, we describe novel data partitioning techniques to decrease the memory usage and</text>
<text font="4" height="17" left="149" textpieces="0" top="993" width="661">to increase parallelism (and so improve the scalability) of Tuffy&#8217;s in-memory inference al-</text>
<text font="4" height="15" left="149" textpieces="0" top="1013" width="661">gorithms. Additionally, we show that for any MLN with an MRF that contains multiple</text>
<text font="4" height="15" left="455" textpieces="0" top="1069" width="8">3</text>
<text font="2" height="12" left="126" textpieces="0" top="129" width="147">paper(PaperID, URL)</text>
<text font="2" height="12" left="127" textpieces="0" top="147" width="145">wrote(Author, Paper)</text>
<text font="2" height="12" left="127" textpieces="0" top="165" width="145">refers(Paper, Paper)</text>
<text font="2" height="12" left="128" textpieces="0" top="183" width="142">cat(Paper, Category)</text>
<text font="2" height="13" left="295" textpieces="1" top="110" width="85">weight   rule</text>
<text font="2" height="13" left="312" textpieces="3" top="128" width="423">5      cat(p, c1), cat(p, c2) =&gt; c1 = c2                       (F1)</text>
<text font="2" height="13" left="312" textpieces="2" top="146" width="422">1      wrote(x, p1), wrote(x, p2), cat(p1, c) =&gt; cat(p2, c)   (F2)</text>
<text font="2" height="13" left="312" textpieces="2" top="164" width="422">2      cat(p1, c), refers(p1, p2) =&gt; cat(p2, c)               (F3)</text>
<text font="2" height="13" left="303" textpieces="2" top="182" width="431">+&#8734;    paper(p, u) =&gt; &#8707;x. wrote(x, p)                        (F4)</text>
<text font="2" height="13" left="310" textpieces="2" top="200" width="424">-1     cat(p, &#8216;Networking&#8217;)                                    (F5)</text>
<text font="2" height="12" left="757" textpieces="0" top="111" width="116">wrote(&#8216;Joe&#8217;, &#8216;P1&#8217;)</text>
<text font="2" height="12" left="757" textpieces="0" top="129" width="116">wrote(&#8216;Joe&#8217;, &#8216;P2&#8217;)</text>
<text font="2" height="12" left="757" textpieces="0" top="147" width="123">wrote(&#8216;Jake&#8217;, &#8216;P3&#8217;)</text>
<text font="2" height="12" left="757" textpieces="0" top="165" width="120">refers(&#8216;P1&#8217;, &#8216;P3&#8217;)</text>
<text font="2" height="12" left="757" textpieces="0" top="183" width="101">cat(&#8216;P2&#8217;, &#8216;DB&#8217;)</text>
<text font="2" height="14" left="757" textpieces="0" top="199" width="17">&#183; &#183; &#183;</text>
<text font="2" height="13" left="175" textpieces="2" top="218" width="673">Schema                                  A Markov Logic Program                               Evidence</text>
<text font="4" height="15" left="108" textpieces="0" top="253" width="702">Figure 1: A Sample Markov Logic Program: The goal is to classify papers by category. As evidence</text>
<text font="4" height="15" left="108" textpieces="0" top="274" width="702">we are given author and citation information of all papers, as well as the labels of a subset of the</text>
<text font="4" height="15" left="108" textpieces="0" top="294" width="702">papers; and we want to classify the remaining papers. Any variable not explicitly quanti&#64257;ed is</text>
<text font="4" height="15" left="108" textpieces="0" top="314" width="158">universally quanti&#64257;ed.</text>
<text font="4" height="15" left="149" textpieces="0" top="364" width="661">components, partitioning exponentially improves search speed, and we quantify this theoret-</text>
<text font="4" height="15" left="149" textpieces="0" top="384" width="41">ically.</text>
<text font="4" height="18" left="133" textpieces="0" top="407" width="677">&#8226; In &#167;3.4, we generalize our partitioning results to arbitrary MLNs using our characterization</text>
<text font="4" height="15" left="149" textpieces="0" top="428" width="661">of the partitioning phenomenon. These techniques result in our highest quality, most space-</text>
<text font="4" height="15" left="149" textpieces="0" top="448" width="128">e&#64259;cient solutions.</text>
<text font="4" height="17" left="108" textpieces="0" top="479" width="702">We present an extensive experimental study on a diverse set of MLN testbeds to demonstrate that</text>
<text font="4" height="17" left="108" textpieces="0" top="499" width="702">our system Tuffy is able to get better result quality more quickly and work over larger datasets</text>
<text font="4" height="15" left="108" textpieces="0" top="519" width="263">than the state-of-the-art approaches.</text>
<text font="3" height="19" left="108" textpieces="1" top="567" width="176">2  Preliminaries</text>
<text font="4" height="15" left="108" textpieces="0" top="607" width="702">We illustrate a Markov Logic Network program using the example of classifying papers by topic</text>
<text font="4" height="17" left="108" textpieces="0" top="627" width="558">area. We then de&#64257;ne the semantics of MLNs, and the mechanics of inference.</text>
<text font="1" height="16" left="108" textpieces="1" top="669" width="233">2.1  The Syntax of MLNs</text>
<text font="4" height="17" left="108" textpieces="0" top="701" width="702">Figure 1 shows an example input MLN program for Tuffy that is used to classify paper references</text>
<text font="4" height="17" left="108" textpieces="0" top="722" width="702">by topic area, such as databases, systems, AI, etc. In this example, a user gives Tuffy a set of</text>
<text font="4" height="15" left="108" textpieces="0" top="742" width="702">relations that capture information about the papers in her dataset: she has extracted authors and</text>
<text font="4" height="15" left="108" textpieces="0" top="762" width="702">citations and stored in them in the relations wrote(Author,Paper) and refers(Paper,Paper). She</text>
<text font="4" height="15" left="108" textpieces="0" top="783" width="702">may also provide evidence, which is data that she knows to be true (or false). Here, the evidence</text>
<text font="4" height="15" left="108" textpieces="0" top="803" width="702">shows that Joe wrote papers P1 and P2 and P1 cited another paper P3. In the relation cat, she</text>
<text font="4" height="17" left="108" textpieces="0" top="823" width="702">provides Tuffy with a subset of papers and the categories into which they fall. The cat relation is</text>
<text font="4" height="15" left="108" textpieces="0" top="844" width="702">incomplete: some papers are not labeled. We can think of each possible labeling of these papers as</text>
<text font="4" height="15" left="108" textpieces="0" top="864" width="702">an instantiation of the cat relation, which can be viewed as a possible world [8]. The classi&#64257;cation</text>
<text font="4" height="15" left="108" textpieces="0" top="884" width="702">task is to &#64257;nd the most likely labeling of papers by topic area, and hence the most likely possible</text>
<text font="4" height="15" left="108" textpieces="0" top="905" width="44">world.</text>
<text font="4" height="15" left="133" textpieces="0" top="922" width="677">To tell the system which possible world it should produce, the user provides (in addition to the</text>
<text font="4" height="15" left="108" textpieces="0" top="940" width="702">above data) a set of rules that incorporate their knowledge of the problem. A simple example rule</text>
<text font="4" height="15" left="108" textpieces="0" top="958" width="38">is F1:</text>
<text font="2" height="12" left="331" textpieces="1" top="978" width="255">cat(p, c1), cat(p, c2) =&gt; c1 = c2  (F1)</text>
<text font="4" height="15" left="115" textpieces="1" top="1003" width="695">Intuitively, F1 says that a paper should be in one category. In MLNs, this rule may be hard,</text>
<text font="4" height="15" left="108" textpieces="0" top="1024" width="702">meaning that it behaves like a standard key constraint: in any possible world, each paper must</text>
<text font="4" height="15" left="455" textpieces="0" top="1069" width="8">4</text>
<text font="4" height="15" left="108" textpieces="0" top="113" width="702">be in at most one category. This rule may also be soft, meaning that it may be violated in some</text>
<text font="4" height="15" left="108" textpieces="0" top="133" width="702">possible worlds. For example, in some worlds a paper may be in two categories. Soft rules also have</text>
<text font="4" height="15" left="108" textpieces="0" top="154" width="701">weights that intuitively tell us how likely the rule is to hold in a possible world. In this example, F1</text>
<text font="4" height="15" left="108" textpieces="1" top="174" width="702">is a soft rule and has weight 5. Roughly, this means that a &#64257;xed paper is e5times more likely to be</text>
<text font="4" height="15" left="108" textpieces="1" top="194" width="702">in a single category compared to being in 2 categories.2 MLNs can also involve data in non-trivial</text>
<text font="4" height="15" left="108" textpieces="0" top="215" width="468">ways, we refer the reader to &#167;A.1 for a more complete exposition.</text>
<text font="4" height="15" left="108" textpieces="1" top="258" width="702">Query Model  Given the data and the rules, a user may write arbitrary queries in terms of the</text>
<text font="4" height="17" left="108" textpieces="0" top="278" width="702">relations. In Tuffy, the system is responsible for &#64257;lling in whatever missing data is needed: in this</text>
<text font="4" height="15" left="108" textpieces="0" top="299" width="702">example, the category of each unlabeled paper is unknown, and so to answer a query the system</text>
<text font="4" height="15" left="108" textpieces="0" top="319" width="441">infers the most likely labels for each paper from the evidence.</text>
<text font="1" height="16" left="108" textpieces="1" top="362" width="219">2.2  Semantics of MLNs</text>
<text font="4" height="17" left="108" textpieces="0" top="391" width="702">We describe the semantics of MLNs. Formally, we &#64257;rst &#64257;x a schema &#963; (as in Figure 1) and a domain</text>
<text font="4" height="15" left="108" textpieces="2" top="409" width="702">of constants D. Given as input a set of formula &#175; F = F1, . . . , FN (in clausal form3) with weights</text>
<text font="4" height="15" left="108" textpieces="1" top="427" width="702">w1, . . . , wN, they de&#64257;ne a probability distribution over possible worlds (deterministic databases).</text>
<text font="4" height="15" left="108" textpieces="0" top="445" width="702">To construct this probability distribution, the &#64257;rst step is grounding: given a formula F with free</text>
<text font="4" height="15" left="108" textpieces="5" top="463" width="544">variables &#175; x = (x1, . . . , xm), then for each &#175; d &#8712; Dm, we create a new formula g&#175;</text>
<text font="5" height="11" left="644" textpieces="0" top="470" width="7">d</text>
<text font="4" height="15" left="656" textpieces="0" top="463" width="154">called a ground clause</text>
<text font="4" height="15" left="108" textpieces="1" top="481" width="64">where g&#175;</text>
<text font="5" height="11" left="163" textpieces="0" top="488" width="7">d</text>
<text font="4" height="15" left="176" textpieces="3" top="481" width="634">denotes the result of substituting each variable xi of F with di. For example, for F3the</text>
<text font="4" height="15" left="108" textpieces="1" top="500" width="695">variables are {p1, p2, c}: one tuple of constants is &#175; d = (&#8216;P1&#8217;, &#8216;P2&#8217;, &#8216;DB&#8217;) and the ground formula f</text>
<text font="5" height="11" left="805" textpieces="1" top="505" width="5">&#175; d</text>
<text font="4" height="15" left="108" textpieces="0" top="518" width="16">is:</text>
<text font="2" height="12" left="283" textpieces="0" top="538" width="352">cat(&#8216;P1&#8217;, &#8216;DB&#8217;), refers(&#8216;P1&#8217;, &#8216;P2&#8217;) =&gt; cat(&#8216;P2&#8217;, &#8216;DB&#8217;)</text>
<text font="4" height="15" left="113" textpieces="0" top="565" width="697">Each constituent in the ground formula, such as cat(&#8216;P1&#8217;, &#8216;DB&#8217;) and refers(&#8216;P1&#8217;, &#8216;P2&#8217;), is called a</text>
<text font="4" height="15" left="108" textpieces="1" top="586" width="701">ground predicate or atom for short. In the worst case there are D3 ground clauses for F3. For each</text>
<text font="4" height="15" left="108" textpieces="1" top="606" width="701">formula Fi (for i = 1 . . . N ), we perform the above process. Each ground clause g of a formula Fi</text>
<text font="4" height="15" left="108" textpieces="1" top="626" width="702">is assigned the same weight, wi. So, a ground clause of F1 has weight 5, while any ground clause</text>
<text font="4" height="15" left="108" textpieces="3" top="647" width="701">of F2 has weight 1. We denote by G = (&#175; g, w) the set of all ground clauses of &#175; F and a function w</text>
<text font="4" height="17" left="108" textpieces="1" top="667" width="702">that maps each ground clause to its assigned weight. Fix an MLN &#175; F , then for any possible world</text>
<text font="4" height="15" left="108" textpieces="0" top="687" width="702">(instance) I we say a ground clause g is violated if w(g) &gt; 0 and g is false in I or if w(g) &lt; 0 and</text>
<text font="4" height="15" left="108" textpieces="0" top="708" width="702">g is true in I. We denote the set of ground clauses violated in a world I as V (I). The cost of the</text>
<text font="4" height="15" left="108" textpieces="0" top="728" width="70">world I is</text>
<text font="4" height="15" left="380" textpieces="0" top="748" width="67">cost(I) =</text>
<text font="5" height="11" left="451" textpieces="0" top="772" width="41">g&#8712;V (I)</text>
<text font="4" height="15" left="496" textpieces="1" top="747" width="314">|w(g)|                                   (1)</text>
<text font="4" height="17" left="108" textpieces="0" top="797" width="670">Through cost, an MLN de&#64257;nes a probability distribution over all instances (denoted Inst) as:</text>
<text font="4" height="15" left="242" textpieces="1" top="836" width="280">Pr[I] = Z&#8722;1exp {&#8722;cost(I)} where Z =</text>
<text font="5" height="11" left="527" textpieces="0" top="859" width="38">J &#8712;Inst</text>
<text font="4" height="15" left="568" textpieces="0" top="836" width="108">exp {&#8722;cost(J )}</text>
<text font="4" height="15" left="108" textpieces="0" top="888" width="702">A lowest cost world I is called a most likely world. Since cost(I) &#8805; 0, if cost(I) = 0 then I is a</text>
<text font="4" height="15" left="108" textpieces="0" top="908" width="702">most likely world. On the other hand the most likely world may have positive cost. There are two</text>
<text font="4" height="17" left="108" textpieces="0" top="929" width="702">main types of inference with MLNs: MAP inference, where we want to &#64257;nd a most likely world,</text>
<text font="6" height="8" left="127" textpieces="0" top="957" width="682">2In MLNs, it is not possible to give a direct probabilistic interpretation of weights [21]. In practice, the weights</text>
<text font="7" height="12" left="108" textpieces="0" top="977" width="702">associated to formula are learned,which compensates for their non-intuitive nature. In this work, we do not discuss</text>
<text font="7" height="12" left="108" textpieces="0" top="993" width="158">the mechanics of learning.</text>
<text font="6" height="8" left="127" textpieces="0" top="1007" width="682">3Clausal form is a disjunction of positive or negative literals. For example, the rule is R(a) =&gt; R(b) is not in</text>
<text font="7" height="12" left="108" textpieces="0" top="1026" width="439">clausal form, but is equivalent to &#172;R(a) &#8744; R(b), which is in clausal form.</text>
<text font="4" height="15" left="455" textpieces="0" top="1069" width="8">5</text>
<text font="4" height="17" left="108" textpieces="0" top="113" width="702">and marginal inference, where we want to compute marginal probabilities. Tuffy is capable of</text>
<text font="4" height="15" left="108" textpieces="0" top="133" width="702">both types of inference, but we present only MAP inference in the body of this paper. We refer</text>
<text font="4" height="15" left="108" textpieces="0" top="154" width="368">the reader to &#167;A.5 for details of marginal inference.</text>
<text font="1" height="16" left="108" textpieces="1" top="196" width="127">2.3  Inference</text>
<text font="4" height="17" left="108" textpieces="0" top="228" width="702">We now describe the state of the art of inference for MLNs (as in Alchemy, the reference MLN</text>
<text font="4" height="15" left="108" textpieces="0" top="249" width="122">implementation).</text>
<text font="4" height="15" left="108" textpieces="1" top="292" width="702">Grounding  Conceptually, to obtain the ground clauses of an MLN formula F , the most straight-</text>
<text font="4" height="15" left="108" textpieces="0" top="312" width="702">forward way is to enumerate all possible assignments to the free variables in F . There have been</text>
<text font="4" height="15" left="108" textpieces="0" top="333" width="702">several heuristics in the literature that improve the grounding process by pruning groundings that</text>
<text font="4" height="17" left="108" textpieces="0" top="353" width="702">have no e&#64256;ect on inference results; we describe the heuristics that Tuffy (and Alchemy) im-</text>
<text font="4" height="15" left="108" textpieces="0" top="373" width="702">plements in &#167;A.3. The set of ground clauses corresponds to a hypergraph where each atom is a</text>
<text font="4" height="15" left="108" textpieces="0" top="394" width="702">node and each clause is a hyperedge. This graph structure is often called a Markov Random Field</text>
<text font="4" height="15" left="108" textpieces="0" top="414" width="375">(MRF). We describe this structure formally in &#167;A.2.</text>
<text font="4" height="15" left="108" textpieces="1" top="457" width="702">Search  Finding a most likely world of an MLN is a generalization of the (NP-hard) MaxSAT</text>
<text font="4" height="15" left="108" textpieces="0" top="477" width="702">problem. In this paper we concentrate on one of the most popular heuristic search algorithms,</text>
<text font="4" height="17" left="108" textpieces="0" top="498" width="702">WalkSAT [14], which is used by Alchemy. WalkSAT works by repeatedly selecting a random</text>
<text font="4" height="15" left="108" textpieces="0" top="518" width="702">violated clause and &#8220;&#64257;xing&#8221; it by &#64258;ipping (i.e., changing the truth value of) an atom in it (see</text>
<text font="4" height="15" left="108" textpieces="0" top="538" width="702">&#167;A.4). As with any heuristic search, we cannot be sure that we have achieved the optimal, and so</text>
<text font="4" height="15" left="108" textpieces="0" top="559" width="702">the goal of any system that executes such a search procedure is: execute more search steps in the</text>
<text font="4" height="17" left="108" textpieces="0" top="579" width="702">same amount of time. To keep the comparison with Alchemy fair, we only discuss WalkSAT in</text>
<text font="4" height="15" left="108" textpieces="0" top="599" width="518">the body and defer our experience with other search algorithms to &#167;C.1.</text>
<text font="4" height="15" left="108" textpieces="1" top="643" width="702">Problem Description  The primary challenge that we address in this paper is scaling both</text>
<text font="4" height="15" left="108" textpieces="0" top="663" width="702">phases of MAP inference algorithms, grounding and search, using an RDBMS. Second, our goal</text>
<text font="4" height="15" left="108" textpieces="0" top="683" width="702">is to improve the number of (e&#64256;ective) steps of the local search procedure using parallelism and</text>
<text font="4" height="15" left="108" textpieces="0" top="704" width="702">partitioning &#8211; but only when it provably improves the search quality. To achieve these goals,</text>
<text font="4" height="17" left="108" textpieces="0" top="724" width="702">we attack three main technical challenges: (1) e&#64259;ciently grounding large MLNs, (2) e&#64259;ciently</text>
<text font="4" height="17" left="108" textpieces="0" top="744" width="702">performing inference (search) on large MLNs, and (3) designing partitioning and partition-aware</text>
<text font="4" height="15" left="108" textpieces="0" top="765" width="506">search algorithms that preserve (or enhance) search quality and speed.</text>
<text font="3" height="19" left="108" textpieces="1" top="813" width="185">3  Tu&#64256;y Systems</text>
<text font="4" height="15" left="108" textpieces="0" top="853" width="702">In this section, we describe our technical contributions: a bottom-up grounding approach to fully</text>
<text font="4" height="15" left="108" textpieces="0" top="873" width="702">leverage the RDBMS (&#167;3.1); a hybrid main-memory RDBMS architecture to support e&#64259;cient end-</text>
<text font="4" height="15" left="108" textpieces="0" top="893" width="702">to-end inference (&#167;3.2). In &#167;3.3 and &#167;3.4 we discuss data partitioning which dramatically improves</text>
<text font="4" height="14" left="108" textpieces="0" top="917" width="248">Tuffy&#8217;s space and time e&#64259;ciency.</text>
<text font="1" height="16" left="108" textpieces="1" top="956" width="398">3.1  Grounding with a Bottom-up Approach</text>
<text font="4" height="17" left="108" textpieces="0" top="988" width="702">We describe how Tuffy performs grounding. In contrast to top-down approaches (similar to</text>
<text font="4" height="17" left="108" textpieces="0" top="1009" width="702">Prolog) that employ nested loops, Tuffy takes a bottom-up approach (similar to Datalog) and</text>
<text font="4" height="15" left="455" textpieces="0" top="1069" width="8">6</text>
<text font="4" height="15" left="108" textpieces="0" top="113" width="702">expresses grounding as a sequence of SQL queries. Each SQL query is optimized by the RDBMS,</text>
<text font="4" height="17" left="108" textpieces="0" top="133" width="702">which allows Tuffy to complete the grounding process orders of magnitude more quickly than</text>
<text font="4" height="15" left="108" textpieces="0" top="154" width="123">prior approaches.</text>
<text font="4" height="15" left="133" textpieces="3" top="174" width="677">For each predicate P ( &#175; A) in the input MLN, Tuffy creates a relation RP(aid, &#175; A, truth) where</text>
<text font="4" height="15" left="108" textpieces="2" top="194" width="702">each row ap represents an atom, aid is a globally unique identi&#64257;er, &#175; A is the tuple of arguments of</text>
<text font="4" height="15" left="108" textpieces="1" top="215" width="702">P , and truth is a three-valued attribute that indicates if apis true or false (in the evidence), or not</text>
<text font="4" height="17" left="108" textpieces="0" top="235" width="702">speci&#64257;ed in the evidence. These tables form the input to grounding, and Tuffy constructs them</text>
<text font="4" height="15" left="108" textpieces="0" top="255" width="284">using standard bulk-loading techniques.</text>
<text font="4" height="17" left="133" textpieces="0" top="276" width="677">In Tuffy, we produce an output table C(cid, lits, weight) where each row corresponds to a</text>
<text font="4" height="15" left="108" textpieces="0" top="296" width="702">single ground clause. Here, cid is the id of a ground clause, lits is an array that stores the atom</text>
<text font="4" height="15" left="108" textpieces="0" top="316" width="702">id of each literal in this clause (and whether or not it is negated), and weight is the weight of</text>
<text font="4" height="15" left="108" textpieces="0" top="337" width="702">this clause. We &#64257;rst consider a formula without existential quanti&#64257;ers. In this case, the formula</text>
<text font="4" height="15" left="108" textpieces="4" top="357" width="702">F can be written as F (&#175; x) = l1&#8744; &#183; &#183; &#183; &#8744; lN where &#175; x are all variables in F . Tuffy produces a SQL</text>
<text font="4" height="15" left="108" textpieces="0" top="377" width="702">query Q for F that joins together the relations corresponding to the predicates in F to produce</text>
<text font="4" height="15" left="108" textpieces="0" top="398" width="702">the atom ids of the ground clauses (and whether or not they are negated). The join conditions in</text>
<text font="4" height="15" left="108" textpieces="0" top="418" width="702">Q enforce variable equality inside F , and incorporate the pruning strategies described in &#167;A.3. For</text>
<text font="4" height="15" left="108" textpieces="0" top="438" width="371">more details on the compilation procedure see &#167;B.2.</text>
<text font="1" height="16" left="108" textpieces="1" top="481" width="367">3.2  A Hybrid Architecture for Inference</text>
<text font="4" height="17" left="108" textpieces="0" top="513" width="702">Our initial prototype of Tuffy ran both grounding and search in the RDBMS. While the grounding</text>
<text font="4" height="15" left="108" textpieces="0" top="533" width="702">phase described in the previous section had good performance and scalability, we found that search</text>
<text font="4" height="15" left="108" textpieces="0" top="554" width="702">in an RDBMS is often a bottleneck. Thus, we design a hybrid architecture that allows e&#64259;cient</text>
<text font="4" height="15" left="108" textpieces="0" top="574" width="702">in-memory search while retaining the performance bene&#64257;ts of RDBMS-based grounding. To see</text>
<text font="4" height="15" left="108" textpieces="0" top="594" width="702">why in-memory search is critical, recall that WalkSAT works by selecting an unsatis&#64257;ed clause C,</text>
<text font="4" height="15" left="108" textpieces="0" top="615" width="702">selecting an atom in C and &#8220;&#64258;ipping&#8221; that atom to satisfy C. Thus, WalkSAT performs a large</text>
<text font="4" height="15" left="108" textpieces="0" top="635" width="702">number of random accesses to the data representing ground clauses and atoms. Moreover, the data</text>
<text font="4" height="15" left="108" textpieces="0" top="655" width="702">that is accessed in one iteration depends on the data that is accessed in the previous iteration. And</text>
<text font="4" height="15" left="108" textpieces="0" top="676" width="702">so, this access pattern prevents both e&#64256;ective caching and parallelism, which causes a high overhead</text>
<text font="4" height="15" left="108" textpieces="0" top="696" width="702">per data access. Thus, we implement a hybrid architecture where the RDBMS performs grounding</text>
<text font="4" height="17" left="108" textpieces="0" top="716" width="702">and Tuffy is able to read the result of grounding from the RDBMS into memory and perform</text>
<text font="4" height="17" left="108" textpieces="0" top="737" width="702">inference. If the grounding result is too large to &#64257;t in memory, Tuffy invokes an implementation</text>
<text font="4" height="15" left="108" textpieces="0" top="757" width="702">of search directly inside the RDBMS (&#167;B.3). This approach is much less e&#64259;cient than in-memory</text>
<text font="4" height="15" left="108" textpieces="0" top="777" width="702">search, but it runs on very large datasets without crashing. &#167;B.4 illustrates the architecture of</text>
<text font="4" height="14" left="108" textpieces="0" top="800" width="161">Tuffy in more detail.</text>
<text font="4" height="15" left="133" textpieces="0" top="818" width="677">While it is clear that this hybrid approach is at least as scalable as a direct memory imple-</text>
<text font="4" height="17" left="108" textpieces="0" top="838" width="702">mentation, such as Alchemy; in fact, there are cases where Tuffy can run in-memory search</text>
<text font="4" height="17" left="108" textpieces="0" top="858" width="702">while Alchemy would crash. The reason is that the space requirement of a purely in-memory</text>
<text font="4" height="15" left="108" textpieces="0" top="879" width="702">implementation is determined by the peak memory footprint throughout grounding and search,</text>
<text font="4" height="17" left="108" textpieces="0" top="899" width="702">whereas Tuffy needs main memory only for search. For example, on a dataset called Relational</text>
<text font="4" height="17" left="108" textpieces="0" top="919" width="702">Classi&#64257;cation (RC), Alchemy allocated 2.8 GB of RAM only to produce 4.8 MB of ground clauses.</text>
<text font="4" height="17" left="108" textpieces="0" top="940" width="304">On RC, Tuffy uses only 19 MB of RAM.</text>
<text font="4" height="15" left="455" textpieces="0" top="1069" width="8">7</text>
<text font="1" height="16" left="108" textpieces="1" top="112" width="374">3.3  Partitioning to Improve Performance</text>
<text font="4" height="17" left="108" textpieces="0" top="144" width="702">In the following two sections, we study how to further improve Tuffy&#8217;s space and time e&#64259;ciency</text>
<text font="4" height="15" left="108" textpieces="0" top="164" width="702">without sacri&#64257;cing its scalability. The underlying idea is simple: we will try to partition the data.</text>
<text font="4" height="15" left="108" textpieces="0" top="185" width="702">By splitting the problem into smaller pieces, we can reduce the memory footprint and introduce</text>
<text font="4" height="15" left="108" textpieces="0" top="205" width="702">parallelism, which conceptually breaks the sequential nature of the search. These are expected</text>
<text font="4" height="15" left="108" textpieces="0" top="225" width="702">bene&#64257;ts of partitioning. An unexpected bene&#64257;t is an exponentially increase of the e&#64256;ective search</text>
<text font="4" height="15" left="108" textpieces="0" top="246" width="281">speed, a point that we return to below.</text>
<text font="4" height="17" left="133" textpieces="0" top="266" width="677">First, observe that the logical forms of MLNs often result in an MRF with multiple disjoint</text>
<text font="4" height="15" left="108" textpieces="0" top="286" width="702">components (see &#167;B.5). For example, on the RC dataset there are 489 components. Let G be</text>
<text font="4" height="15" left="108" textpieces="2" top="307" width="702">an MRF with components G1, &#183; &#183; &#183; , Gk; let I be a truth assignment to the atoms in G and Ii its</text>
<text font="4" height="15" left="108" textpieces="0" top="327" width="313">projection over Gi. Then, it&#8217;s clear that &#8704;I:</text>
<text font="4" height="15" left="361" textpieces="0" top="360" width="76">costG(I) =</text>
<text font="5" height="11" left="443" textpieces="0" top="383" width="37">1&#8804;i&#8804;k</text>
<text font="4" height="15" left="483" textpieces="1" top="360" width="73">costGi(Ii).</text>
<text font="4" height="15" left="108" textpieces="1" top="411" width="701">Hence, instead of minimizing costG(I) directly, it su&#64259;ces to minimize each individual costGi(Ii).</text>
<text font="4" height="15" left="108" textpieces="1" top="431" width="702">The bene&#64257;t is that, even if G itself does not &#64257;t in memory, it is possible that each Gidoes. As such,</text>
<text font="4" height="15" left="108" textpieces="1" top="451" width="684">we can solve each Giwith in-memory search one by one, and &#64257;nally merge the results together.</text>
<text font="4" height="15" left="133" textpieces="0" top="472" width="677">Component detection is done after the grounding phase and before the search phase, as follows.</text>
<text font="4" height="15" left="108" textpieces="0" top="492" width="702">We maintain an in-memory union-&#64257;nd structure over the nodes, and scan the clause table while</text>
<text font="4" height="15" left="108" textpieces="0" top="512" width="702">updating the union-&#64257;nd structure. The end result is the set of connected components in the MRF.</text>
<text font="4" height="15" left="108" textpieces="0" top="533" width="423">An immediate issue raised by partitioning is I/O e&#64259;ciency.</text>
<text font="4" height="15" left="108" textpieces="1" top="575" width="702">E&#64259;cient Data Loading  Once an MRF is split into components, loading in and running inference</text>
<text font="4" height="15" left="108" textpieces="0" top="596" width="702">on each component sequentially one by one may incur many I/O operations, as there may be many</text>
<text font="4" height="15" left="108" textpieces="0" top="616" width="702">partitions. For example, the MRF of the Information Extraction (IE) dataset contains thousands</text>
<text font="4" height="15" left="108" textpieces="0" top="636" width="702">of 2-cliques and 3-cliques. One solution is to group the components into batches. The goal is to</text>
<text font="4" height="15" left="108" textpieces="0" top="656" width="702">minimize the total number of batches (and thereby the I/O cost of loading), and the constraint is</text>
<text font="4" height="15" left="108" textpieces="0" top="677" width="702">that each batch cannot exceed the memory budget. This is essentially the bin packing problem,</text>
<text font="4" height="15" left="108" textpieces="0" top="697" width="418">and we implement the First Fit Decreasing algorithm [28].</text>
<text font="4" height="17" left="133" textpieces="0" top="717" width="677">Once the partitions are in memory, we can take advantage of parallelism. In Tuffy, we execute</text>
<text font="4" height="15" left="108" textpieces="0" top="738" width="702">threads using a round-robin policy. It is future work to consider more advanced thread scheduling</text>
<text font="4" height="15" left="108" textpieces="0" top="758" width="537">policies, such as the Gittins Index in the multi-armed bandit literature [10].</text>
<text font="4" height="15" left="108" textpieces="1" top="801" width="702">Quality  Although processing each component individually produces solutions that are no worse</text>
<text font="4" height="15" left="108" textpieces="0" top="821" width="702">than processing the whole graph at once, we give an example to illustrate that independently</text>
<text font="4" height="15" left="108" textpieces="0" top="841" width="558">processing each component may result in exponentially faster speed of search.</text>
<text font="4" height="15" left="108" textpieces="0" top="874" width="702">Example 1 Consider an MRF consisting of N identical connected components each containing</text>
<text font="4" height="15" left="108" textpieces="1" top="895" width="333">two atoms {Xi, Yi} and three weighted clauses</text>
<text font="4" height="15" left="350" textpieces="2" top="926" width="216">{(Xi, 1), (Yi, 1), (Xi&#8744; Yi, &#8722;1)},</text>
<text font="4" height="15" left="108" textpieces="0" top="958" width="702">where i = 1 . . . N . Based on how WalkSAT works, it&#8217;s not hard to show that, if N = 1, starting</text>
<text font="4" height="15" left="108" textpieces="2" top="979" width="702">from a random state, the expected hitting time4 of the optimal state, i.e. (X1, Y1) = (1, 1), is no</text>
<text font="6" height="8" left="127" textpieces="0" top="1007" width="682">4The hitting time is a standard notion from Markov Chains [9], it is a random variable that represents the number</text>
<text font="7" height="12" left="108" textpieces="0" top="1026" width="405">of steps taken by WalkSAT to reach an optimum for the &#64257;rst time.</text>
<text font="4" height="15" left="455" textpieces="0" top="1069" width="8">8</text>
<text font="2" height="15" left="415" textpieces="1" top="126" width="91">G1         G2 </text>
<text font="7" height="13" left="456" textpieces="0" top="119" width="10">e </text>
<text font="7" height="13" left="437" textpieces="1" top="131" width="48">a      b </text>
<text font="4" height="15" left="404" textpieces="0" top="180" width="110">Figure 2: Ex. 2</text>
<text font="4" height="15" left="108" textpieces="0" top="232" width="702">more than 4. Therefore, if we run WalkSAT on each component separately, the expected runtime</text>
<text font="4" height="15" left="108" textpieces="0" top="252" width="702">of reaching the optimum is no more than 4N . Now consider the case where we run WalkSAT on</text>
<text font="4" height="15" left="108" textpieces="0" top="273" width="702">the whole MRF. Intuitively, reaching the optimal state requires &#8220;&#64257;xing&#8221; suboptimal components</text>
<text font="4" height="15" left="108" textpieces="0" top="293" width="702">one by one. As the number of optimal components increases, however, it becomes more and more</text>
<text font="4" height="15" left="108" textpieces="0" top="313" width="702">likely that one step of WalkSAT &#8220;breaks&#8221; an optimal component instead of &#64257;xing a suboptimal</text>
<text font="4" height="15" left="108" textpieces="0" top="334" width="702">component. Such check and balance makes it very di&#64259;cult for WalkSAT to reach the optimum.</text>
<text font="4" height="15" left="108" textpieces="1" top="354" width="697">Indeed, calculation in &#167;B.6 shows that the expected run time is at least 2N &#8211; an exponential gap!</text>
<text font="4" height="15" left="133" textpieces="0" top="392" width="677">To generalize this example, we need some notations. Let G be an MRF with components</text>
<text font="4" height="15" left="108" textpieces="3" top="413" width="702">G1, . . . , GN. For i = 1, . . . , N , let Oi be the set of optimal states of Gi, and Si the set of non-</text>
<text font="4" height="15" left="108" textpieces="3" top="433" width="702">optimal states of Githat di&#64256;er only by one bit from some x&#8727;&#8712; Oi; let Pi(x &#8594; y) be the transition</text>
<text font="4" height="15" left="108" textpieces="0" top="453" width="701">probability of WalkSAT running on Gi, i.e., the probability that one step of WalkSAT would take</text>
<text font="4" height="15" left="108" textpieces="3" top="474" width="702">Gi from x to y. Let x be a state of Gi, denote by vi(x) the number of violated clauses in Gi at</text>
<text font="4" height="15" left="108" textpieces="0" top="494" width="101">state x; de&#64257;ne</text>
<text font="4" height="15" left="289" textpieces="0" top="516" width="53">&#945;i(x) =</text>
<text font="5" height="11" left="348" textpieces="0" top="539" width="29">y&#8712;Oi</text>
<text font="4" height="15" left="381" textpieces="1" top="516" width="137">Pi(x &#8594; y), &#946;i(x) =</text>
<text font="5" height="11" left="522" textpieces="0" top="539" width="27">y&#8712;Si</text>
<text font="4" height="15" left="553" textpieces="0" top="516" width="76">Pi(x &#8594; y).</text>
<text font="4" height="15" left="108" textpieces="0" top="563" width="357">For any non-empty subset H &#8838; {1, . . . , N }, de&#64257;ne</text>
<text font="4" height="15" left="330" textpieces="0" top="609" width="53">r(H) =</text>
<text font="4" height="15" left="392" textpieces="2" top="597" width="158">mini&#8712;Hminx&#8712;Oivi(x)&#946;</text>
<text font="5" height="11" left="552" textpieces="0" top="603" width="26">i(x)</text>
<text font="4" height="15" left="389" textpieces="3" top="620" width="193">maxi&#8712;Hmaxx&#8712;Sivi(x)&#945;i(x)</text>
<text font="4" height="15" left="584" textpieces="0" top="609" width="5">.</text>
<text font="4" height="15" left="108" textpieces="0" top="654" width="702">Theorem 3.1. Let H be any non-empty subset of {1, . . . , N } s.t. r = r(H) &gt; 0, then Whole-</text>
<text font="4" height="15" left="108" textpieces="1" top="675" width="702">MRF WalkSAT on G takes at least 2|H|r/(2+r) more steps than component-wise WalkSAT on the</text>
<text font="4" height="15" left="108" textpieces="0" top="695" width="125">components of G.</text>
<text font="4" height="15" left="108" textpieces="1" top="729" width="702">The proof can be found in &#167;B.6.   In the worst case, r = 0 &#8211; i.e., WalkSAT never jumps out</text>
<text font="4" height="15" left="108" textpieces="1" top="749" width="702">of an optimal state in all components &#8211; and partitioning would become pure overhead.   On an</text>
<text font="4" height="15" left="108" textpieces="0" top="769" width="702">information extraction (IE) benchmark dataset, there is some H with |H| = 1196 and r(H) = 0.5.</text>
<text font="4" height="15" left="108" textpieces="1" top="790" width="702">Thus, the gap on this dataset is at least 2200&#8776; 1060. In practice, this explains why Tuffy produces</text>
<text font="4" height="17" left="108" textpieces="0" top="810" width="702">lower cost solutions in minutes than non-partition aware approaches such as Alchemy produce</text>
<text font="4" height="15" left="108" textpieces="0" top="830" width="112">even after days.</text>
<text font="1" height="16" left="108" textpieces="1" top="873" width="288">3.4  Further Partitioning MRFs</text>
<text font="4" height="15" left="108" textpieces="0" top="905" width="702">Although our algorithms are more scalable than prior approaches, if the largest component does</text>
<text font="4" height="15" left="108" textpieces="0" top="925" width="702">not &#64257;t in memory then we are forced to run the in-RDBMS version of inference, which is much</text>
<text font="4" height="15" left="108" textpieces="0" top="946" width="702">slower. Intuitively, if the graph is only weakly connected, then we should still be able to get the</text>
<text font="4" height="15" left="108" textpieces="0" top="966" width="576">exponential speed up of partitioning. To gain intuition, we consider an example.</text>
<text font="4" height="15" left="108" textpieces="1" top="1000" width="701">Example 2 Consider an MRF consisting of two equally sized subgraphs G1and G2, plus an edge</text>
<text font="4" height="15" left="108" textpieces="1" top="1020" width="702">e = (a, b) between them (Figure 2). Suppose that the expected hitting time of WalkSAT on Giis Hi.</text>
<text font="4" height="15" left="455" textpieces="0" top="1069" width="8">9</text>
<text font="4" height="15" left="108" textpieces="2" top="113" width="702">Since H1 and H2 are essentially independent, the hitting time of WalkSAT on G could be roughly</text>
<text font="4" height="15" left="108" textpieces="0" top="133" width="701">H1H2. On the other hand, consider the following scheme: enumerate all possible truth assignments</text>
<text font="4" height="15" left="108" textpieces="0" top="154" width="702">to one of the boundary variables {a, b}, say a &#8211; of which there are two &#8211; and conditioning on each</text>
<text font="4" height="15" left="108" textpieces="2" top="174" width="702">assignment, run WalkSAT on G1 and G2 independently. Clearly, the overall hitting time is no</text>
<text font="4" height="15" left="108" textpieces="4" top="194" width="702">more than 2(H1+ H2), which is a huge improvement over H1H2 since Hi is usually a high-order</text>
<text font="4" height="15" left="108" textpieces="0" top="215" width="348">polynomial or even exponential in the size of Gi.</text>
<text font="4" height="15" left="133" textpieces="0" top="248" width="677">To capitalize on this idea, we need to address two challenges: 1) designing an e&#64259;cient MRF</text>
<text font="4" height="15" left="108" textpieces="0" top="269" width="702">partitioning algorithm; and 2) designing an e&#64256;ective partition-aware search algorithm. We address</text>
<text font="4" height="15" left="108" textpieces="0" top="289" width="152">each of them in turn.</text>
<text font="4" height="15" left="108" textpieces="1" top="332" width="702">MRF Partitioning  Intuitively, to maximally utilize the memory budget, we want to partition</text>
<text font="4" height="15" left="108" textpieces="0" top="353" width="702">the MRF into roughly equal sizes; to minimize information loss, we want to minimize total weight</text>
<text font="4" height="15" left="108" textpieces="0" top="373" width="702">of clauses that span over multiple partitions, i.e., the cut size. To capture this notion, we de&#64257;ne a</text>
<text font="4" height="15" left="108" textpieces="2" top="393" width="684">balanced bisection of a hypergraph G = (V, E) as a partition of V = V1&#8746; V2 such that |V1| = |V</text>
<text font="5" height="11" left="794" textpieces="0" top="399" width="15">2|.</text>
<text font="4" height="15" left="108" textpieces="3" top="414" width="501">The cost of a bisection (V1, V2) is |{e &#8712; E|e &#8745; V1= &#8709; and e &#8745; V2= &#8709;}|.</text>
<text font="4" height="15" left="108" textpieces="0" top="447" width="702">Theorem 3.2. Consider the MLN &#915; given by the single rule p(x), r(x, y) &#8594; p(y) where r is an</text>
<text font="4" height="15" left="108" textpieces="0" top="468" width="702">evidence predicate. Then, the problem of &#64257;nding a minimum-cost balanced bisection of the MRF</text>
<text font="4" height="15" left="108" textpieces="0" top="488" width="462">that results from &#915; is NP-hard in the size of the evidence (data).</text>
<text font="4" height="15" left="108" textpieces="0" top="522" width="702">The proof (&#167;B.7) is by reduction to the graph minimum bisection problem [15], which is hard</text>
<text font="4" height="15" left="108" textpieces="0" top="542" width="702">to approximate (unless P = NP, there is no PTAS). In fact, the problem we are facing (multi-</text>
<text font="4" height="15" left="108" textpieces="0" top="562" width="702">way hypergraph partitioning) is more challenging than graph bisection, and has been extensively</text>
<text font="4" height="15" left="108" textpieces="0" top="583" width="702">studied [13, 25]. And so, we design a simple, greedy partitioning algorithm: it assigns each clause</text>
<text font="4" height="15" left="108" textpieces="0" top="603" width="702">to a bin in descending order by clause weight, subject to the constraint that no component in the</text>
<text font="4" height="15" left="108" textpieces="0" top="623" width="609">resulting graph is larger than an input parameter &#946;. We include pseudocode in &#167;B.8.</text>
<text font="4" height="15" left="108" textpieces="1" top="666" width="702">Partition-aware Search  We need to re&#64257;ne the search procedure to be aware of partitions: the</text>
<text font="4" height="15" left="108" textpieces="0" top="687" width="702">central challenge is that a clause in the cut may depend on atoms in two distinct partitions. Hence,</text>
<text font="4" height="15" left="108" textpieces="0" top="707" width="702">there are dependencies between the partitions. We exploit the idea in Example 2 to design the</text>
<text font="4" height="15" left="108" textpieces="0" top="727" width="702">following partition-aware search scheme &#8211; which is an instance of the Gauss-Seidel method from</text>
<text font="4" height="15" left="108" textpieces="1" top="748" width="702">nonlinear optimization [3, pg. 219]. Denote by X1, . . . , Xk the states (i.e., truth assignments to the</text>
<text font="4" height="15" left="108" textpieces="1" top="768" width="346">atoms) of the partitions. First initialize Xi= x0</text>
<text font="5" height="11" left="448" textpieces="0" top="776" width="4">i</text>
<text font="4" height="15" left="461" textpieces="0" top="768" width="349">for i = 1 . . . k. For t = 1 . . . T , for i = 1 . . . k, run</text>
<text font="4" height="15" left="108" textpieces="0" top="788" width="128">WalkSAT on xt&#8722;1</text>
<text font="5" height="11" left="215" textpieces="0" top="796" width="4">i</text>
<text font="4" height="15" left="243" textpieces="0" top="788" width="134">conditioned on {xt</text>
<text font="5" height="11" left="372" textpieces="0" top="796" width="5">j</text>
<text font="4" height="15" left="379" textpieces="0" top="788" width="139">|1 &#8804; j &lt; i} &#8746; {xt&#8722;1</text>
<text font="5" height="11" left="496" textpieces="0" top="796" width="5">j</text>
<text font="4" height="15" left="518" textpieces="0" top="788" width="174">|i &lt; j &#8804; k} to obtain xt</text>
<text font="5" height="11" left="687" textpieces="0" top="796" width="4">i</text>
<text font="4" height="15" left="693" textpieces="0" top="788" width="117">. Finally, return</text>
<text font="4" height="15" left="108" textpieces="0" top="810" width="25">{xT</text>
<text font="5" height="11" left="126" textpieces="0" top="819" width="4">i</text>
<text font="4" height="15" left="135" textpieces="0" top="810" width="84">|1 &#8804; i &#8804; k}.</text>
<text font="3" height="19" left="108" textpieces="1" top="859" width="169">4  Experiments</text>
<text font="4" height="17" left="108" textpieces="0" top="899" width="702">In this section, we validate &#64257;rst that our system Tuffy is orders of magnitude more scalable and</text>
<text font="4" height="15" left="108" textpieces="0" top="920" width="702">e&#64259;cient than prior approaches. We then validate that each of our techniques contributes to the</text>
<text font="4" height="15" left="108" textpieces="0" top="940" width="34">goal.</text>
<text font="4" height="15" left="108" textpieces="1" top="983" width="702">Experimental Setup  We select Alchemy, the currently most widely used MLN system, as our</text>
<text font="4" height="17" left="108" textpieces="0" top="1003" width="702">comparison point. Alchemy and Tuffy are implemented in C++ and Java, respectively. The</text>
<text font="4" height="17" left="108" textpieces="0" top="1024" width="702">RDBMS used by Tuffy is PostgreSQL 8.4. Unless speci&#64257;ed otherwise, all experiments are run</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">10</text>
<text font="4" height="15" left="108" textpieces="0" top="113" width="702">on an Intel Core2 at 2.4GHz with 4 GB of RAM running Red Hat Enterprise Linux 5. For fair</text>
<text font="4" height="17" left="108" textpieces="0" top="133" width="594">comparison, in all experiments Tuffy runs a single thread unless otherwise noted.</text>
<text font="4" height="15" left="108" textpieces="1" top="177" width="702">Datasets  We run Alchemy and Tuffy on four datasets; three of them (including their MLNs)</text>
<text font="4" height="17" left="108" textpieces="0" top="197" width="702">are taken directly from the Alchemy website [7]: Link Prediction (LP), given an administrative</text>
<text font="4" height="15" left="108" textpieces="0" top="217" width="702">database of a CS department, the goal is to predict student-adviser relationships; Information</text>
<text font="4" height="15" left="108" textpieces="0" top="238" width="702">Extraction (IE), given a set of Citeseer citations, the goal is to extract from them structured records</text>
<text font="4" height="15" left="108" textpieces="0" top="258" width="702">based on some domain-speci&#64257;c heuristics; and Entity Resolution (ER), which is to deduplicate</text>
<text font="4" height="15" left="108" textpieces="0" top="278" width="702">citation records based on word similarity. These tasks have been extensively used in prior work.</text>
<text font="4" height="15" left="108" textpieces="0" top="299" width="702">The last task, Relational Classi&#64257;cation (RC), performs classi&#64257;cation on the Cora dataset [16]; RC</text>
<text font="4" height="15" left="108" textpieces="0" top="319" width="548">contains all the rules in Figure 1. Table 1 contains statistics about the data.</text>
<text font="2" height="13" left="436" textpieces="3" top="354" width="189">LP      IE     RC    ER</text>
<text font="2" height="13" left="293" textpieces="4" top="373" width="332">#relations             22       18        4     10</text>
<text font="2" height="13" left="293" textpieces="4" top="391" width="332">#rules                 94      1K       15   3.8K</text>
<text font="2" height="13" left="293" textpieces="4" top="409" width="331">#entities             302    2.6K     51K    510</text>
<text font="2" height="13" left="293" textpieces="4" top="426" width="331">#evidence tuples     731   0.25M   0.43M    676</text>
<text font="2" height="13" left="293" textpieces="4" top="444" width="332">#query atoms      4.6K   0.34M     10K   16K</text>
<text font="2" height="13" left="293" textpieces="4" top="462" width="331">#components           1     5341      489       1</text>
<text font="4" height="15" left="365" textpieces="0" top="498" width="189">Table 1: Dataset statistics</text>
<text font="1" height="16" left="108" textpieces="1" top="562" width="253">4.1  High-level Performance</text>
<text font="4" height="17" left="108" textpieces="0" top="594" width="702">We empirically demonstrate that Tuffy with all the techniques we have described has faster</text>
<text font="4" height="15" left="108" textpieces="0" top="614" width="702">grounding, higher search speed, lower memory usage, and in some cases produces much better</text>
<text font="4" height="17" left="108" textpieces="0" top="634" width="702">solutions than a competitor main memory approach, Alchemy. Recall that the name of the game</text>
<text font="4" height="17" left="108" textpieces="0" top="655" width="702">is to produce low-cost solutions quickly. With this in mind, we run Tuffy and Alchemy on each</text>
<text font="4" height="15" left="108" textpieces="0" top="675" width="702">dataset for 7500 seconds, and track the cost of the best solution found up to any moment; on</text>
<text font="4" height="15" left="108" textpieces="0" top="695" width="702">datasets that have multiple components, namely IE and RC, we apply the partitioning strategy</text>
<text font="4" height="17" left="108" textpieces="0" top="715" width="702">in &#167;3.3 on Tuffy. As shown in Figure 3, Tuffy often reaches a best solution within orders of</text>
<text font="4" height="17" left="108" textpieces="0" top="736" width="702">magnitude less time than Alchemy; secondly, the result quality of Tuffy is at least as good as &#8211;</text>
<text font="4" height="17" left="108" textpieces="0" top="756" width="702">sometimes substantially better (e.g., on IE and RC) than &#8211; Alchemy. Here, we have zoomed the</text>
<text font="4" height="15" left="108" textpieces="0" top="776" width="702">time axes into interesting areas. Since &#8220;solution cost&#8221; is unde&#64257;ned during grounding, each curve</text>
<text font="4" height="15" left="108" textpieces="0" top="797" width="702">begins only when grounding is completed. We analyze the experiment results in more details in</text>
<text font="4" height="15" left="108" textpieces="0" top="817" width="157">the following sections.</text>
<text font="1" height="16" left="108" textpieces="1" top="860" width="324">4.2  E&#64256;ect of Bottom-up Grounding</text>
<text font="4" height="17" left="108" textpieces="0" top="892" width="702">We validate that the RDBMS-based grounding approach in Tuffy allows us to complete the</text>
<text font="4" height="17" left="108" textpieces="0" top="912" width="702">grounding process orders of magnitude more e&#64259;ciently than Alchemy. To make this point, we</text>
<text font="4" height="17" left="108" textpieces="0" top="932" width="702">run Tuffy and Alchemy on the four datasets, and show their grounding time in Table 2. We can</text>
<text font="4" height="17" left="108" textpieces="0" top="953" width="702">see that Tuffy outperforms Alchemy by orders of magnitude at run time in the grounding phase</text>
<text font="4" height="15" left="108" textpieces="0" top="973" width="702">(a factor of 225 on the ER dataset). To understand the di&#64256;erences, we dug deeper with a lesion</text>
<text font="4" height="15" left="108" textpieces="0" top="993" width="702">study, and found that sort join and hash join algorithms (along with predicate pushdown) are the</text>
<text font="4" height="17" left="108" textpieces="0" top="1014" width="702">key components of the RDBMS that speeds up the grounding process of Tuffy (&#167;C.2). Tuffy</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">11</text>
<text font="9" height="13" left="471" textpieces="0" top="182" width="44">1.0E+03</text>
<text font="9" height="13" left="471" textpieces="0" top="150" width="44">2.0E+03</text>
<text font="9" height="13" left="471" textpieces="0" top="117" width="44">3.0E+03</text>
<text font="9" height="13" left="524" textpieces="2" top="200" width="134">0         20         40</text>
<text font="10" height="15" left="465" textpieces="0" top="157" width="0">cost </text>
<text font="11" height="17" left="607" textpieces="0" top="117" width="16">IE </text>
<text font="4" height="17" left="619" textpieces="0" top="132" width="62">Alchemy </text>
<text font="4" height="17" left="555" textpieces="0" top="172" width="38">Tuffy </text>
<text font="9" height="13" left="241" textpieces="0" top="183" width="44">0.0E+00</text>
<text font="9" height="13" left="241" textpieces="0" top="161" width="44">1.0E+04</text>
<text font="9" height="13" left="241" textpieces="0" top="139" width="44">2.0E+04</text>
<text font="9" height="13" left="241" textpieces="0" top="117" width="44">3.0E+04</text>
<text font="9" height="13" left="294" textpieces="2" top="200" width="159">0           50          100</text>
<text font="10" height="15" left="235" textpieces="0" top="158" width="0">cost </text>
<text font="11" height="17" left="342" textpieces="0" top="120" width="20">LP </text>
<text font="4" height="17" left="380" textpieces="1" top="142" width="-21">Alchemy                  Tuffy </text>
<text font="9" height="13" left="470" textpieces="0" top="287" width="44">0.0E+00</text>
<text font="9" height="13" left="470" textpieces="0" top="254" width="44">1.0E+05</text>
<text font="9" height="13" left="470" textpieces="0" top="222" width="44">2.0E+05</text>
<text font="9" height="13" left="523" textpieces="1" top="304" width="170">0   2000 4000 6000 8000</text>
<text font="10" height="15" left="464" textpieces="0" top="262" width="0">cost </text>
<text font="11" height="17" left="602" textpieces="0" top="220" width="21">ER </text>
<text font="7" height="13" left="535" textpieces="0" top="242" width="163">Alchemy grounding took 7 hr. </text>
<text font="4" height="17" left="585" textpieces="0" top="258" width="38">Tuffy </text>
<text font="9" height="13" left="241" textpieces="0" top="287" width="44">0.0E+00</text>
<text font="9" height="13" left="241" textpieces="0" top="265" width="44">2.0E+03</text>
<text font="9" height="13" left="241" textpieces="0" top="244" width="44">4.0E+03</text>
<text font="9" height="13" left="241" textpieces="0" top="222" width="44">6.0E+03</text>
<text font="9" height="13" left="294" textpieces="3" top="304" width="142">0    2000  4000  6000</text>
<text font="10" height="15" left="235" textpieces="0" top="262" width="0">cost </text>
<text font="11" height="17" left="341" textpieces="0" top="224" width="22">RC </text>
<text font="4" height="17" left="385" textpieces="0" top="242" width="62">Alchemy </text>
<text font="4" height="17" left="330" textpieces="0" top="254" width="38">Tuffy </text>
<text font="4" height="17" left="183" textpieces="0" top="347" width="552">Figure 3: Time-cost plots of Alchemy vs. Tuffy; the x axes are time (sec)</text>
<text font="4" height="17" left="108" textpieces="0" top="400" width="702">obviates the need for Alchemy to reimplement all of the optimization techniques in an RDBMS</text>
<text font="4" height="15" left="108" textpieces="0" top="420" width="94">from scratch.</text>
<text font="4" height="15" left="416" textpieces="3" top="453" width="181">LP  IE    RC     ER</text>
<text font="4" height="14" left="320" textpieces="4" top="476" width="277">Alchemy    48   13  3,913  23,891</text>
<text font="4" height="14" left="332" textpieces="4" top="497" width="266">Tuffy       6   13     40     106</text>
<text font="4" height="15" left="350" textpieces="0" top="532" width="218">Table 2: Grounding time (sec)</text>
<text font="1" height="16" left="108" textpieces="1" top="599" width="307">4.3  E&#64256;ect of Hybrid Architecture</text>
<text font="4" height="17" left="108" textpieces="0" top="631" width="702">We validate two technical claims: (1) the hybrid memory management strategy of Tuffy (even</text>
<text font="4" height="15" left="108" textpieces="0" top="651" width="702">without our partitioning optimizations) has comparable search rates to existing main memory</text>
<text font="4" height="17" left="108" textpieces="0" top="671" width="702">implementations (and much faster than RDBMS-based implementation) and (2) Tuffy maintains</text>
<text font="4" height="15" left="108" textpieces="0" top="692" width="702">a much smaller memory footprint (again without partitioning). Thus, we compare three approaches:</text>
<text font="4" height="17" left="108" textpieces="0" top="712" width="702">(1) Tuffy without the partitioning optimizations, called Tuffy-p (read: Tu&#64256;y minus p), (2) a</text>
<text font="4" height="17" left="108" textpieces="0" top="732" width="702">version of Tuffy (also without partitioning) that implements RDBMS-based WalkSAT (detailed</text>
<text font="4" height="17" left="108" textpieces="0" top="752" width="296">in &#167;B.3), Tuffy-mm, and (3) Alchemy.</text>
<text font="4" height="15" left="133" textpieces="0" top="773" width="677">Figure 4 illustrates the time-cost plots on LP and RC of all three approaches. We see from</text>
<text font="4" height="17" left="108" textpieces="0" top="793" width="702">RC that Tuffy-p is able to ground much more quickly than Alchemy (40s compared to 3913s).</text>
<text font="4" height="17" left="108" textpieces="0" top="813" width="702">Additionally, we see that, compared to Tuffy-mm, Tuffy-p&#8217;s in-memory search is orders of</text>
<text font="4" height="15" left="108" textpieces="0" top="834" width="702">magnitude faster at getting to their best reported solution (both approaches &#64257;nish grounding at</text>
<text font="4" height="15" left="108" textpieces="0" top="854" width="702">the same time, and so start search at the same time). To understand why, we measure the &#64258;ipping</text>
<text font="4" height="15" left="108" textpieces="0" top="874" width="702">rate, which is the number of steps performed by WalkSAT per second. As shown in Table 3, the</text>
<text font="4" height="17" left="108" textpieces="0" top="895" width="702">reason is that Tuffy-mm has a dramatically lower &#64258;ipping rate. We discuss the performance</text>
<text font="4" height="15" left="108" textpieces="0" top="915" width="433">bound of any RDBMS-based search implementation in &#167;C.1.</text>
<text font="4" height="17" left="133" textpieces="0" top="935" width="677">To validate our second claim, that Tuffy-p has a smaller memory footprint, we see in Table 4,</text>
<text font="4" height="17" left="108" textpieces="0" top="956" width="702">that on all datasets, the memory footprint of Tuffy is no more than 5% of Alchemy. Drilling</text>
<text font="4" height="17" left="108" textpieces="0" top="976" width="702">down, the reason is that the intermediate state size of Alchemy&#8217;s grounding process may be larger</text>
<text font="4" height="17" left="108" textpieces="0" top="996" width="702">than the size of grounding results. For example, on the RC dataset, Alchemy allocated 2.8 GB</text>
<text font="4" height="17" left="108" textpieces="0" top="1017" width="702">of RAM only to produce 4.8 MB of ground clauses. While Alchemy has to hold everything in</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">12</text>
<text font="7" height="13" left="240" textpieces="0" top="204" width="43">0.0E+00</text>
<text font="7" height="13" left="240" textpieces="0" top="168" width="43">1.0E+04</text>
<text font="7" height="13" left="240" textpieces="0" top="132" width="43">2.0E+04</text>
<text font="7" height="13" left="292" textpieces="2" top="221" width="135">0       1000     2000</text>
<text font="10" height="15" left="233" textpieces="0" top="167" width="0">cost </text>
<text font="10" height="15" left="339" textpieces="0" top="240" width="64">time (sec) </text>
<text font="11" height="17" left="367" textpieces="0" top="116" width="20">LP </text>
<text font="2" height="15" left="345" textpieces="0" top="155" width="97">Alchemy (solid) </text>
<text font="2" height="15" left="330" textpieces="0" top="139" width="88">Tuffy-p (dash) </text>
<text font="2" height="15" left="380" textpieces="0" top="174" width="63">Tuffy-mm </text>
<text font="7" height="13" left="473" textpieces="0" top="204" width="43">0.0E+00</text>
<text font="7" height="13" left="473" textpieces="0" top="163" width="43">2.0E+05</text>
<text font="7" height="13" left="473" textpieces="0" top="122" width="43">4.0E+05</text>
<text font="7" height="13" left="525" textpieces="2" top="221" width="169">0          4000        8000</text>
<text font="10" height="15" left="466" textpieces="0" top="167" width="0">cost </text>
<text font="10" height="15" left="574" textpieces="0" top="240" width="64">time (sec) </text>
<text font="11" height="17" left="600" textpieces="0" top="120" width="22">RC </text>
<text font="2" height="15" left="634" textpieces="0" top="159" width="56">Alchemy </text>
<text font="2" height="15" left="550" textpieces="0" top="189" width="47">Tuffy-p </text>
<text font="2" height="15" left="540" textpieces="0" top="139" width="63">Tuffy-mm </text>
<text font="4" height="17" left="108" textpieces="0" top="277" width="702">Figure 4: Time-cost plots of Alchemy vs. Tuffy-p (i.e., Tuffy without partitioning) vs. Tuffy-</text>
<text font="4" height="17" left="108" textpieces="0" top="297" width="325">mm (i.e., Tuffy with RDBMS-based search)</text>
<text font="4" height="15" left="421" textpieces="3" top="336" width="196">LP      IE     RC   ER</text>
<text font="4" height="14" left="301" textpieces="4" top="360" width="322">Alchemy   0.20M     1M    1.9K  0.9K</text>
<text font="4" height="14" left="295" textpieces="4" top="381" width="327">Tuffy-mm      0.9      13      0.9   0.03</text>
<text font="4" height="14" left="305" textpieces="4" top="402" width="318">Tuffy-p    0.11M  0.39M  0.17M  7.9K</text>
<text font="4" height="15" left="331" textpieces="0" top="436" width="255">Table 3: Flipping rates (#&#64258;ips/sec)</text>
<text font="4" height="17" left="108" textpieces="0" top="488" width="702">memory, Tuffy only needs to load the grounding result from the RDBMS at the end of grounding.</text>
<text font="4" height="17" left="108" textpieces="0" top="509" width="702">It follows that, given the same resources, there are MLNs that Tuffy can handle e&#64259;ciently while</text>
<text font="4" height="14" left="108" textpieces="0" top="532" width="702">Alchemy would crash. Indeed, on a dataset called &#8220;ER+&#8221; which is twice as large as ER, Alchemy</text>
<text font="4" height="15" left="108" textpieces="0" top="549" width="701">exhausts all 4GB of RAM and crashes soon after launching5, whereas Tuffy runs normally with</text>
<text font="4" height="15" left="108" textpieces="0" top="570" width="244">peak RAM usage of roughly 2GB.</text>
<text font="4" height="17" left="133" textpieces="0" top="590" width="677">From these experiments, we conclude that the hybrid architecture is crucial to Tuffy&#8217;s overall</text>
<text font="4" height="15" left="108" textpieces="0" top="610" width="68">e&#64259;ciency.</text>
<text font="1" height="16" left="108" textpieces="1" top="653" width="233">4.4  E&#64256;ect of Partitioning</text>
<text font="4" height="15" left="108" textpieces="0" top="685" width="702">In this section, we validate that, when there are multiple components in the data, partitioning not</text>
<text font="4" height="17" left="108" textpieces="0" top="705" width="702">only improves Tuffy&#8217;s space e&#64259;ciency, but &#8211; due to Theorem 3.1 &#8211; may actually enable Tuffy</text>
<text font="4" height="17" left="108" textpieces="0" top="726" width="702">to &#64257;nd substantially higher quality results. We compare Tuffy&#8217;s performance (with partitioning</text>
<text font="4" height="17" left="108" textpieces="0" top="746" width="535">enabled) against Tuffy-p: a version of Tuffy with partitioning disabled.</text>
<text font="4" height="17" left="133" textpieces="0" top="766" width="677">We run the search phase on each of the four datasets using three approaches: Alchemy,</text>
<text font="4" height="14" left="108" textpieces="0" top="790" width="702">Tuffy-p, and Tuffy (with partitioning). Tuffy-p and Alchemy run WalkSAT on the whole</text>
<text font="4" height="15" left="108" textpieces="1" top="807" width="702">MRF for 107 steps. Tuffy runs WalkSAT on each component in the MRF independently, each</text>
<text font="4" height="15" left="108" textpieces="1" top="827" width="219">component Gi receiving 107|G</text>
<text font="5" height="11" left="329" textpieces="0" top="833" width="163">i|/|G| steps, where |Gi</text>
<text font="4" height="15" left="494" textpieces="0" top="826" width="316">| and |G| are the numbers of atoms in this</text>
<text font="6" height="8" left="127" textpieces="0" top="857" width="494">5We verify on a separate machine that Alchemy requires at least 23GB of RAM.</text>
<text font="4" height="15" left="422" textpieces="3" top="908" width="246">LP        IE      RC       ER</text>
<text font="4" height="15" left="267" textpieces="4" top="929" width="402">clause table      5.2 MB   0.6 MB  4.8 MB  164 MB</text>
<text font="4" height="14" left="250" textpieces="4" top="952" width="418">Alchemy RAM   411 MB  206 MB  2.8 GB   3.5 GB</text>
<text font="4" height="14" left="254" textpieces="4" top="973" width="415">Tuffy-p RAM      9 MB     8 MB   19 MB  184 MB</text>
<text font="4" height="17" left="191" textpieces="0" top="1008" width="536">Table 4: Space e&#64259;ciency of Alchemy vs. Tuffy-p (without partitioning)</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">13</text>
<text font="4" height="15" left="108" textpieces="0" top="113" width="576">component and the MRF, respectively. This is weighted round-robin scheduling.</text>
<text font="4" height="15" left="426" textpieces="3" top="149" width="208">LP    IE    RC      ER</text>
<text font="4" height="15" left="289" textpieces="4" top="170" width="346">#components       1   5341     489        1</text>
<text font="4" height="14" left="283" textpieces="4" top="194" width="352">Tuffy-p RAM   9MB  8MB  19MB  184MB</text>
<text font="4" height="14" left="291" textpieces="4" top="215" width="344">Tuffy RAM    9MB  8MB  15MB  184MB</text>
<text font="4" height="14" left="289" textpieces="4" top="236" width="346">Tuffy-p cost    2534  1933    1943    18717</text>
<text font="4" height="14" left="296" textpieces="4" top="257" width="339">Tuffy cost     2534  1635    1281    18717</text>
<text font="4" height="17" left="169" textpieces="0" top="291" width="579">Table 5: Performance of Tuffy vs. Tuffy-p (i.e., Tuffy without partitioning)</text>
<text font="7" height="13" left="241" textpieces="0" top="440" width="27">1000</text>
<text font="7" height="13" left="241" textpieces="0" top="418" width="27">1400</text>
<text font="7" height="13" left="241" textpieces="0" top="395" width="27">1800</text>
<text font="7" height="13" left="241" textpieces="0" top="373" width="27">2200</text>
<text font="7" height="13" left="241" textpieces="0" top="350" width="27">2600</text>
<text font="7" height="13" left="277" textpieces="4" top="458" width="192">0      20     40     60     80</text>
<text font="10" height="15" left="235" textpieces="0" top="403" width="0">cost </text>
<text font="10" height="15" left="340" textpieces="0" top="477" width="64">time (sec) </text>
<text font="12" height="20" left="381" textpieces="0" top="350" width="20">IE </text>
<text font="7" height="13" left="385" textpieces="0" top="430" width="31">Tuffy </text>
<text font="7" height="13" left="286" textpieces="0" top="360" width="89">Tuffy-p (dotted) </text>
<text font="7" height="13" left="286" textpieces="0" top="376" width="86">Alchemy (solid) </text>
<text font="7" height="13" left="490" textpieces="0" top="440" width="7">0</text>
<text font="7" height="13" left="470" textpieces="0" top="410" width="27">1000</text>
<text font="7" height="13" left="470" textpieces="0" top="380" width="27">2000</text>
<text font="7" height="13" left="470" textpieces="0" top="350" width="27">3000</text>
<text font="7" height="13" left="506" textpieces="3" top="458" width="183">0       100      200      300</text>
<text font="10" height="15" left="464" textpieces="0" top="403" width="0">cost </text>
<text font="10" height="15" left="564" textpieces="0" top="477" width="64">time (sec) </text>
<text font="12" height="20" left="566" textpieces="0" top="350" width="26">RC </text>
<text font="7" height="13" left="605" textpieces="0" top="390" width="31">Tuffy </text>
<text font="7" height="13" left="615" textpieces="0" top="358" width="42">Tuffy-p </text>
<text font="5" height="12" left="522" textpieces="0" top="429" width="167">Alchemy grounding took over 1 hr. </text>
<text font="4" height="17" left="158" textpieces="0" top="518" width="601">Figure 5: Time-cost plots of Tuffy vs Tuffy-p (i.e., Tuffy without partitioning)</text>
<text font="4" height="15" left="133" textpieces="0" top="559" width="677">As shown in Table 5, when there are multiple components in the MRF, partitioning allows</text>
<text font="4" height="14" left="108" textpieces="0" top="582" width="702">Tuffy to use less memory than Tuffy-p. (The IE dataset is too small to yield notable di&#64256;erences).</text>
<text font="4" height="17" left="108" textpieces="0" top="599" width="702">We see that Tuffy&#8217;s component-wise inference produces signi&#64257;cantly better results than Tuffy-</text>
<text font="4" height="15" left="108" textpieces="0" top="620" width="702">p. We then extend the run time of all systems. As shown in Figure 5, there continues to be a</text>
<text font="4" height="17" left="108" textpieces="0" top="640" width="702">gap between Tuffy&#8217;s component-wise search approach and the original WalkSAT running on the</text>
<text font="4" height="15" left="108" textpieces="0" top="660" width="702">whole MRF. This gap is predicted by our theoretical analysis in &#167;3.3. Thus, we have veri&#64257;ed that</text>
<text font="4" height="17" left="108" textpieces="0" top="681" width="685">partitioning makes Tuffy substantially more e&#64259;cient in terms of both space and search speed.</text>
<text font="4" height="17" left="133" textpieces="0" top="701" width="677">We also validate that Tuffy&#8217;s loading and parallelism makes a substantial di&#64256;erence: without</text>
<text font="4" height="17" left="108" textpieces="1" top="721" width="702">our batch loading technique, Tuffy takes 448s to perform 106 search steps per component on RC,</text>
<text font="4" height="15" left="108" textpieces="0" top="742" width="702">while 117s to perform the same operation with batch loading. With the addition of 8 threads (on</text>
<text font="4" height="15" left="108" textpieces="0" top="762" width="702">8 cores), we further reduce the runtime to 28s. Additional loading and parallelism experiments</text>
<text font="4" height="15" left="108" textpieces="0" top="782" width="702">in &#167;C.3 support our claim that our loading algorithm and partitioning algorithm contribute to</text>
<text font="4" height="15" left="108" textpieces="0" top="803" width="200">improving processing speed.</text>
<text font="1" height="16" left="108" textpieces="1" top="845" width="307">4.5  E&#64256;ect of Further Partitioning</text>
<text font="4" height="15" left="108" textpieces="0" top="877" width="702">To validate our claim that splitting MRF components can further improve both space e&#64259;ciency and</text>
<text font="4" height="17" left="108" textpieces="0" top="898" width="702">sometimes also search quality (&#167;3.4), we run Tuffy on RC, ER, and LP with di&#64256;erent memory</text>
<text font="4" height="15" left="108" textpieces="0" top="918" width="702">budgets &#8211; which are fed to the partitioning algorithm as the bound of partition size. On each</text>
<text font="4" height="17" left="108" textpieces="0" top="938" width="702">dataset, we give Tuffy three memory budgets, with the largest one corresponding to the case</text>
<text font="4" height="15" left="108" textpieces="0" top="959" width="702">when no components are split; note that according to the partitioning algorithm, the memory</text>
<text font="4" height="15" left="108" textpieces="0" top="979" width="702">budget is inversely correlated to partitioning granularity. Figure 6 shows the experiment results.</text>
<text font="4" height="15" left="108" textpieces="0" top="999" width="702">On RC, we see another improvement of the result quality (cf. Figure 5). Similar to Example 2, we</text>
<text font="4" height="15" left="108" textpieces="0" top="1020" width="702">believe the reason to be graph sparsity: &#8220;13MB&#8221; cuts only about 420 out of the total 10K clauses.</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">14</text>
<text font="2" height="15" left="181" textpieces="0" top="232" width="30">0E+0</text>
<text font="2" height="15" left="181" textpieces="0" top="191" width="30">1E+3</text>
<text font="2" height="15" left="181" textpieces="0" top="149" width="29">2E+3</text>
<text font="2" height="15" left="181" textpieces="0" top="108" width="29">3E+3</text>
<text font="2" height="15" left="220" textpieces="3" top="251" width="132">0    100  200  300</text>
<text font="11" height="17" left="174" textpieces="0" top="178" width="0">cost </text>
<text font="11" height="17" left="248" textpieces="0" top="272" width="72">time (sec) </text>
<text font="11" height="17" left="277" textpieces="0" top="109" width="22">RC </text>
<text font="4" height="17" left="311" textpieces="0" top="126" width="40">15MB</text>
<text font="4" height="17" left="311" textpieces="0" top="146" width="40">13MB</text>
<text font="4" height="17" left="311" textpieces="0" top="166" width="40">12MB</text>
<text font="2" height="15" left="375" textpieces="0" top="232" width="41">2.4E+3</text>
<text font="2" height="15" left="375" textpieces="0" top="191" width="41">2.6E+3</text>
<text font="2" height="15" left="375" textpieces="0" top="149" width="41">2.8E+3</text>
<text font="2" height="15" left="375" textpieces="0" top="108" width="41">3.0E+3</text>
<text font="2" height="15" left="425" textpieces="2" top="251" width="124">0    50  100 150</text>
<text font="11" height="17" left="367" textpieces="0" top="178" width="0">cost </text>
<text font="11" height="17" left="449" textpieces="0" top="272" width="72">time (sec) </text>
<text font="13" height="19" left="467" textpieces="0" top="111" width="22">LP </text>
<text font="4" height="17" left="506" textpieces="0" top="137" width="32">9MB</text>
<text font="4" height="17" left="506" textpieces="0" top="156" width="32">5MB</text>
<text font="4" height="17" left="506" textpieces="0" top="176" width="44">3.5MB</text>
<text font="2" height="15" left="575" textpieces="0" top="232" width="30">0E+0</text>
<text font="2" height="15" left="575" textpieces="0" top="201" width="29">2E+4</text>
<text font="2" height="15" left="575" textpieces="0" top="170" width="29">4E+4</text>
<text font="2" height="15" left="575" textpieces="0" top="139" width="30">6E+4</text>
<text font="2" height="15" left="575" textpieces="0" top="108" width="29">8E+4</text>
<text font="2" height="15" left="614" textpieces="1" top="251" width="132">0   500 1000 1500</text>
<text font="11" height="17" left="567" textpieces="0" top="178" width="0">cost </text>
<text font="11" height="17" left="640" textpieces="0" top="272" width="72">time (sec) </text>
<text font="13" height="19" left="666" textpieces="0" top="111" width="24">ER </text>
<text font="4" height="17" left="701" textpieces="0" top="134" width="48">200MB</text>
<text font="4" height="17" left="701" textpieces="0" top="155" width="48">100MB</text>
<text font="4" height="17" left="701" textpieces="0" top="176" width="40">50MB</text>
<text font="4" height="17" left="217" textpieces="0" top="288" width="485">Figure 6: Time-cost plots of Tuffy with di&#64256;erent memory budgets</text>
<text font="4" height="15" left="108" textpieces="0" top="341" width="702">In contrast, while MRF partitioning lowers RAM usage considerably on ER, it also leads to slower</text>
<text font="4" height="15" left="108" textpieces="0" top="361" width="702">convergence of result quality &#8211; which correlates with poor partitioning quality: the MRF of ER</text>
<text font="4" height="15" left="108" textpieces="0" top="381" width="702">is quite dense and even 2-way partitioning (&#8220;100MB&#8221;) would cut over 1.4M out of the total 2M</text>
<text font="4" height="15" left="108" textpieces="0" top="402" width="702">clauses. The dataset LP illustrates the interesting tradeo&#64256; where a coarse partition is bene&#64257;cial</text>
<text font="4" height="15" left="108" textpieces="0" top="422" width="620">whereas &#64257;ner grained partitions would be detrimental. We discuss this tradeo&#64256; in &#167;??.</text>
<text font="3" height="19" left="108" textpieces="1" top="470" width="361">5  Tradeo&#64256;s in MRF Partitioning</text>
<text font="4" height="15" left="108" textpieces="0" top="510" width="702">Experiments in &#167;4 show that, even when many clauses are cut due to partitioning, the search results</text>
<text font="4" height="15" left="108" textpieces="0" top="530" width="702">can still be signi&#64257;cantly better than when no components are split (especially on the RC dataset).</text>
<text font="4" height="15" left="108" textpieces="0" top="551" width="702">This is true even when a component is split into a relatively small number of pieces. To explain</text>
<text font="4" height="15" left="108" textpieces="0" top="571" width="702">this phenomenon, note that the bound in Theorem 3.1 is rather conservative &#8211; speed-up of search</text>
<text font="4" height="15" left="108" textpieces="0" top="591" width="374">is more dramatic than as predicted by Theorem 3.1.</text>
<text font="4" height="15" left="108" textpieces="1" top="629" width="702">Example 1 Consider an MRF G with two identical components G1, G2, and let p be the stationary</text>
<text font="4" height="15" left="108" textpieces="0" top="650" width="701">probability of the optima on each Gi. Then, after entering a communication class containing some</text>
<text font="4" height="15" left="108" textpieces="0" top="670" width="702">optimum, component-wise WalkSAT would need an expected total of 2/p steps to reach the optima;</text>
<text font="4" height="15" left="108" textpieces="2" top="690" width="702">whereas running WalkSAT on G as a whole would require roughly6 1/p2 steps. When p = 10&#8722;4,</text>
<text font="4" height="15" left="108" textpieces="1" top="711" width="701">the di&#64256;erence is between 2 &#8727; 104 and 108. In contrast, by focusing on the optimal states and their</text>
<text font="4" height="15" left="108" textpieces="0" top="731" width="456">neighbors, Theorem 3.1 only predicts a gap of no more than 22.</text>
<text font="4" height="15" left="133" textpieces="0" top="769" width="677">The implication of this observation is that, even when the largest component in the MRF is</text>
<text font="4" height="15" left="108" textpieces="0" top="790" width="702">smaller than the RAM, we may still want to partition the components to even smaller pieces. This</text>
<text font="4" height="15" left="108" textpieces="0" top="810" width="702">raises an interesting question: How do we decide the optimal partitioning granularity? Furthermore,</text>
<text font="4" height="15" left="108" textpieces="0" top="830" width="702">given two partitioning schemes, how do we decide which one is better, i.e., produces better search</text>
<text font="4" height="15" left="108" textpieces="0" top="851" width="702">results? To answer those questions, we have to characterize the e&#64256;ect of partitioning granularity</text>
<text font="4" height="15" left="108" textpieces="0" top="871" width="235">and cut size on inference quality.</text>
<text font="4" height="15" left="133" textpieces="0" top="891" width="677">Let us introduce a few de&#64257;nitions. An atom is called iTrue (resp. iFalse) if it appears in a</text>
<text font="4" height="15" left="108" textpieces="0" top="912" width="702">positive (resp. negative) literal of a positive-weighted ground clause, or in a negative (resp. positive)</text>
<text font="4" height="15" left="108" textpieces="0" top="932" width="702">literal of a negative-weighted ground clause. If an atom is both iTrue and iFalse, it is called critical.</text>
<text font="4" height="15" left="108" textpieces="0" top="952" width="702">Intuitively, it is the critical atoms that are responsible for making WalkSAT &#8220;oscillate&#8221; around the</text>
<text font="4" height="15" left="108" textpieces="0" top="973" width="702">optima and thereby diminishing p, which in turn widens the gap in the above example. Hence,</text>
<text font="4" height="15" left="108" textpieces="0" top="993" width="702">we use the number of critical atoms to estimate how hard it is for WalkSAT to &#64257;nd an optimal</text>
<text font="6" height="8" left="127" textpieces="0" top="1022" width="659">6That is assuming that the number of violated clauses in optimal states in both components are comparable.</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">15</text>
<text font="4" height="15" left="108" textpieces="1" top="113" width="702">solution. Since we expect WalkSAT&#8217;s hitting time to be superlinear7 in the number of critical</text>
<text font="4" height="15" left="108" textpieces="0" top="133" width="702">atoms, we prefer &#64257;ner partitioning granularity. However, &#64257;ne granularity also implies larger cut</text>
<text font="4" height="15" left="108" textpieces="0" top="154" width="628">size, which would slow down the convergence rate of the Gauss-Seidel inference scheme.</text>
<text font="14" height="6" left="265" textpieces="0" top="397" width="4">0</text>
<text font="14" height="6" left="295" textpieces="0" top="388" width="9">0.2</text>
<text font="14" height="6" left="324" textpieces="0" top="379" width="9">0.4</text>
<text font="14" height="6" left="355" textpieces="0" top="371" width="9">0.6</text>
<text font="14" height="6" left="385" textpieces="0" top="363" width="9">0.8</text>
<text font="14" height="6" left="414" textpieces="0" top="354" width="4">1</text>
<text font="14" height="6" left="251" textpieces="0" top="396" width="4">0</text>
<text font="14" height="6" left="223" textpieces="0" top="384" width="9">0.1</text>
<text font="14" height="6" left="200" textpieces="0" top="373" width="9">0.2</text>
<text font="14" height="6" left="177" textpieces="0" top="362" width="9">0.3</text>
<text font="14" height="6" left="155" textpieces="0" top="351" width="9">0.4</text>
<text font="14" height="6" left="132" textpieces="0" top="339" width="9">0.5</text>
<text font="14" height="6" left="138" textpieces="0" top="332" width="4">0</text>
<text font="14" height="6" left="131" textpieces="0" top="307" width="11">500</text>
<text font="14" height="6" left="128" textpieces="0" top="283" width="14">1000</text>
<text font="14" height="6" left="128" textpieces="0" top="259" width="14">1500</text>
<text font="14" height="6" left="128" textpieces="0" top="235" width="14">2000</text>
<text font="14" height="6" left="409" textpieces="0" top="196" width="2"> </text>
<text font="15" height="8" left="326" textpieces="0" top="400" width="77">&#945;: Fraction of critical atoms</text>
<text font="14" height="6" left="326" textpieces="0" top="409" width="61">in the largest partition</text>
<text font="15" height="8" left="146" textpieces="0" top="396" width="61">&#946;: Fraction of clauses</text>
<text font="14" height="6" left="178" textpieces="0" top="405" width="26">in the cut</text>
<text font="14" height="6" left="146" textpieces="0" top="391" width="2"> </text>
<text font="14" height="6" left="123" textpieces="0" top="301" width="0">Lowest cost</text>
<text font="14" height="6" left="434" textpieces="0" top="382" width="11">400</text>
<text font="14" height="6" left="434" textpieces="0" top="357" width="11">600</text>
<text font="14" height="6" left="434" textpieces="0" top="333" width="11">800</text>
<text font="14" height="6" left="434" textpieces="0" top="309" width="14">1000</text>
<text font="14" height="6" left="434" textpieces="0" top="284" width="14">1200</text>
<text font="14" height="6" left="434" textpieces="0" top="260" width="14">1400</text>
<text font="14" height="6" left="434" textpieces="0" top="235" width="14">1600</text>
<text font="14" height="6" left="434" textpieces="0" top="210" width="14">1800</text>
<text font="7" height="12" left="237" textpieces="0" top="428" width="103">(a) Overall trend</text>
<text font="14" height="6" left="812" textpieces="0" top="347" width="4">0</text>
<text font="14" height="6" left="787" textpieces="0" top="358" width="9">0.2</text>
<text font="14" height="6" left="762" textpieces="0" top="370" width="9">0.4</text>
<text font="14" height="6" left="737" textpieces="0" top="381" width="9">0.6</text>
<text font="14" height="6" left="712" textpieces="0" top="393" width="9">0.8</text>
<text font="14" height="6" left="688" textpieces="0" top="404" width="4">1</text>
<text font="14" height="6" left="501" textpieces="0" top="363" width="4">0</text>
<text font="14" height="6" left="530" textpieces="0" top="372" width="9">0.1</text>
<text font="14" height="6" left="565" textpieces="0" top="380" width="9">0.2</text>
<text font="14" height="6" left="599" textpieces="0" top="388" width="9">0.3</text>
<text font="14" height="6" left="633" textpieces="0" top="397" width="9">0.4</text>
<text font="14" height="6" left="667" textpieces="0" top="405" width="9">0.5</text>
<text font="14" height="6" left="501" textpieces="0" top="356" width="4">0</text>
<text font="14" height="6" left="494" textpieces="0" top="331" width="11">500</text>
<text font="14" height="6" left="490" textpieces="0" top="307" width="14">1000</text>
<text font="14" height="6" left="490" textpieces="0" top="283" width="14">1500</text>
<text font="14" height="6" left="490" textpieces="0" top="259" width="14">2000</text>
<text font="14" height="6" left="806" textpieces="0" top="204" width="2"> </text>
<text font="15" height="8" left="734" textpieces="0" top="390" width="77">&#945;: Fraction of critical atoms</text>
<text font="14" height="6" left="734" textpieces="0" top="399" width="61">in the largest partition</text>
<text font="15" height="8" left="535" textpieces="0" top="395" width="61">&#946;: Fraction of clauses</text>
<text font="14" height="6" left="567" textpieces="0" top="404" width="26">in the cut</text>
<text font="14" height="6" left="509" textpieces="0" top="400" width="2"> </text>
<text font="14" height="6" left="486" textpieces="0" top="324" width="0">Lowest cost</text>
<text font="7" height="12" left="592" textpieces="0" top="428" width="118">(b) Function &#64257;tting</text>
<text font="4" height="15" left="126" textpieces="0" top="461" width="666">Figure 7: E&#64256;ect of partitioning granularity and cut size on search quality on the RC dataset.</text>
<text font="14" height="6" left="228" textpieces="0" top="735" width="9">0.1</text>
<text font="14" height="6" left="249" textpieces="0" top="732" width="9">0.2</text>
<text font="14" height="6" left="269" textpieces="0" top="728" width="9">0.3</text>
<text font="14" height="6" left="289" textpieces="0" top="725" width="9">0.4</text>
<text font="14" height="6" left="310" textpieces="0" top="722" width="9">0.5</text>
<text font="14" height="6" left="330" textpieces="0" top="718" width="9">0.6</text>
<text font="14" height="6" left="351" textpieces="0" top="714" width="9">0.7</text>
<text font="14" height="6" left="371" textpieces="0" top="711" width="9">0.8</text>
<text font="14" height="6" left="392" textpieces="0" top="707" width="9">0.9</text>
<text font="14" height="6" left="412" textpieces="0" top="704" width="4">1</text>
<text font="14" height="6" left="214" textpieces="0" top="732" width="4">0</text>
<text font="14" height="6" left="189" textpieces="0" top="714" width="9">0.2</text>
<text font="14" height="6" left="170" textpieces="0" top="695" width="9">0.4</text>
<text font="14" height="6" left="150" textpieces="0" top="676" width="9">0.6</text>
<text font="14" height="6" left="131" textpieces="0" top="658" width="9">0.8</text>
<text font="14" height="6" left="127" textpieces="0" top="649" width="14">2480</text>
<text font="14" height="6" left="127" textpieces="0" top="634" width="14">2500</text>
<text font="14" height="6" left="127" textpieces="0" top="619" width="14">2520</text>
<text font="14" height="6" left="127" textpieces="0" top="604" width="14">2540</text>
<text font="14" height="6" left="127" textpieces="0" top="589" width="14">2560</text>
<text font="14" height="6" left="127" textpieces="0" top="574" width="14">2580</text>
<text font="14" height="6" left="127" textpieces="0" top="560" width="14">2600</text>
<text font="14" height="6" left="407" textpieces="0" top="533" width="2"> </text>
<text font="15" height="8" left="298" textpieces="0" top="739" width="76">&#945;: Fraction of critical atoms</text>
<text font="14" height="6" left="298" textpieces="0" top="748" width="61">in the largest partition</text>
<text font="15" height="8" left="120" textpieces="0" top="729" width="60">&#946;: Fraction of clauses</text>
<text font="14" height="6" left="151" textpieces="0" top="738" width="26">in the cut</text>
<text font="14" height="6" left="145" textpieces="0" top="728" width="2"> </text>
<text font="14" height="6" left="122" textpieces="0" top="624" width="0">Lowest cost</text>
<text font="14" height="6" left="432" textpieces="0" top="705" width="26">2534.587</text>
<text font="14" height="6" left="432" textpieces="0" top="684" width="26">2534.588</text>
<text font="14" height="6" left="432" textpieces="0" top="661" width="26">2534.589</text>
<text font="14" height="6" left="432" textpieces="0" top="639" width="23">2534.59</text>
<text font="14" height="6" left="432" textpieces="0" top="617" width="26">2534.591</text>
<text font="14" height="6" left="432" textpieces="0" top="594" width="26">2534.592</text>
<text font="14" height="6" left="432" textpieces="0" top="573" width="26">2534.593</text>
<text font="14" height="6" left="432" textpieces="0" top="550" width="26">2534.594</text>
<text font="7" height="12" left="237" textpieces="0" top="766" width="103">(a) Overall trend</text>
<text font="14" height="6" left="636" textpieces="2" top="728" width="75">0              0.2            0.4</text>
<text font="14" height="6" left="737" textpieces="1" top="725" width="44">0.6             0.8</text>
<text font="14" height="6" left="809" textpieces="0" top="722" width="4">1</text>
<text font="14" height="6" left="636" textpieces="1" top="728" width="-24">0                  0.2</text>
<text font="14" height="6" left="573" textpieces="0" top="724" width="9">0.4</text>
<text font="14" height="6" left="542" textpieces="1" top="721" width="-22">0.6                    0.8</text>
<text font="14" height="6" left="496" textpieces="0" top="708" width="14">2480</text>
<text font="14" height="6" left="496" textpieces="0" top="680" width="14">2500</text>
<text font="14" height="6" left="496" textpieces="0" top="651" width="14">2520</text>
<text font="14" height="6" left="496" textpieces="0" top="622" width="14">2540</text>
<text font="14" height="6" left="496" textpieces="0" top="594" width="14">2560</text>
<text font="14" height="6" left="496" textpieces="0" top="566" width="14">2580</text>
<text font="14" height="6" left="496" textpieces="0" top="537" width="14">2600</text>
<text font="14" height="6" left="810" textpieces="0" top="531" width="2"> </text>
<text font="15" height="8" left="678" textpieces="0" top="738" width="77">&#945;: Fraction of critical atoms</text>
<text font="14" height="6" left="685" textpieces="0" top="747" width="61">in the largest partition</text>
<text font="15" height="8" left="548" textpieces="0" top="737" width="60">&#946;: Fraction of clauses</text>
<text font="14" height="6" left="564" textpieces="0" top="746" width="26">in the cut</text>
<text font="14" height="6" left="515" textpieces="0" top="720" width="2"> </text>
<text font="14" height="6" left="491" textpieces="0" top="638" width="0">Lowest cost</text>
<text font="7" height="12" left="592" textpieces="0" top="766" width="118">(b) Function &#64257;tting</text>
<text font="4" height="15" left="127" textpieces="0" top="800" width="664">Figure 8: E&#64256;ect of partitioning granularity and cut size on search quality on the LP dataset.</text>
<text font="4" height="15" left="133" textpieces="0" top="840" width="677">To study the tradeo&#64256; between critical atoms and cut size, we conduct the following experiment.</text>
<text font="4" height="15" left="108" textpieces="0" top="860" width="702">On each of the three usable datasets (i.e., RC, LP, and RC), we select the largest component in</text>
<text font="4" height="15" left="108" textpieces="0" top="880" width="702">the MRF, and partition it with various granularities: 2-way, 4-way, 8-way, and 16-way. For each</text>
<text font="4" height="15" left="108" textpieces="0" top="901" width="702">granularity, we generate several hundred random partitioning schemes, and measure two quantities</text>
<text font="4" height="15" left="108" textpieces="0" top="921" width="701">of each scheme: 1) &#945;, which is the fraction of critical atoms in the largest partition8, serving as an</text>
<text font="4" height="15" left="108" textpieces="0" top="941" width="702">estimate of per-partition WalkSAT search speed (lower &#945; values are imply faster search speed); and</text>
<text font="4" height="15" left="108" textpieces="0" top="962" width="702">2) &#946;, which is the fraction of clauses in the cut, serving as an estimate of the Gauss-Seidel scheme&#8217;s</text>
<text font="6" height="8" left="127" textpieces="0" top="990" width="528">7Note that naive exhaustive search has a run time exponential in the number of atoms.</text>
<text font="6" height="8" left="127" textpieces="0" top="1007" width="682">8As such, &#945; is at least 1/k for k-way partitioning. In the experiments, we ensure that &#945; &#8712; [1/2k, 1/k] for k-way</text>
<text font="7" height="12" left="108" textpieces="0" top="1026" width="569">partitioning, so that there are no overlaps of &#945; values from the four partitioning granularities.</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">16</text>
<text font="14" height="6" left="253" textpieces="0" top="334" width="4">0</text>
<text font="14" height="6" left="287" textpieces="0" top="326" width="9">0.2</text>
<text font="14" height="6" left="322" textpieces="0" top="319" width="9">0.4</text>
<text font="14" height="6" left="356" textpieces="0" top="313" width="9">0.6</text>
<text font="14" height="6" left="391" textpieces="0" top="306" width="9">0.8</text>
<text font="14" height="6" left="426" textpieces="0" top="299" width="4">1</text>
<text font="14" height="6" left="237" textpieces="0" top="332" width="4">0</text>
<text font="14" height="6" left="211" textpieces="0" top="320" width="9">0.2</text>
<text font="14" height="6" left="190" textpieces="0" top="309" width="9">0.4</text>
<text font="14" height="6" left="169" textpieces="0" top="297" width="9">0.6</text>
<text font="14" height="6" left="148" textpieces="0" top="286" width="9">0.8</text>
<text font="14" height="6" left="133" textpieces="0" top="275" width="4">1</text>
<text font="14" height="6" left="134" textpieces="0" top="266" width="4">1</text>
<text font="14" height="6" left="128" textpieces="0" top="247" width="9">1.5</text>
<text font="14" height="6" left="134" textpieces="0" top="228" width="4">2</text>
<text font="14" height="6" left="128" textpieces="0" top="209" width="9">2.5</text>
<text font="14" height="6" left="134" textpieces="0" top="190" width="4">3</text>
<text font="14" height="6" left="128" textpieces="0" top="171" width="9">3.5</text>
<text font="14" height="6" left="134" textpieces="0" top="152" width="4">4</text>
<text font="14" height="6" left="143" textpieces="0" top="137" width="15">x 104</text>
<text font="14" height="6" left="420" textpieces="0" top="121" width="2"> </text>
<text font="15" height="9" left="321" textpieces="0" top="339" width="81">&#945;: Fraction of critical atoms</text>
<text font="14" height="6" left="321" textpieces="0" top="349" width="64">in the largest partition</text>
<text font="15" height="9" left="132" textpieces="0" top="333" width="64">&#946;: Fraction of clauses</text>
<text font="14" height="6" left="165" textpieces="0" top="343" width="27">in the cut</text>
<text font="14" height="6" left="142" textpieces="0" top="327" width="2"> </text>
<text font="14" height="6" left="123" textpieces="0" top="228" width="0">Lowest cost</text>
<text font="14" height="6" left="446" textpieces="0" top="309" width="9">1.6</text>
<text font="14" height="6" left="446" textpieces="0" top="291" width="9">1.8</text>
<text font="14" height="6" left="446" textpieces="0" top="272" width="4">2</text>
<text font="14" height="6" left="446" textpieces="0" top="253" width="9">2.2</text>
<text font="14" height="6" left="446" textpieces="0" top="235" width="9">2.4</text>
<text font="14" height="6" left="446" textpieces="0" top="216" width="9">2.6</text>
<text font="14" height="6" left="446" textpieces="0" top="197" width="9">2.8</text>
<text font="14" height="6" left="446" textpieces="0" top="179" width="4">3</text>
<text font="14" height="6" left="446" textpieces="0" top="160" width="9">3.2</text>
<text font="14" height="6" left="446" textpieces="0" top="141" width="9">3.4</text>
<text font="14" height="6" left="446" textpieces="0" top="123" width="9">3.6</text>
<text font="14" height="6" left="444" textpieces="0" top="115" width="15">x 104</text>
<text font="7" height="12" left="237" textpieces="0" top="368" width="103">(a) Overall trend</text>
<text font="14" height="6" left="816" textpieces="0" top="302" width="4">0</text>
<text font="14" height="6" left="785" textpieces="0" top="312" width="9">0.2</text>
<text font="14" height="6" left="755" textpieces="0" top="321" width="9">0.4</text>
<text font="14" height="6" left="724" textpieces="0" top="330" width="9">0.6</text>
<text font="14" height="6" left="694" textpieces="0" top="339" width="9">0.8</text>
<text font="14" height="6" left="663" textpieces="0" top="348" width="4">1</text>
<text font="14" height="6" left="494" textpieces="0" top="302" width="4">0</text>
<text font="14" height="6" left="520" textpieces="0" top="312" width="9">0.2</text>
<text font="14" height="6" left="551" textpieces="0" top="321" width="9">0.4</text>
<text font="14" height="6" left="581" textpieces="0" top="330" width="9">0.6</text>
<text font="14" height="6" left="612" textpieces="0" top="339" width="9">0.8</text>
<text font="14" height="6" left="648" textpieces="0" top="348" width="4">1</text>
<text font="14" height="6" left="494" textpieces="0" top="295" width="4">1</text>
<text font="14" height="6" left="489" textpieces="0" top="277" width="9">1.5</text>
<text font="14" height="6" left="494" textpieces="0" top="258" width="4">2</text>
<text font="14" height="6" left="489" textpieces="0" top="239" width="9">2.5</text>
<text font="14" height="6" left="494" textpieces="0" top="221" width="4">3</text>
<text font="14" height="6" left="489" textpieces="0" top="202" width="9">3.5</text>
<text font="14" height="6" left="494" textpieces="0" top="184" width="4">4</text>
<text font="14" height="6" left="504" textpieces="0" top="168" width="15">x 104</text>
<text font="14" height="6" left="810" textpieces="0" top="141" width="2"> </text>
<text font="15" height="9" left="729" textpieces="0" top="337" width="79">&#945;: Fraction of critical atoms</text>
<text font="14" height="6" left="729" textpieces="0" top="346" width="63">in the largest partition</text>
<text font="15" height="9" left="521" textpieces="0" top="336" width="63">&#946;: Fraction of clauses</text>
<text font="14" height="6" left="554" textpieces="1" top="345" width="-49">in the cut                                        </text>
<text font="14" height="6" left="484" textpieces="0" top="257" width="0">Lowest cost</text>
<text font="7" height="12" left="592" textpieces="0" top="368" width="118">(b) Function &#64257;tting</text>
<text font="4" height="15" left="126" textpieces="0" top="402" width="666">Figure 9: E&#64256;ect of partitioning granularity and cut size on search quality on the ER dataset.</text>
<text font="4" height="15" left="108" textpieces="0" top="454" width="702">convergence speed (again, lower &#946; values imply faster convergence). On each partitioning scheme,</text>
<text font="4" height="15" left="108" textpieces="1" top="474" width="702">we run the Gauss-Seidel inference scheme with two rounds, and 105WalkSAT steps in each round.</text>
<text font="4" height="15" left="108" textpieces="0" top="495" width="702">Finally, we measure the lowest total cost from each partitioning scheme. We plot the results in</text>
<text font="4" height="15" left="108" textpieces="0" top="515" width="139">Figures 7, 8, and 9.</text>
<text font="4" height="15" left="133" textpieces="0" top="535" width="677">On the RC dataset, we observe that partition sizes are the deciding factor of result quality &#8211;</text>
<text font="4" height="15" left="108" textpieces="0" top="556" width="702">as &#945; decreases, result quality improves substantially (from roughly 1200 to roughly 500), despite</text>
<text font="4" height="15" left="108" textpieces="0" top="576" width="702">increases in the cut size (i.e., &#946;). As shown in Figure 7, the partitioning schemes in di&#64256;erent</text>
<text font="4" height="15" left="108" textpieces="0" top="596" width="702">granularity groups form distinct strata. In particular, although the &#64257;nest partitioning schemes cut</text>
<text font="4" height="15" left="108" textpieces="0" top="617" width="702">almost half of the clauses, they still yield the best result quality. Note that RC has a relatively</text>
<text font="4" height="15" left="108" textpieces="0" top="637" width="702">sparse MRF, and the normalized cut size &#946; remains relatively small. For datasets like this, one</text>
<text font="4" height="15" left="108" textpieces="0" top="657" width="635">should partition aggressively to both improve search quality and increase parallelization.</text>
<text font="4" height="15" left="133" textpieces="0" top="678" width="677">On the LP dataset, we see that, as partition sizes decrease, the cut size increases more rapidly</text>
<text font="4" height="15" left="108" textpieces="0" top="698" width="702">(compared to RC), but the result quality remains largely the same (almost always between 2500 and</text>
<text font="4" height="15" left="108" textpieces="0" top="718" width="702">2560). For datasets like this, one should partition aggressively to take advantage of parallelization.</text>
<text font="4" height="15" left="133" textpieces="0" top="739" width="677">On the ER dataset, as partition sizes decrease, the cut size increases even more rapidly (com-</text>
<text font="4" height="15" left="108" textpieces="1" top="759" width="702">pared to LP), and the average result quality deteriorates signi&#64257;cantly (from roughly 2.1 &#8727; 104 to</text>
<text font="4" height="15" left="108" textpieces="0" top="779" width="701">roughly 3.5 &#8727; 104). Note that on ER, even 4-way partitioning already cuts almost all of the clauses</text>
<text font="4" height="15" left="108" textpieces="0" top="800" width="701">(with &#946; &#8776; 0.8) with lowest costs around 2.4 &#8727; 204. For datasets like this, one should avoid parti-</text>
<text font="4" height="15" left="108" textpieces="0" top="820" width="393">tioning so long as the MRF component &#64257;ts in memory.</text>
<text font="4" height="15" left="133" textpieces="0" top="840" width="677">From those results, it is clear that both &#945; and &#946; play a crucial role in result quality. Given a</text>
<text font="4" height="15" left="108" textpieces="0" top="861" width="702">partitioning scheme P , we use the following simple formula to estimate how well P will perform in</text>
<text font="4" height="15" left="108" textpieces="0" top="881" width="168">terms of search quality:</text>
<text font="4" height="15" left="406" textpieces="2" top="901" width="103">EP = &#945;P+ &#946;P</text>
<text font="4" height="15" left="133" textpieces="0" top="931" width="677">Since smaller values of &#945; and &#946; are more desirable, one should favor partitionings with smaller</text>
<text font="4" height="15" left="108" textpieces="1" top="952" width="702">EP values. To empirically validate this heuristic, on each of the three &#64257;gures, we also plot the plane</text>
<text font="4" height="15" left="108" textpieces="4" top="972" width="702">z = c1EP + c2 = c1(&#945; + &#946;) + c2 (where ci are positive constants) that minimizes the least square</text>
<text font="4" height="15" left="108" textpieces="0" top="992" width="702">error to the data points. Part (b) of each &#64257;gure shows that this is a reasonable approximation of</text>
<text font="4" height="15" left="108" textpieces="3" top="1013" width="702">the general trend. Thus, we conclude that EP = &#945;P+ &#946;P is a reasonable heuristic to estimate the</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">17</text>
<text font="4" height="15" left="108" textpieces="0" top="113" width="306">&#8220;quality&#8221; of di&#64256;erent partitioning schemes.</text>
<text font="4" height="15" left="133" textpieces="0" top="133" width="677">To further validate and re&#64257;ne this simple heuristic, we plan to perform more extensive exper-</text>
<text font="4" height="15" left="108" textpieces="0" top="154" width="702">iments with more diverse datasets. It is also interesting to develop a theory on this tradeo&#64256; and</text>
<text font="4" height="15" left="108" textpieces="0" top="174" width="579">investigate how to design partitioning algorithms that make use of this heuristic.</text>
<text font="3" height="19" left="108" textpieces="1" top="222" width="343">6  Conclusion and Future Work</text>
<text font="4" height="17" left="108" textpieces="0" top="262" width="702">Motivated by a large set of data-rich applications, we study how to push MLN inference inside an</text>
<text font="4" height="17" left="108" textpieces="0" top="282" width="702">RDBMS. We &#64257;nd that the grounding phase of MLN inference performs many relational operations</text>
<text font="4" height="17" left="108" textpieces="0" top="303" width="702">and that these operations are a substantial bottleneck in state-of-the-art MLN implementations</text>
<text font="4" height="17" left="108" textpieces="0" top="323" width="702">such as Alchemy. By using an RDBMS, Tuffy not only achieves scalability, but also speeds up the</text>
<text font="4" height="15" left="108" textpieces="0" top="343" width="702">grounding phase by orders of magnitude. We then develop a hybrid solution with RDBMS-based</text>
<text font="4" height="17" left="108" textpieces="0" top="364" width="702">grounding and in-memory search. To further improve the space and time e&#64259;ciency of Tuffy,</text>
<text font="4" height="15" left="108" textpieces="0" top="384" width="702">we study a partitioning approach that allows for in-memory search even when the dataset does</text>
<text font="4" height="15" left="108" textpieces="0" top="404" width="702">not &#64257;t in memory. We identi&#64257;ed and quanti&#64257;ed a particular kind of speed-up theoretically and</text>
<text font="4" height="15" left="108" textpieces="0" top="425" width="702">experimentally; it allows us to produce higher quality results in a shorter amount of time and to</text>
<text font="4" height="15" left="108" textpieces="0" top="445" width="702">run on much larger datasets than were impossible with prior approaches. As future work, we plan</text>
<text font="4" height="15" left="108" textpieces="0" top="465" width="702">to further study the tradeo&#64256; of partitioning, and apply our main techniques (hybrid architecture</text>
<text font="4" height="15" left="108" textpieces="0" top="486" width="260">and partitioning) to other problems.</text>
<text font="3" height="19" left="108" textpieces="1" top="534" width="227">7  Acknowledgement</text>
<text font="4" height="15" left="108" textpieces="0" top="574" width="702">We gratefully acknowledge the support of Defense Advanced Research Projects Agency (DARPA)</text>
<text font="4" height="15" left="108" textpieces="0" top="594" width="702">Machine Reading Program under Air Force Research Laboratory (AFRL) prime contract no.</text>
<text font="4" height="15" left="108" textpieces="0" top="614" width="702">FA8750-09-C-0181. Any opinions, &#64257;ndings, and conclusion or recommendations expressed in this</text>
<text font="4" height="15" left="108" textpieces="0" top="635" width="702">material are those of the authors and do not necessarily re&#64258;ect the view of the DARPA, AFRL, or</text>
<text font="4" height="15" left="108" textpieces="0" top="655" width="142">the US government.</text>
<text font="3" height="19" left="108" textpieces="0" top="701" width="113">References</text>
<text font="2" height="13" left="115" textpieces="0" top="738" width="695">[1] L. Antova, T. Jansen, C. Koch, and D. Olteanu. Fast and simple relational processing of uncertain</text>
<text font="2" height="13" left="139" textpieces="0" top="756" width="140">data. In ICDE, 2008.</text>
<text font="2" height="13" left="115" textpieces="0" top="783" width="695">[2] O. Benjelloun, A. Sarma, A. Halevy, M. Theobald, and J. Widom. Databases with uncertainty and</text>
<text font="2" height="13" left="139" textpieces="0" top="801" width="159">lineage. VLDB J., 2008.</text>
<text font="2" height="13" left="115" textpieces="0" top="828" width="695">[3] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and distributed computation: Numerical methods. Prentice-</text>
<text font="2" height="13" left="139" textpieces="0" top="846" width="70">Hall, 1989.</text>
<text font="2" height="13" left="115" textpieces="0" top="873" width="657">[4] N. N. Dalvi and D. Suciu. E&#64259;cient query evaluation on probabilistic databases. In VLDB, 2004.</text>
<text font="2" height="13" left="115" textpieces="0" top="899" width="695">[5] A. Deshpande and S. Madden. MauveDB: Supporting model-based user views in database systems. In</text>
<text font="2" height="13" left="139" textpieces="0" top="917" width="105">SIGMOD, 2006.</text>
<text font="2" height="13" left="115" textpieces="0" top="944" width="640">[6] P. Domingos and D. Lowd. Markov Logic: An Interface Layer for Arti&#64257;cial Intelligence. 2009.</text>
<text font="2" height="14" left="115" textpieces="0" top="971" width="415">[7] P. Domingos et al. http://alchemy.cs.washington.edu/.</text>
<text font="2" height="13" left="115" textpieces="0" top="998" width="613">[8] R. Fagin, J. Y. Halpern, and N. Megiddo. A logic for reasoning about probabilities. 1990.</text>
<text font="2" height="13" left="115" textpieces="0" top="1025" width="565">[9] W. Feller. An introduction to probability theory and its applications. Vol. I. 1950.</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">18</text>
<text font="2" height="13" left="108" textpieces="0" top="114" width="634">[10] S. Guha and K. Munagala. Multi-armed bandits with metric switching costs. ICALP, 2009.</text>
<text font="2" height="13" left="108" textpieces="0" top="141" width="702">[11] R. Jampani, F. Xu, M. Wu, L. L. Perez, C. M. Jermaine, and P. J. Haas. MCDB: a monte carlo</text>
<text font="2" height="13" left="139" textpieces="0" top="159" width="381">approach to managing uncertain data. In SIGMOD, 2008.</text>
<text font="2" height="13" left="108" textpieces="0" top="186" width="702">[12] B. Kanagal and A. Deshpande. Online &#64257;ltering, smoothing and probabilistic modeling of streaming</text>
<text font="2" height="13" left="139" textpieces="0" top="204" width="140">data. In ICDE, 2008.</text>
<text font="2" height="13" left="108" textpieces="0" top="231" width="702">[13] G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. Multilevel hypergraph partitioning: Applications</text>
<text font="2" height="13" left="139" textpieces="0" top="249" width="407">in VLSI domain. VLSI Systems, IEEE Transactions on, 2002.</text>
<text font="2" height="13" left="108" textpieces="0" top="275" width="702">[14] H. Kautz, B. Selman, and Y. Jiang. A general stochastic approach to solving problems with hard and</text>
<text font="2" height="13" left="139" textpieces="0" top="293" width="494">soft constraints. The Satis&#64257;ability Problem: Theory and Applications, 1997.</text>
<text font="2" height="13" left="108" textpieces="0" top="320" width="702">[15] S. Khot. Ruling out PTAS for graph min-bisection, densest subgraph and bipartite clique. In FOCS,</text>
<text font="2" height="13" left="139" textpieces="0" top="338" width="34">2004.</text>
<text font="2" height="13" left="108" textpieces="0" top="365" width="702">[16] A. McCallum, K. Nigam, J. Rennie, and K. Seymore. Automating the construction of internet portals</text>
<text font="2" height="13" left="139" textpieces="0" top="383" width="395">with machine learning. Information Retrieval Journal, 2000.</text>
<text font="2" height="13" left="108" textpieces="0" top="410" width="638">[17] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. 1988.</text>
<text font="2" height="13" left="108" textpieces="0" top="437" width="576">[18] H. Poon and P. Domingos. Joint inference in information extraction. In AAAI &#8217;07.</text>
<text font="2" height="13" left="108" textpieces="1" top="464" width="702">[19] C. R&#180; e, N. N. Dalvi, and D. Suciu. E&#64259;cient top-k query evaluation on probabilistic data. In ICDE, 2007.</text>
<text font="2" height="13" left="108" textpieces="1" top="491" width="702">[20] C. R&#180; e, J. Letchner, M. Balazinska, and D. Suciu. Event queries on correlated probabilistic streams. In</text>
<text font="2" height="13" left="139" textpieces="0" top="509" width="105">SIGMOD, 2008.</text>
<text font="2" height="13" left="108" textpieces="0" top="535" width="573">[21] M. Richardson and P. Domingos. Markov logic networks. Machine Learning, 2006.</text>
<text font="2" height="13" left="108" textpieces="0" top="562" width="660">[22] S. Riedel and I. Meza-Ruiz. Collective semantic role labeling with Markov logic. In CoNLL &#8217;08.</text>
<text font="2" height="13" left="108" textpieces="0" top="589" width="702">[23] P. Sen and A. Deshpande. Representing and querying correlated tuples in probabilistic databases. In</text>
<text font="2" height="13" left="139" textpieces="0" top="607" width="81">ICDE, 2007.</text>
<text font="2" height="13" left="108" textpieces="0" top="634" width="702">[24] P. Sen, A. Deshpande, and L. Getoor. PrDB: managing and exploiting rich correlations in probabilistic</text>
<text font="2" height="13" left="139" textpieces="0" top="652" width="177">databases. VLDB J., 2009.</text>
<text font="2" height="13" left="108" textpieces="0" top="679" width="702">[25] H. Simon and S. Teng. How good is recursive bisection? SIAM Journal on Scienti&#64257;c Computing, 1997.</text>
<text font="2" height="13" left="108" textpieces="0" top="706" width="547">[26] P. Singla and P. Domingos. Entity resolution with Markov logic. In ICDE &#8217;06.</text>
<text font="2" height="13" left="108" textpieces="0" top="733" width="702">[27] P. Singla, H. Kautz, J. Luo, and A. Gallagher. Discovery of social relationships in consumer photo</text>
<text font="2" height="13" left="139" textpieces="0" top="751" width="389">collections using Markov Logic. In CVPR Workshops 2008.</text>
<text font="2" height="13" left="108" textpieces="0" top="778" width="442">[28] V. Vazirani. Approximation algorithms. Springer Verlag, 2001.</text>
<text font="2" height="13" left="108" textpieces="0" top="804" width="702">[29] D. Z. Wang, E. Michelakis, M. N. Garofalakis, and J. M. Hellerstein. BayesStore: managing large,</text>
<text font="2" height="13" left="139" textpieces="0" top="822" width="516">uncertain data repositories with probabilistic graphical models. PVLDB, 2008.</text>
<text font="2" height="13" left="108" textpieces="0" top="849" width="688">[30] F. Wu and D. S. Weld. Automatically re&#64257;ning the Wikipedia infobox ontology. In WWW &#8217;08, 2008.</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">19</text>
<text font="3" height="19" left="108" textpieces="1" top="110" width="318">A  Material for Preliminaries</text>
<text font="1" height="16" left="108" textpieces="1" top="149" width="366">A.1  More Details on the MLN Program</text>
<text font="4" height="17" left="108" textpieces="0" top="179" width="701">Rules in MLNs are expressive and may involve data in non-trivial ways. For example, consider F2:</text>
<text font="2" height="12" left="271" textpieces="1" top="214" width="375">wrote(x, p1), wrote(x, p2), cat(p1, c) =&gt; cat(p2, c)  (F2)</text>
<text font="4" height="15" left="113" textpieces="0" top="247" width="697">Intuitively, this rule says that all the papers written by a particular person are likely to be in the</text>
<text font="4" height="15" left="108" textpieces="1" top="267" width="702">same category. Rules may also have existential quanti&#64257;ers: F4 says &#8220;any paper in our database</text>
<text font="4" height="15" left="108" textpieces="0" top="287" width="702">must have at least one author.&#8221; It is also a hard rule, which is indicated by the in&#64257;nite weight, and</text>
<text font="4" height="15" left="108" textpieces="0" top="308" width="702">so no possible world may violate this rule. The weight of a formula may also be negative, which</text>
<text font="4" height="15" left="108" textpieces="1" top="328" width="702">e&#64256;ectively means that the negation of the formula is likely to hold. For example, F5 models our</text>
<text font="4" height="17" left="108" textpieces="0" top="348" width="702">belief that none or very few of the unlabeled papers belong to &#8216;Networking&#8217;. Tuffy supports all</text>
<text font="4" height="15" left="108" textpieces="0" top="369" width="121">of these features.</text>
<text font="4" height="17" left="133" textpieces="0" top="389" width="677">If the input MLN contains hard rules (indicated by a weight of +&#8734; or &#8722;&#8734;), then we insist</text>
<text font="4" height="15" left="108" textpieces="0" top="409" width="702">that the set of possible worlds (Inst) only contain worlds that satisfy every hard rule with +&#8734; and</text>
<text font="4" height="15" left="108" textpieces="0" top="430" width="702">violate every rule with &#8722;&#8734;. In practice, schemata have type information, and we can use this to</text>
<text font="4" height="15" left="108" textpieces="0" top="450" width="702">remove nonsensical ground clauses, e.g., both attributes of refers are paper references, and so it</text>
<text font="4" height="15" left="108" textpieces="0" top="470" width="504">is unnecessary to ground this predicate with another type, say person.</text>
<text font="1" height="16" left="108" textpieces="1" top="513" width="250">A.2  Markov Random Field</text>
<text font="4" height="15" left="108" textpieces="0" top="545" width="702">A Boolean Markov Random Field (or Boolean Markov network is a model of the joint distribution</text>
<text font="4" height="15" left="108" textpieces="2" top="565" width="702">of a set of Boolean random variables &#175; X = (X1, . . . , XN). It is de&#64257;ned by a hypergraph G = (X, E);</text>
<text font="4" height="15" left="108" textpieces="0" top="586" width="701">for each hyperedge e &#8712; E there is a potential function (aka &#8220;feature&#8221;) denoted &#966;e, which is a</text>
<text font="4" height="15" left="108" textpieces="0" top="606" width="702">function from the values of the set of variables in e to non-negative real numbers. This de&#64257;nes a</text>
<text font="4" height="15" left="108" textpieces="2" top="626" width="282">joint distribution Pr( &#175; X = &#175; x) as follows:</text>
<text font="4" height="15" left="365" textpieces="2" top="670" width="93">Pr( &#175;  X = &#175; x) =</text>
<text font="4" height="15" left="467" textpieces="0" top="659" width="8">1</text>
<text font="4" height="15" left="464" textpieces="0" top="681" width="11">Z</text>
<text font="5" height="11" left="481" textpieces="0" top="693" width="24">e&#8712;E</text>
<text font="4" height="15" left="508" textpieces="1" top="670" width="44">&#966;e(&#175; xe)</text>
<text font="4" height="15" left="108" textpieces="4" top="726" width="692">where &#175; x &#8712; {0, 1}N, Z is a normalization constant and &#175; xe denotes the values of the variables in e.</text>
<text font="4" height="15" left="133" textpieces="0" top="746" width="258">Fix a set of constants C = {c1, . . . , c</text>
<text font="5" height="11" left="393" textpieces="1" top="751" width="417">M}. An MLN de&#64257;nes a Boolean Markov Random Field as</text>
<text font="4" height="15" left="108" textpieces="0" top="766" width="702">follows: for each possible grounding of each predicate (i.e., atom), create a node (and so a Boolean</text>
<text font="4" height="15" left="108" textpieces="0" top="787" width="702">random variable). For example, there will be a node refers(p1, p2) for each pair of papers p1, p2.</text>
<text font="4" height="15" left="108" textpieces="1" top="807" width="702">For each formula Fiwe ground it in all possible ways, then we create a hyperedge e that contains the</text>
<text font="4" height="15" left="108" textpieces="0" top="827" width="702">nodes corresponding to all terms in the formula. For example, the key constraint creates hyperedges</text>
<text font="4" height="15" left="108" textpieces="0" top="848" width="702">for each paper and all of its potential categories. We refer to this graph as the ground network.</text>
<text font="4" height="15" left="108" textpieces="0" top="868" width="598">Once we have the ground network, our task reduces to inference in Markov models.</text>
<text font="4" height="15" left="133" textpieces="0" top="888" width="677">Explicitly representing such ground networks is prohibitively expensive and unnecessary. In</text>
<text font="4" height="15" left="108" textpieces="0" top="908" width="702">practice, we only include non-evidence nodes and groundings that are relevant to answering the</text>
<text font="4" height="15" left="108" textpieces="0" top="929" width="122">query (see &#167;A.3).</text>
<text font="1" height="16" left="108" textpieces="1" top="972" width="378">A.3  Optimizing MLN Grounding Process</text>
<text font="4" height="17" left="108" textpieces="0" top="1003" width="702">Conceptually, we might ground an MLN formula by enumerating all possible assignments to its</text>
<text font="4" height="15" left="108" textpieces="0" top="1024" width="701">free variables. However, this is both impractical and unnecessary. For example, if we ground F2</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">20</text>
<text font="4" height="15" left="108" textpieces="1" top="113" width="702">exhaustively this way, the result would contain |D|4 ground clauses. Fortunately, in practice a</text>
<text font="4" height="15" left="108" textpieces="0" top="133" width="702">vast majority of ground clauses are satis&#64257;ed by evidence regardless of the assignments to unknown</text>
<text font="4" height="15" left="108" textpieces="1" top="154" width="611">truth values; we can safely discard such clauses [42]. Consider the ground clause g&#175;</text>
<text font="5" height="11" left="711" textpieces="0" top="161" width="7">d</text>
<text font="4" height="15" left="725" textpieces="1" top="154" width="85">of F2 where</text>
<text font="4" height="15" left="111" textpieces="1" top="170" width="637">&#175; d =(&#8216;Joe&#8217;, &#8216;P2&#8217;, &#8216;P3&#8217;, &#8216;DB&#8217;). Suppose that wrote(&#8216;Joe&#8217;, &#8216;P3&#8217;) is known to be false, then g</text>
<text font="5" height="11" left="750" textpieces="2" top="178" width="60">&#175; d will be</text>
<text font="4" height="15" left="108" textpieces="0" top="194" width="369">satis&#64257;ed no matter how the other atoms are set (g</text>
<text font="5" height="11" left="480" textpieces="4" top="199" width="330">&#175;  d is an implication). Hence, we can ignore g&#175; d</text>
<text font="4" height="15" left="108" textpieces="0" top="215" width="174">during the search phase.</text>
<text font="4" height="15" left="133" textpieces="0" top="235" width="677">Pushing this idea further, [41] proposes a method called &#8220;lazy inference&#8221; which is implemented</text>
<text font="4" height="17" left="108" textpieces="0" top="255" width="702">by Alchemy. Speci&#64257;cally, Alchemy works under the more aggressive hypothesis that most atoms</text>
<text font="4" height="15" left="108" textpieces="0" top="276" width="702">will be false in the &#64257;nal solution, and in fact throughout the entire execution. To make this idea</text>
<text font="4" height="15" left="108" textpieces="0" top="296" width="702">precise, call a ground clause active if it can be violated by &#64258;ipping zero or more active atoms,</text>
<text font="4" height="15" left="108" textpieces="1" top="316" width="702">where an atom is active if its value &#64258;ips at any point during execution.  Observe that in the</text>
<text font="4" height="15" left="108" textpieces="1" top="337" width="287">preceding example the ground clause g&#175;</text>
<text font="5" height="11" left="387" textpieces="1" top="344" width="423">d is not active. Alchemy keeps only active ground clauses</text>
<text font="4" height="15" left="108" textpieces="0" top="357" width="702">in memory, which can be much smaller than the full set ground clauses. Furthermore, as on-the-&#64258;y</text>
<text font="4" height="17" left="108" textpieces="0" top="377" width="702">incremental grounding is more expensive than batch grounding, Alchemy uses the following one-</text>
<text font="4" height="15" left="108" textpieces="0" top="398" width="702">step look-ahead strategy: assume all atoms are inactive and compute active clauses; activate the</text>
<text font="4" height="15" left="108" textpieces="0" top="418" width="702">atoms in the grounding result and recompute active clauses. This &#8220;look-ahead&#8221; procedure could</text>
<text font="4" height="17" left="108" textpieces="0" top="438" width="702">be repeatedly applied until convergence, resulting in an active closure. Tuffy implements this</text>
<text font="4" height="15" left="108" textpieces="0" top="459" width="128">closure algorithm.</text>
<text font="4" height="15" left="133" textpieces="0" top="479" width="677">In addition, we use a pruning strategy that ensures grounding to focus on only predicates and</text>
<text font="4" height="15" left="108" textpieces="0" top="499" width="702">rules that are relevant to the query. Similar ideas can be found in KBMC [46] inference and the</text>
<text font="4" height="15" left="108" textpieces="0" top="520" width="386">use of Rete algorithm [33] in production rule systems.</text>
<text font="1" height="16" left="108" textpieces="1" top="562" width="274">A.4  The WalkSAT Algorithm</text>
<text font="4" height="15" left="108" textpieces="0" top="594" width="503">For completeness, we list the pseudocode of WalkSAT in Algorithm 1.</text>
<text font="4" height="17" left="108" textpieces="0" top="631" width="299">Algorithm 1 The WalkSAT Algorithm</text>
<text font="4" height="15" left="108" textpieces="0" top="655" width="193">Input: A: an set of atoms</text>
<text font="4" height="15" left="108" textpieces="0" top="676" width="324">Input: C: an set of weighted ground clauses</text>
<text font="4" height="15" left="108" textpieces="0" top="696" width="204">Input: MaxFlips, MaxTries</text>
<text font="4" height="15" left="108" textpieces="1" top="716" width="276">Output: &#963;&#8727;: a truth assignment to A</text>
<text font="7" height="12" left="117" textpieces="1" top="739" width="134">1: lowCost &#8592; +&#8734;</text>
<text font="7" height="12" left="117" textpieces="1" top="759" width="223">2: for try = 1 to MaxTries do</text>
<text font="7" height="12" left="117" textpieces="1" top="779" width="304">3:    &#963; &#8592; a random truth assignment to A</text>
<text font="7" height="12" left="117" textpieces="1" top="800" width="247">4:    for flip = 1 to MaxFlips do</text>
<text font="7" height="12" left="117" textpieces="1" top="820" width="316">5:       pick a random c &#8712; C that is violated</text>
<text font="7" height="12" left="117" textpieces="1" top="840" width="366">6:       rand &#8592; a random &#64258;oat between 0.0 and 1.0</text>
<text font="7" height="12" left="117" textpieces="1" top="861" width="190">7:       if rand &#8804; 0.5 then</text>
<text font="7" height="12" left="117" textpieces="1" top="881" width="239">8:          &#64258;ip a random atom in c</text>
<text font="7" height="12" left="117" textpieces="1" top="901" width="82">9:       else</text>
<text font="7" height="12" left="110" textpieces="1" top="922" width="398">10:          &#64258;ip an atom in c s.t. the cost decreases most</text>
<text font="7" height="12" left="110" textpieces="1" top="942" width="236">11:       if cost &lt; lowCost then</text>
<text font="7" height="12" left="110" textpieces="2" top="962" width="256">12:          lowCost &#8592; cost, &#963;&#8727; &#8592; &#963;</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">21</text>
<text font="1" height="16" left="108" textpieces="1" top="112" width="305">A.5  Marginal Inference of MLNs</text>
<text font="4" height="15" left="108" textpieces="0" top="144" width="702">In marginal inference, we are given a set of atoms together with a truth assignment to them.</text>
<text font="4" height="15" left="108" textpieces="0" top="164" width="702">The goal is to estimate the marginal probability of this partial assignment. Since this problem</text>
<text font="4" height="15" left="108" textpieces="0" top="185" width="702">is generally intractable, we usually resort to sampling methods. The state-of-the-art marginal</text>
<text font="4" height="17" left="108" textpieces="0" top="205" width="702">inference algorithm of MLNs is MC-SAT [40], which is implemented in both Alchemy and Tuffy.</text>
<text font="4" height="15" left="108" textpieces="0" top="225" width="702">In MC-SAT, each sampling step consists of a call to a heuristic SAT sampler named SampleSAT [45].</text>
<text font="4" height="17" left="108" textpieces="0" top="246" width="702">Essentially, SampleSAT is a combination of simulated annealing and WalkSAT. And so, Tuffy is</text>
<text font="4" height="15" left="108" textpieces="0" top="266" width="420">able to perform marginal inference more e&#64259;ciently as well.</text>
<text font="3" height="19" left="108" textpieces="1" top="314" width="263">B  Material for Systems</text>
<text font="1" height="16" left="108" textpieces="1" top="354" width="404">B.1  An Example SQL Query For Grounding</text>
<text font="4" height="15" left="108" textpieces="1" top="385" width="702">Consider the formula F3 in Fig. 1. Suppose that the actual schemata of cat and refers are</text>
<text font="4" height="14" left="108" textpieces="0" top="407" width="702">cat(tid, paper, category, truth, state) and refers(tid, paper1, paper2, truth, state), respectively.</text>
<text font="4" height="15" left="108" textpieces="0" top="426" width="702">Given the evidence &#8211; as indicated by the &#8220;truth&#8221; attributes &#8211; we can ground the clauses for this</text>
<text font="4" height="15" left="108" textpieces="0" top="446" width="281">formula using the following SQL query.</text>
<text font="17" height="14" left="109" textpieces="1" top="477" width="317">S E L E C T - t1 . tid , - t2 . tid , t3 . tid</text>
<text font="17" height="14" left="109" textpieces="1" top="497" width="307">F R O M  cat t1 , r e f e r s t2 , cat t3</text>
<text font="17" height="14" left="109" textpieces="3" top="518" width="339">W H E R E ( t1 . t r u t h =N U L L OR t1 . t r u t h )</text>
<text font="17" height="14" left="109" textpieces="3" top="538" width="318">AND ( t2 . t r u t h =N U L L OR t2 . t r u t h )</text>
<text font="17" height="14" left="109" textpieces="3" top="558" width="359">AND ( t3 . t r u t h =N U L L OR NOT t3 . t r u t h )</text>
<text font="17" height="14" left="109" textpieces="1" top="579" width="224">AND t1 . p a p e r = t2 . p a p e r 1</text>
<text font="17" height="14" left="109" textpieces="1" top="599" width="275">AND t1 . c a t e g o r y = t3 . c a t e g o r y</text>
<text font="17" height="14" left="109" textpieces="1" top="619" width="224">AND t2 . p a p e r 2 = t3 . p a p e r</text>
<text font="4" height="15" left="133" textpieces="0" top="647" width="677">Note that the tids of t1 and t2 are negated because the corresponding predicates are negated</text>
<text font="4" height="15" left="108" textpieces="0" top="668" width="180">in the clausal form of F3.</text>
<text font="1" height="16" left="108" textpieces="1" top="710" width="410">B.2  A Compilation Algorithm for Grounding</text>
<text font="4" height="17" left="108" textpieces="0" top="742" width="702">Algorithm 2 is a basic algorithm of expressing the grounding process of an MLN formula in SQL.</text>
<text font="4" height="15" left="108" textpieces="0" top="763" width="702">To support existential quanti&#64257;ers, we used PostgreSQL&#8217;s array aggregate feature. The ideas in &#167;A.3</text>
<text font="4" height="15" left="108" textpieces="0" top="783" width="370">can be easily implemented on top of this algorithm.</text>
<text font="1" height="16" left="108" textpieces="1" top="826" width="368">B.3  Implementing WalkSAT in RDBMS</text>
<text font="4" height="15" left="108" textpieces="0" top="858" width="702">WalkSAT is a stochastic local search algorithm; its random access patterns pose considerable chal-</text>
<text font="4" height="17" left="108" textpieces="0" top="878" width="702">lenges to the design of Tuffy. More speci&#64257;cally, the following operations are di&#64259;cult to implement</text>
<text font="4" height="15" left="108" textpieces="0" top="898" width="702">e&#64259;ciently with on-disk data: 1) uniformly sample an unsatis&#64257;ed clause; 2) random access (read-</text>
<text font="4" height="15" left="108" textpieces="0" top="919" width="702">/write) to per-atom or per-clause data structures; and 3) traverse clauses involving a given atom.</text>
<text font="4" height="15" left="108" textpieces="0" top="939" width="702">Atoms are cached as in-memory arrays, while the per-clause data structures are read-only. Each</text>
<text font="4" height="15" left="108" textpieces="0" top="959" width="651">step of WalkSAT involves a scan over the clauses and many random accesses to the atoms.</text>
<text font="4" height="15" left="133" textpieces="0" top="980" width="677">Although our design process iterated over numerous combinations of various design choices, we</text>
<text font="4" height="15" left="108" textpieces="0" top="1000" width="702">were still unable to reduce the gap as reported in &#167;4.2. For example, compared to clause table</text>
<text font="4" height="15" left="108" textpieces="0" top="1020" width="702">scans, one might suspect that indexing could improve search speed by reading less data at each</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">22</text>
<text font="4" height="15" left="108" textpieces="0" top="112" width="283">Algorithm 2 MLN Grounding in SQL</text>
<text font="4" height="17" left="108" textpieces="0" top="139" width="238">Input: an MLN formula &#966; = &#8744;k</text>
<text font="5" height="11" left="339" textpieces="0" top="147" width="30">i=1li</text>
<text font="4" height="15" left="375" textpieces="1" top="139" width="418">where each li is a literal supported by predicate table r(li)</text>
<text font="4" height="15" left="108" textpieces="0" top="160" width="296">Output: a SQL query Q that grounds &#966;</text>
<text font="7" height="12" left="117" textpieces="2" top="182" width="402">1: FROM clause of Q includes &#8216;r(li) ti&#8217; for each literal li</text>
<text font="7" height="12" left="117" textpieces="1" top="202" width="409">2: SELECT clause of Q contains &#8216;ti.aid&#8217; for each literal li</text>
<text font="7" height="12" left="117" textpieces="2" top="223" width="693">3: For each positive (resp. negative) literal li, there is a WHERE predicate &#8216;ti.truth = true&#8217; (resp.</text>
<text font="4" height="15" left="136" textpieces="0" top="241" width="121">&#8216;ti.truth = false&#8217;)</text>
<text font="7" height="12" left="117" textpieces="1" top="263" width="693">4: For each variable x in &#966;, there is a WHERE predicate that equates the corresponding columns</text>
<text font="4" height="15" left="136" textpieces="1" top="282" width="188">of ti&#8217;s with li containing x</text>
<text font="7" height="12" left="117" textpieces="1" top="304" width="668">5: For each constant argument of li, there is an equal-constant WHERE predicate for table ti</text>
<text font="7" height="12" left="117" textpieces="1" top="324" width="417">6: Form a conjunction with the above WHERE predicates</text>
<text font="4" height="15" left="108" textpieces="0" top="379" width="702">step. However, we actually found that the cost of maintaining indices often outweighs the bene&#64257;t</text>
<text font="4" height="15" left="108" textpieces="0" top="399" width="702">provided by indexing. Moreover, we found it very di&#64259;cult to get around RDBMS overhead such</text>
<text font="4" height="15" left="108" textpieces="0" top="419" width="260">as PostgreSQL&#8217;s mandatory MVCC.</text>
<text font="1" height="16" left="108" textpieces="1" top="462" width="402">B.4  Illustrating Tu&#64256;y&#8217;s Hybrid Architecture</text>
<text font="4" height="17" left="108" textpieces="0" top="494" width="702">Figure 10 illustrates the hybrid memory management approach of Tuffy. Alchemy is a represen-</text>
<text font="4" height="17" left="108" textpieces="0" top="514" width="702">tative of prior art MLN systems, which uses RAM for both grounding and search; Tuffy-mm is</text>
<text font="4" height="17" left="108" textpieces="0" top="535" width="702">a version of Tuffy we developed that uses an RDBMS for all memory management; and Tuffy</text>
<text font="4" height="15" left="108" textpieces="0" top="555" width="310">is the hybrid approach as discussed in &#167;3.2.</text>
<text font="8" height="10" left="476" textpieces="1" top="657" width="-64">RDBMS                               RAM </text>
<text font="8" height="10" left="389" textpieces="2" top="720" width="207">RAM                 RDBMS                 RAM </text>
<text font="8" height="10" left="567" textpieces="1" top="657" width="-198">RDBMS                                                                            Grounding </text>
<text font="4" height="16" left="310" textpieces="0" top="718" width="47">Search </text>
<text font="11" height="16" left="373" textpieces="2" top="595" width="231">Alchemy    Tuffy-mm      Tuffy </text>
<text font="4" height="15" left="318" textpieces="0" top="776" width="281">Figure 10: Comparison of architectures</text>
<text font="1" height="16" left="108" textpieces="1" top="839" width="371">B.5  MLNs Causing MRF Fragmentation</text>
<text font="4" height="14" left="108" textpieces="0" top="874" width="702">MLN rules usually model the interaction of relationships and attributes of some underlying entities.</text>
<text font="4" height="15" left="108" textpieces="0" top="892" width="702">As such, one can de&#64257;ne entity-based transitive closures, which directly corresponds to components</text>
<text font="4" height="15" left="108" textpieces="0" top="912" width="702">in the MRF. Since in real world data the interactions are usually sparse, one can expect to see</text>
<text font="4" height="15" left="108" textpieces="0" top="932" width="702">multiple components in the MRF. A concrete example is the paper classi&#64257;cation running example,</text>
<text font="4" height="15" left="108" textpieces="0" top="953" width="702">where the primary entities are papers, and the interactions are de&#64257;ned by citations and common</text>
<text font="4" height="15" left="108" textpieces="0" top="973" width="651">authors. Indeed, our RC dataset yields hundreds of components in the MRF (see Table 5).</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">23</text>
<text font="1" height="16" left="108" textpieces="1" top="112" width="161">B.6  Theorem 3.1</text>
<text font="4" height="15" left="108" textpieces="0" top="144" width="702">of Theorem 3.1. We follow the notations of the theorem. Without loss of generality and for ease</text>
<text font="4" height="15" left="108" textpieces="1" top="164" width="702">of notation, suppose H = {1, . . . , N }. Denote by &#8486; the state space of G. Let Qk &#8838; &#8486; be the</text>
<text font="4" height="15" left="108" textpieces="1" top="185" width="702">set of states of G where there are exactly k non-optimal components.  For any state x &#8712; &#8486;,</text>
<text font="4" height="15" left="108" textpieces="1" top="205" width="702">de&#64257;ne H(x) = E[Hx(Q0)], i.e., the expected hitting time of an optimal state from x. De&#64257;ne</text>
<text font="4" height="15" left="108" textpieces="5" top="225" width="701">fk= minx&#8712;QkH(x); in particular, f0= 0. De&#64257;ne gk= fk+1&#8722; fk. For any x, y &#8712; &#8486;, let Pr(x &#8594; y)</text>
<text font="4" height="15" left="108" textpieces="0" top="246" width="702">be the transition probability of WalkSAT, i.e., the probability that next state will be y given current</text>
<text font="4" height="15" left="108" textpieces="0" top="266" width="702">state x. Note that Pr(x &#8594; y) &gt; 0 only if y &#8712; N (x), where N (x) is the set of states that di&#64256;er from</text>
<text font="4" height="15" left="108" textpieces="0" top="286" width="418">x by at most one bit. For any A &#8838; &#8486;, de&#64257;ne Pr(x &#8594; A) =</text>
<text font="5" height="11" left="547" textpieces="0" top="294" width="25">y&#8712;A</text>
<text font="4" height="15" left="576" textpieces="0" top="286" width="78">Pr(x &#8594; y).</text>
<text font="4" height="15" left="133" textpieces="0" top="307" width="176">For any x &#8712; Qk, we have</text>
<text font="4" height="15" left="294" textpieces="2" top="345" width="105">H(x)  =  1 +</text>
<text font="5" height="11" left="402" textpieces="0" top="368" width="24">y&#8712;&#8486;</text>
<text font="4" height="15" left="429" textpieces="0" top="345" width="110">Pr(x &#8594; y)H(y)</text>
<text font="4" height="15" left="346" textpieces="1" top="392" width="53">=  1 +</text>
<text font="5" height="11" left="402" textpieces="0" top="416" width="109">t&#8712;{&#8722;1,0,1} y&#8712;Qk+t</text>
<text font="4" height="15" left="514" textpieces="0" top="392" width="110">Pr(x &#8594; y)H(y)</text>
<text font="4" height="15" left="346" textpieces="0" top="439" width="52">&#8805; 1 +</text>
<text font="5" height="11" left="402" textpieces="0" top="464" width="109">t&#8712;{&#8722;1,0,1} y&#8712;Qk+t</text>
<text font="4" height="15" left="514" textpieces="0" top="440" width="108">Pr(x &#8594; y)fk+t.</text>
<text font="4" height="15" left="108" textpieces="0" top="495" width="45">De&#64257;ne</text>
<text font="4" height="15" left="300" textpieces="1" top="515" width="20">Px</text>
<text font="5" height="11" left="311" textpieces="0" top="522" width="10">+</text>
<text font="4" height="15" left="326" textpieces="1" top="515" width="161">= Pr(x &#8594; Qk+1),  Px</text>
<text font="5" height="11" left="479" textpieces="0" top="522" width="10">&#8722;</text>
<text font="4" height="15" left="494" textpieces="0" top="515" width="123">= Pr(x &#8594; Qk&#8722;1),</text>
<text font="4" height="15" left="108" textpieces="1" top="545" width="193">then Pr(x &#8594; Qk) = 1 &#8722; Px</text>
<text font="5" height="11" left="291" textpieces="0" top="552" width="10">+</text>
<text font="4" height="15" left="306" textpieces="1" top="544" width="36">&#8722; Px</text>
<text font="5" height="11" left="333" textpieces="0" top="552" width="46">&#8722;, and</text>
<text font="4" height="15" left="281" textpieces="1" top="582" width="157">H(x) &#8805; 1 + fk(1 &#8722; Px</text>
<text font="5" height="11" left="428" textpieces="0" top="589" width="10">+</text>
<text font="4" height="15" left="443" textpieces="1" top="581" width="36">&#8722; Px</text>
<text font="5" height="11" left="470" textpieces="0" top="589" width="67">&#8722;) + fk&#8722;1</text>
<text font="4" height="15" left="539" textpieces="0" top="582" width="18">Px</text>
<text font="5" height="11" left="549" textpieces="0" top="589" width="10">&#8722;</text>
<text font="4" height="15" left="563" textpieces="1" top="582" width="67">+ fk+1Px</text>
<text font="5" height="11" left="622" textpieces="0" top="589" width="15">+.</text>
<text font="4" height="15" left="133" textpieces="2" top="619" width="676">Since this inequality holds for any x &#8712; Qk, we can &#64257;x it to be some x&#8727; &#8712; Qk s.t. H(x&#8727;) = fk.</text>
<text font="4" height="15" left="108" textpieces="1" top="639" width="100">Then gk&#8722;1Px&#8727;</text>
<text font="5" height="11" left="193" textpieces="0" top="646" width="10">&#8722;</text>
<text font="4" height="15" left="214" textpieces="1" top="638" width="87">&#8805; 1 + gkPx&#8727;</text>
<text font="5" height="11" left="286" textpieces="0" top="646" width="10">+</text>
<text font="4" height="15" left="302" textpieces="2" top="639" width="207">, which implies gk&#8722;1&#8805; gkPx&#8727;</text>
<text font="5" height="11" left="494" textpieces="0" top="646" width="10">+</text>
<text font="4" height="15" left="511" textpieces="1" top="639" width="34">/Px&#8727;</text>
<text font="5" height="11" left="529" textpieces="0" top="646" width="10">&#8722;</text>
<text font="4" height="15" left="546" textpieces="0" top="639" width="5">.</text>
<text font="4" height="15" left="133" textpieces="1" top="660" width="413">Now without loss of generality assume that in x&#8727;, G1, . . . , G</text>
<text font="5" height="11" left="548" textpieces="1" top="665" width="262">kare non-optimal while Gk+1, . . . , GN</text>
<text font="4" height="15" left="108" textpieces="0" top="680" width="138">are optimal. Let x&#8727;</text>
<text font="5" height="11" left="240" textpieces="0" top="687" width="4">i</text>
<text font="4" height="15" left="253" textpieces="1" top="680" width="298">be the projection of x&#8727; on Gi. Then since</text>
<text font="4" height="15" left="269" textpieces="1" top="731" width="26">Px&#8727;</text>
<text font="5" height="11" left="279" textpieces="0" top="738" width="10">&#8722;</text>
<text font="4" height="15" left="300" textpieces="0" top="731" width="13">=</text>
<text font="5" height="11" left="337" textpieces="0" top="716" width="7">k</text>
<text font="5" height="11" left="337" textpieces="0" top="728" width="6">1</text>
<text font="4" height="15" left="347" textpieces="0" top="720" width="34">vi(x&#8727;</text>
<text font="5" height="11" left="376" textpieces="0" top="728" width="4">i</text>
<text font="4" height="15" left="383" textpieces="0" top="720" width="43">)&#945;i(x&#8727;</text>
<text font="5" height="11" left="421" textpieces="0" top="728" width="4">i</text>
<text font="4" height="15" left="428" textpieces="0" top="720" width="6">)</text>
<text font="5" height="11" left="357" textpieces="0" top="741" width="10">N</text>
<text font="5" height="11" left="357" textpieces="0" top="753" width="6">1</text>
<text font="4" height="15" left="372" textpieces="0" top="745" width="34">vi(x&#8727;</text>
<text font="5" height="11" left="400" textpieces="0" top="753" width="4">i</text>
<text font="4" height="15" left="408" textpieces="0" top="745" width="6">)</text>
<text font="4" height="15" left="436" textpieces="1" top="731" width="48">,  Px&#8727;</text>
<text font="5" height="11" left="470" textpieces="0" top="738" width="10">+</text>
<text font="4" height="15" left="491" textpieces="0" top="731" width="13">=</text>
<text font="5" height="11" left="528" textpieces="0" top="714" width="10">N</text>
<text font="5" height="11" left="528" textpieces="0" top="726" width="23">k+1</text>
<text font="4" height="15" left="554" textpieces="1" top="718" width="37">vj(x&#8727;</text>
<text font="5" height="11" left="584" textpieces="0" top="726" width="5">j</text>
<text font="4" height="15" left="592" textpieces="0" top="718" width="43">)&#946;j(x&#8727;</text>
<text font="5" height="11" left="629" textpieces="0" top="726" width="5">j</text>
<text font="4" height="15" left="637" textpieces="0" top="718" width="6">)</text>
<text font="5" height="11" left="557" textpieces="0" top="741" width="10">N</text>
<text font="5" height="11" left="557" textpieces="0" top="753" width="6">1</text>
<text font="4" height="15" left="572" textpieces="0" top="745" width="34">vi(x&#8727;</text>
<text font="5" height="11" left="600" textpieces="0" top="753" width="4">i</text>
<text font="4" height="15" left="607" textpieces="0" top="745" width="6">)</text>
<text font="4" height="15" left="645" textpieces="0" top="731" width="5">,</text>
<text font="4" height="15" left="108" textpieces="0" top="779" width="56">we have</text>
<text font="4" height="15" left="301" textpieces="1" top="810" width="69">gk&#8722;1&#8805; gk</text>
<text font="5" height="11" left="389" textpieces="0" top="792" width="10">N</text>
<text font="5" height="11" left="389" textpieces="0" top="805" width="23">k+1</text>
<text font="4" height="15" left="416" textpieces="1" top="797" width="37">vj(x&#8727;</text>
<text font="5" height="11" left="446" textpieces="0" top="804" width="5">j</text>
<text font="4" height="15" left="453" textpieces="1" top="797" width="45">)&#946;j(x&#8727;</text>
<text font="5" height="11" left="491" textpieces="0" top="804" width="5">j</text>
<text font="4" height="15" left="498" textpieces="0" top="797" width="6">)</text>
<text font="5" height="11" left="399" textpieces="0" top="819" width="7">k</text>
<text font="5" height="11" left="399" textpieces="0" top="832" width="6">1</text>
<text font="4" height="15" left="409" textpieces="0" top="824" width="34">vi(x&#8727;</text>
<text font="5" height="11" left="438" textpieces="0" top="832" width="4">i</text>
<text font="4" height="15" left="445" textpieces="0" top="824" width="43">)&#945;i(x&#8727;</text>
<text font="5" height="11" left="482" textpieces="0" top="832" width="4">i</text>
<text font="4" height="15" left="489" textpieces="0" top="824" width="6">)</text>
<text font="4" height="15" left="511" textpieces="0" top="809" width="32">&#8805; gk</text>
<text font="4" height="15" left="546" textpieces="0" top="799" width="65">r(N &#8722; k)</text>
<text font="4" height="15" left="573" textpieces="0" top="821" width="9">k</text>
<text font="4" height="15" left="612" textpieces="0" top="810" width="5">,</text>
<text font="4" height="15" left="108" textpieces="0" top="851" width="400">where the second inequality follows from de&#64257;nition of r.</text>
<text font="4" height="15" left="133" textpieces="4" top="872" width="633">For all k &#8804; rN/(r + 2), we have gk&#8722;1&#8805; 2gk. Since gk&#8805; 1 for any k, f1= g0&#8805; 2rN/(r+2).</text>
<text font="4" height="15" left="133" textpieces="0" top="906" width="676">According to this theorem, the gap on Example 1 is at least 2N/3; in fact, a more detailed analysis</text>
<text font="4" height="15" left="108" textpieces="0" top="926" width="222">reveals that the gap is at least</text>
<text font="5" height="11" left="344" textpieces="0" top="921" width="28">N &#8722;1</text>
<text font="6" height="8" left="353" textpieces="0" top="932" width="9">N</text>
<text font="6" height="8" left="355" textpieces="0" top="942" width="5">2</text>
<text font="4" height="15" left="385" textpieces="0" top="925" width="64">&#8776; &#920;(2N/</text>
<text font="4" height="15" left="451" textpieces="0" top="911" width="14">&#8730;</text>
<text font="4" height="15" left="464" textpieces="1" top="926" width="346">N ).9 Figure 11 shows the experiment results of</text>
<text font="4" height="17" left="108" textpieces="0" top="951" width="702">running Alchemy, Tuffy, and Tuffy-p (i.e., Tuffy without partitioning) on Example 1 with</text>
<text font="4" height="15" left="108" textpieces="0" top="971" width="127">1000 components.</text>
<text font="6" height="8" left="127" textpieces="2" top="1000" width="683">9Instead of applying the bound r directly, we can use the actual values of &#945;i, &#946;i, and vi  to solve the recurrence</text>
<text font="7" height="12" left="108" textpieces="0" top="1019" width="36">on gk.</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">24</text>
<text font="2" height="15" left="340" textpieces="0" top="232" width="22">500</text>
<text font="2" height="15" left="332" textpieces="0" top="193" width="30">1000</text>
<text font="2" height="15" left="332" textpieces="0" top="154" width="30">1500</text>
<text font="2" height="15" left="332" textpieces="0" top="115" width="30">2000</text>
<text font="2" height="15" left="372" textpieces="4" top="251" width="232">0       20      40      60      80</text>
<text font="18" height="21" left="324" textpieces="0" top="182" width="0">cost </text>
<text font="18" height="21" left="443" textpieces="0" top="273" width="90">time (sec) </text>
<text font="11" height="17" left="394" textpieces="0" top="125" width="188">Performance on Example 1 </text>
<text font="2" height="15" left="454" textpieces="0" top="164" width="97">Tuffy-p (dotted) </text>
<text font="2" height="15" left="454" textpieces="0" top="182" width="94">Alchemy (solid) </text>
<text font="2" height="15" left="454" textpieces="0" top="214" width="34">Tuffy </text>
<text font="4" height="15" left="292" textpieces="0" top="319" width="334">Figure 11: E&#64256;ect of partitioning on Example 1</text>
<text font="4" height="15" left="133" textpieces="0" top="371" width="677">Note that the analysis of Theorem 3.1 actually applies to not only WalkSAT, but stochastic</text>
<text font="4" height="15" left="108" textpieces="0" top="391" width="702">local search in general. Since stochastic local search algorithms are used in many statistical models,</text>
<text font="4" height="15" left="108" textpieces="0" top="411" width="702">we believe that our observation here and corresponding techniques have much wider implications</text>
<text font="4" height="17" left="108" textpieces="0" top="432" width="152">than MLN inference.</text>
<text font="1" height="16" left="108" textpieces="1" top="474" width="321">B.7  Hardness of MRF Partitioning</text>
<text font="4" height="15" left="108" textpieces="0" top="506" width="702">A bisection of a graph G = (V, E) with an even number of vertices is a pair of disjoint subsets</text>
<text font="4" height="15" left="108" textpieces="2" top="526" width="702">V1, V2&#8834; V of equal size. The cost of a bisection is the number of edges adjacent to both V1and V2.</text>
<text font="4" height="15" left="108" textpieces="0" top="547" width="702">The problem of Minimum Graph Bisection (MGB) is to &#64257;nd a bisection with minimum cost. This</text>
<text font="4" height="15" left="108" textpieces="0" top="567" width="702">problem admits no PTAS [15]. The hardness of MGB directly implies the hardness of partitioning</text>
<text font="4" height="15" left="108" textpieces="0" top="587" width="702">MRFs. As such, one may wonder if it still holds w.r.t. the domain size for a given MLN program</text>
<text font="4" height="15" left="108" textpieces="0" top="608" width="528">(hence of size O(1)). The following theorem shows that the answer is yes.</text>
<text font="4" height="15" left="108" textpieces="0" top="641" width="702">Theorem B.1. MGB can be reduced to the problem of &#64257;nding a minimum bisection of the MRF</text>
<text font="4" height="15" left="108" textpieces="0" top="661" width="229">generated an MLN of size O(1).</text>
<text font="4" height="15" left="108" textpieces="0" top="694" width="563">Proof. Consider the MLN that contains a single formula of the following form:</text>
<text font="4" height="15" left="388" textpieces="0" top="730" width="143">p(x), r(x, y) &#8594; p(y),</text>
<text font="4" height="15" left="108" textpieces="0" top="766" width="702">where p is query and r is evidence. For any graph G = (V, E), we can set the domain of the</text>
<text font="4" height="15" left="108" textpieces="0" top="787" width="702">predicates to be V , and let r = E. The MRF generated by the above MLN (using techniques in</text>
<text font="4" height="15" left="108" textpieces="0" top="806" width="702">&#167;A.3) is identical to G. Hence, if we could bisect the MRF in time polynomial in |V |, MGB would</text>
<text font="4" height="15" left="108" textpieces="0" top="827" width="59">be in P.</text>
<text font="1" height="16" left="108" textpieces="1" top="870" width="307">B.8  MRF Partitioning Algorithm</text>
<text font="4" height="15" left="108" textpieces="0" top="902" width="702">We provide a very simple MRF partitioning algorithm (Algorithm 3) that is inspired by Kruskal&#8217;s</text>
<text font="4" height="15" left="108" textpieces="0" top="922" width="702">minimum spanning tree algorithm. Its greediness on clause wights serving as a simple heuristic to</text>
<text font="4" height="15" left="108" textpieces="0" top="943" width="171">minimizing the cut size.</text>
<text font="4" height="15" left="133" textpieces="0" top="963" width="677">To explain the partitioning procedure, we provide the following de&#64257;nitions. Each clause c in the</text>
<text font="4" height="15" left="108" textpieces="1" top="983" width="689">MRF G = (V, E) is assigned to an atom in c. A partition of the MRF is a subgraph Gi= (Vi, E</text>
<text font="5" height="11" left="799" textpieces="0" top="989" width="10">i)</text>
<text font="4" height="15" left="108" textpieces="2" top="1003" width="702">de&#64257;ned by a subset of atoms Vi&#8838; V ; Ei is the set of clauses assigned to some atom in Vi. The size</text>
<text font="4" height="15" left="108" textpieces="1" top="1024" width="702">of Gi as referred to by Algorithm 3 can be any monotone function in Gi; in practice, it is de&#64257;ned</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">25</text>
<text font="4" height="15" left="108" textpieces="0" top="113" width="701">to be the total number of literals and atoms in Gi. Note that when the parameter &#946; is set to +&#8734;,</text>
<text font="4" height="15" left="108" textpieces="0" top="133" width="329">the output is the connected components of G.</text>
<text font="4" height="15" left="108" textpieces="0" top="170" width="386">Algorithm 3 A Simple MRF Partitioning Algorithm</text>
<text font="4" height="17" left="108" textpieces="0" top="195" width="430">Input: an MRF G = (V, E) with clause weights w : E &#8594; R</text>
<text font="4" height="15" left="108" textpieces="0" top="216" width="218">Input: partition size bound &#946;</text>
<text font="4" height="15" left="108" textpieces="0" top="236" width="560">Output: a partitioning of V s.t. the size of each partition is no larger than &#946;</text>
<text font="7" height="12" left="117" textpieces="1" top="258" width="338">1: initialize hypergraph H = (V, F ) with F = &#8709;</text>
<text font="7" height="12" left="117" textpieces="1" top="279" width="311">2: for all e &#8712; E in |w|-descending order do</text>
<text font="7" height="12" left="117" textpieces="1" top="299" width="469">3:    F &#8592; F &#8746; e if afterwards no component in H is larger than &#946;</text>
<text font="7" height="12" left="117" textpieces="1" top="319" width="421">4: return the collection of per-component atom sets in H</text>
<text font="4" height="15" left="133" textpieces="0" top="362" width="677">Our implementation of Algorithm 3 only uses RAM to maintain a union-&#64257;nd structure of the</text>
<text font="4" height="15" left="108" textpieces="0" top="383" width="702">nodes, and performs all other operations in the RDBMS. For example, we use SQL queries to</text>
<text font="4" height="15" left="108" textpieces="0" top="403" width="667">&#8220;assign&#8221; clauses to atoms, and to compute the partition of clauses from a partition of atoms.</text>
<text font="3" height="19" left="108" textpieces="1" top="451" width="311">C  Material for Experiments</text>
<text font="1" height="16" left="108" textpieces="1" top="491" width="320">C.1  Alternative Search Algorithms</text>
<text font="4" height="15" left="108" textpieces="0" top="523" width="702">As shown in &#167;4.3, RDBMS-based implementation of WalkSAT is several orders of magnitude slower</text>
<text font="4" height="15" left="108" textpieces="0" top="543" width="702">than the in-memory counter part. This gap is consistent with the I/O performance of disk vs. main</text>
<text font="4" height="15" left="108" textpieces="0" top="563" width="702">memory. One might imagine some clever caching schemes for WalkSAT, but even assuming that</text>
<text font="4" height="15" left="108" textpieces="0" top="584" width="702">a &#64258;ip incurs only one random I/O operation (which is usually on the order of 10 ms), the &#64258;ipping</text>
<text font="4" height="15" left="108" textpieces="0" top="604" width="702">rate of RDBMS-based search is still no more than 100 &#64258;ips/sec. Thus, it is highly unlikely that</text>
<text font="4" height="15" left="108" textpieces="0" top="624" width="592">disk-based search implementations could catch up to their in-memory counterpart.</text>
<text font="4" height="15" left="133" textpieces="0" top="645" width="677">We explored alternative search algorithms by designing a MaxSAT algorithm called SweepSAT</text>
<text font="4" height="15" left="108" textpieces="0" top="665" width="702">that is more I/O friendly than WalkSAT. Although experiments show that SweepSAT gives faster</text>
<text font="4" height="15" left="108" textpieces="0" top="685" width="702">search speed than WalkSAT when both are implemented in an RDBMS, we eventually decided that</text>
<text font="4" height="15" left="108" textpieces="0" top="706" width="702">search algorithmics is orthogonal to our investigation of how the hybrid architecture and the idea</text>
<text font="4" height="17" left="108" textpieces="0" top="726" width="331">of partitioning can bene&#64257;t the Tuffy system.</text>
<text font="1" height="16" left="108" textpieces="1" top="769" width="342">C.2  Lesion Study of Tu&#64256;y Grounding</text>
<text font="4" height="17" left="108" textpieces="0" top="801" width="702">To understand which part of the RDBMS contributes the most to Tuffy&#8217;s fast grounding speed,</text>
<text font="4" height="15" left="108" textpieces="0" top="821" width="702">we conduct a lesion study by comparing the grounding time in three settings: 1) full optimizer,</text>
<text font="4" height="15" left="108" textpieces="0" top="841" width="702">where the RDBMS is free to optimize SQL queries in all ways; 2) &#64257;xed join order, where we force</text>
<text font="4" height="17" left="108" textpieces="0" top="862" width="702">the RDBMS to use the same join order as Alchemy does; 3) &#64257;xed join algorithm, where we</text>
<text font="4" height="15" left="108" textpieces="0" top="882" width="702">force the RDBMS to use nested loop join only. The results are shown in Table 6. Clearly, being</text>
<text font="4" height="17" left="108" textpieces="0" top="902" width="567">able to use various join algorithms is the key to Tuffy&#8217;s fast grounding speed.</text>
<text font="1" height="16" left="108" textpieces="1" top="945" width="313">C.3  Data Loading and Parallelism</text>
<text font="4" height="15" left="108" textpieces="0" top="977" width="702">To validate the importance of batch data loading and parallelism (&#167;3.3), we run three versions of</text>
<text font="4" height="14" left="108" textpieces="0" top="1000" width="702">Tuffy on the IE and RC datasets: 1) Tu&#64256;y, which has batch loading but no parallelism; 2) Tu&#64256;y-</text>
<text font="4" height="15" left="108" textpieces="0" top="1018" width="709">batch, which loads components one by one and does not use parallelism; and 3) Tu&#64256;y+parallelism,</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">26</text>
<text font="2" height="13" left="434" textpieces="3" top="111" width="206">LP   IE       RC       ER</text>
<text font="2" height="13" left="299" textpieces="4" top="129" width="341">Full optimizer         6    13         40        106</text>
<text font="2" height="13" left="292" textpieces="4" top="148" width="348">Fixed join order        7    13         43        111</text>
<text font="2" height="13" left="277" textpieces="4" top="166" width="363">Fixed join algorithm   112   306   &gt;36,000   &gt;16,000</text>
<text font="4" height="15" left="330" textpieces="0" top="202" width="257">Table 6: Grounding time in seconds</text>
<text font="2" height="13" left="500" textpieces="1" top="240" width="62">IE   RC</text>
<text font="2" height="13" left="377" textpieces="2" top="259" width="184">Tu&#64256;y-batch       448   133</text>
<text font="2" height="13" left="398" textpieces="2" top="277" width="163">Tu&#64256;y          117    77</text>
<text font="2" height="13" left="357" textpieces="2" top="296" width="204">Tu&#64256;y+parallelism    28    42</text>
<text font="4" height="15" left="280" textpieces="0" top="331" width="358">Table 7: Comparison of execution time in seconds</text>
<text font="4" height="15" left="108" textpieces="0" top="384" width="702">which has both batch loading and parallelism. We use the same WalkSAT parameters on each com-</text>
<text font="4" height="15" left="108" textpieces="1" top="404" width="702">ponent (up to 106 &#64258;ips per component10) and run all three settings on the same machine with an</text>
<text font="4" height="15" left="108" textpieces="0" top="425" width="558">8-core Xeon CPU. Table 7 shows the end-to-end running time of each setting.</text>
<text font="4" height="15" left="133" textpieces="0" top="445" width="677">Clearly, loading the components one by one incurs signi&#64257;cant I/O cost on both datasets. The</text>
<text font="4" height="15" left="108" textpieces="0" top="465" width="702">grounding + partitioning time of IE and RC are 11 seconds and 35 seconds, respectively. Hence,</text>
<text font="4" height="15" left="108" textpieces="0" top="485" width="503">Tu&#64256;y+parallelism achieved roughly 6-time speed up on both datasets.</text>
<text font="3" height="19" left="108" textpieces="1" top="533" width="299">D  Extended Related Work</text>
<text font="4" height="17" left="108" textpieces="0" top="574" width="702">The idea of using the stochastic local search algorithm WalkSAT to &#64257;nd the most likely world is due</text>
<text font="4" height="17" left="108" textpieces="0" top="594" width="702">to Kautz et al. [14]. Singla and Domingos [43] proposed lazy grounding and applies it to WalkSAT,</text>
<text font="4" height="17" left="108" textpieces="0" top="614" width="702">resulting in an algorithm called LazySAT that is implemented in Alchemy. The idea of ignoring</text>
<text font="4" height="15" left="108" textpieces="0" top="635" width="702">ground clauses that are satis&#64257;ed by evidence is highlighted as an e&#64256;ective way of speeding up the</text>
<text font="4" height="14" left="108" textpieces="0" top="658" width="702">MLN grounding process in Shavlik and Natarajan [42], which formulates the grounding process</text>
<text font="4" height="15" left="108" textpieces="0" top="675" width="702">as nested loops and provides heuristics to approximate the optimal looping order. Mihalkova and</text>
<text font="4" height="15" left="108" textpieces="0" top="696" width="702">Mooney [36] also employ a bottom-up approach, but they address structure learning of MLNs</text>
<text font="4" height="15" left="108" textpieces="0" top="716" width="702">whereas we focus on inference. As an orthogonal approach to scaling MLN inference, Mihalkova</text>
<text font="4" height="15" left="108" textpieces="0" top="736" width="702">and Richardson [37] study how to avoid redundant computation by clustering similar query literals.</text>
<text font="4" height="17" left="108" textpieces="0" top="756" width="702">It is an interesting problem to incorporate their techniques into Tuffy. While Tuffy employs</text>
<text font="4" height="15" left="108" textpieces="0" top="777" width="702">the simple WalkSAT algorithm, there are more advanced techniques for MAP inference [32, 35]; we</text>
<text font="4" height="17" left="108" textpieces="0" top="797" width="702">plan to integrate them into upcoming versions of Tuffy. For hypergraph partitioning, there are</text>
<text font="4" height="15" left="108" textpieces="0" top="817" width="702">established solutions such as hMETIS [13]. However, their existing implementations are limited by</text>
<text font="4" height="15" left="108" textpieces="0" top="838" width="702">memory size, and it is challenging to implement the same algorithms e&#64259;ciently for on-disk data &#8211;</text>
<text font="4" height="15" left="108" textpieces="0" top="858" width="306">which motivated us to design Algorithm 3.</text>
<text font="4" height="15" left="133" textpieces="0" top="878" width="677">The technique of cutset conditioning [17] from the SAT and probabilistic inference literature is</text>
<text font="4" height="15" left="108" textpieces="0" top="899" width="702">closely related to our partitioning technique [31, 38]. Cutset conditioning recursively conditions on</text>
<text font="4" height="15" left="108" textpieces="0" top="919" width="702">cutsets of graphical models, and at each step exhaustively enumerates all con&#64257;gurations of the cut,</text>
<text font="4" height="15" left="108" textpieces="0" top="939" width="702">which is impractical in our scenario: even for small datasets, the cut size can easily be thousands,</text>
<text font="4" height="15" left="108" textpieces="0" top="960" width="702">making exhaustive enumeration infeasible. Instead, we use a Gauss-Seidel strategy, which proves</text>
<text font="4" height="15" left="108" textpieces="0" top="980" width="702">to be e&#64259;cient and e&#64256;ective in practice. Additionally, our conceptual goals are di&#64256;erent: our goal is</text>
<text font="6" height="8" left="121" textpieces="0" top="1009" width="453">10Early stopping could occur for components that have zero-cost solutions.</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">27</text>
<text font="4" height="15" left="108" textpieces="0" top="113" width="702">to &#64257;nd an analytic formula that quanti&#64257;es the e&#64256;ect of partitioning and then, we use this formula</text>
<text font="4" height="15" left="108" textpieces="0" top="133" width="702">to optimize the IO and scheduling behavior of a wide class of local search algorithms; in contrast,</text>
<text font="4" height="15" left="108" textpieces="0" top="154" width="413">prior work focuses on designing new inference algorithms.</text>
<text font="4" height="17" left="133" textpieces="0" top="174" width="677">Finally, we note that there are statistical-logical frameworks similar to MLNs, such as Proba-</text>
<text font="4" height="15" left="108" textpieces="0" top="194" width="702">bilistic Relational Models [34] and Relational Markov Models [44]. Since inference on those models</text>
<text font="4" height="17" left="108" textpieces="0" top="215" width="702">also requires grounding and search, we believe that the lessons we learned with MLNs will carry</text>
<text font="4" height="15" left="108" textpieces="0" top="235" width="129">over to them, too.</text>
<text font="3" height="19" left="108" textpieces="0" top="281" width="113">References</text>
<text font="2" height="13" left="108" textpieces="0" top="318" width="669">[31] D. Allen and A. Darwiche. New advances in inference by recursive conditioning. In UAI03, 2003.</text>
<text font="2" height="13" left="108" textpieces="0" top="345" width="702">[32] J. Duchi, D. Tarlow, G. Elidan, and D. Koller. Using combinatorial optimization within max-product</text>
<text font="2" height="13" left="139" textpieces="0" top="363" width="207">belief propagation. NIPS, 2007.</text>
<text font="2" height="13" left="108" textpieces="0" top="390" width="702">[33] C. Forgy. Rete: A fast algorithm for the many pattern/many object pattern match problem. Arti&#64257;cial</text>
<text font="2" height="13" left="139" textpieces="0" top="408" width="116">intelligence, 1982.</text>
<text font="2" height="13" left="108" textpieces="0" top="435" width="702">[34] N. Friedman, L. Getoor, D. Koller, and A. Pfe&#64256;er. Learning probabilistic relational models. In IJCAI,</text>
<text font="2" height="13" left="139" textpieces="0" top="453" width="34">1999.</text>
<text font="2" height="13" left="108" textpieces="0" top="480" width="702">[35] R. Gupta, A. Diwan, and S. Sarawagi. E&#64259;cient inference with cardinality-based clique potentials. In</text>
<text font="2" height="13" left="139" textpieces="0" top="497" width="82">ICML, 2007.</text>
<text font="2" height="13" left="108" textpieces="0" top="524" width="700">[36] L. Mihalkova and R. Mooney. Bottom-up learning of Markov logic network structure. In ICML, 2007.</text>
<text font="2" height="13" left="108" textpieces="0" top="551" width="702">[37] L. Mihalkova and M. Richardson. Speeding up inference in statistical relational learning by clustering</text>
<text font="2" height="13" left="139" textpieces="0" top="569" width="380">similar query literals. Inductive Logic Programming, 2010.</text>
<text font="2" height="13" left="108" textpieces="0" top="596" width="702">[38] T. Park and A. Van Gelder. Partitioning methods for satis&#64257;ability testing on large formulas. Automated</text>
<text font="2" height="13" left="139" textpieces="0" top="614" width="109">Deduction, 1996.</text>
<text font="2" height="13" left="108" textpieces="0" top="641" width="638">[39] J. Pearl. Probabilistic reasoning in intelligent systems: networks of plausible inference. 1988.</text>
<text font="2" height="13" left="108" textpieces="0" top="668" width="702">[40] H. Poon and P. Domingos. Sound and e&#64259;cient inference with probabilistic and deterministic dependen-</text>
<text font="2" height="13" left="139" textpieces="0" top="686" width="135">cies. In AAAI, 2006.</text>
<text font="2" height="13" left="108" textpieces="0" top="713" width="702">[41] H. Poon, P. Domingos, and M. Sumner. A general method for reducing the complexity of relational</text>
<text font="2" height="13" left="139" textpieces="0" top="731" width="332">inference and its application to MCMC. AAAI-08.</text>
<text font="2" height="13" left="108" textpieces="0" top="758" width="702">[42] J. Shavlik and S. Natarajan. Speeding up inference in Markov logic networks by preprocessing to reduce</text>
<text font="2" height="13" left="139" textpieces="0" top="775" width="371">the size of the resulting grounded network. In IJCAI-09.</text>
<text font="2" height="13" left="108" textpieces="0" top="802" width="630">[43] P. Singla and P. Domingos. Memory-e&#64259;cient inference in relational domains. In AAAI &#8217;06.</text>
<text font="2" height="13" left="108" textpieces="0" top="829" width="702">[44] B. Taskar, P. Abbeel, and D. Koller. Discriminative probabilistic models for relational data. In UAI,</text>
<text font="2" height="13" left="139" textpieces="0" top="847" width="34">2002.</text>
<text font="2" height="13" left="108" textpieces="0" top="874" width="702">[45] W. Wei, J. Erenrich, and B. Selman. Towards e&#64259;cient sampling: Exploiting random walk strategies.</text>
<text font="2" height="13" left="139" textpieces="0" top="892" width="101">In AAAI, 2004.</text>
<text font="2" height="13" left="108" textpieces="0" top="919" width="702">[46] M. Wellman, J. Breese, and R. Goldman. From knowledge bases to decision models. The Knowledge</text>
<text font="2" height="13" left="139" textpieces="0" top="937" width="171">Engineering Review, 1992.</text>
<text font="4" height="15" left="451" textpieces="0" top="1069" width="16">28</text>
